















The basic object is a ghost. A ghost is a description of some object discovered "for real" in the filesystem somewhere. 

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash


my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information.

The ghosttree is an abstraction around a collection of ghosts
there can be different implementations
the most basic one is just a hashing container that is as lazy as possible

$GhostTree = getAllGhostsUnderPath();

my @ghosts = GhostTree->ghostsForHash();


+++++++++++


# gfs snapshot .
# generates a snapshot file

# gfs condense /etc /var snap:barf.gs
# gfs condense snap:barf.gs snap:arf.gs snap:new.gs

# will reconstruct using known afns, keeps orphans
# gfs reconstruct --test snap:barf.gs foo/
# 150 files reconstructed, 4 not located (see gfsrun.3234.orphaned.txt for details)

# make symlinks rather than copy files
# gfs reconstruct --symlink 

# uses xxx and yyy as hash sources
# gfs reconstruct --test snap:barf.gs foo/ --source=xxx --source=yyyy

# verify all descriptors in /etc
# gfs verify /etc

# creates a GFS entity that can be pushed to
# gfs gfs create /etc gfs:foo.gfs

# gfs gfs push /etc gfs:foo.gfs
# makes sure that /etc can be reconstructed by gfs:foo.gfs

# gfs gfs add gfs:goo.
# a .gfs file can contain storage and 




+++++++++++++++++


seems like the best separation of concerns is a new idea:

FSE is really about the interface to a collection of files
TruthKeeper is the module that generates ghosts
it also tracks when ghost records are stale and may need to be pushed out to new places


+++++++++++++++++




Definitions:

	Source: A place to take things from, but must remain untouched
	Auxiliary|Pillage: A place we don't give a shit about and okay to move things

+++++




Thats sort of annoying, right? 

Everything is a ghost refactor. In this design, ghosts are interachanable with filenames
and FSE is really stripped down

M: main
FSE: Filesystem manager
	getStat
	getFullHash
	listDirectory
	copy
	move

G: Ghost
	getSize()
	learnSize() - accomodate potentially new information
	learnStat
TK: Truthkeeper
GS: Ghoststore


1-How do .gcpip files get populated initially?
2-How does .gcip file change when contents is changed?
3-How does a snapshot work?

	gfs snapshot filea fileb

	main: gs = TK->makeGhostsFromCommandline( @ARGV );
			obtainGhostForLFN( shift @ARGV )
				look inside our local run cache for these ghosts first
				if not, pull in the ghoststore of the parent using existing functions
		i = gs->getDescendingIterator(); - populate this GhostIterator with a queue of the roots
		g = i->next();
			pop queue 
			if( g->hasChildren() )
				g->getChildren()
					getChildren finds out the last time dirent was populated
					    if ok, then TK->getCachedChildren( g );
					    	load the .gcip file
					    if not ok, children = FSM->getImmediateChildren( g )
					    		this function opens the directory, does a full scan,
					    		and creates a ghoststore 
					    		calls TK->makeGhostForLFN( "fs:" . thingWeFindInDir)
					    	g->learnChildrenPresent( children );
					    		setChildrenMarkPresent now scans the known children
					    		marking those that aren't found as "not present"
					    		and setting the lastDirEnt date
			onThunk
				g->finalizeChildrenPresent()
					computes the hashes of names and subhashes
					populates lastHashDate() 
					g->learnFullHash(X);
						if different, sets dirty flag
						if date different, sets it, and sets dirty flag
					g->flush();
						Tk->flush() - called on thunk only for directory case
						writes out the gcip file and cleans the dirty flag
		g->forceCharacterize();
			checks all of its fields. For those that don't seem right, 
			we call the FSM -> retrieveFullHashHard
							-> retrieveStatHard




	gfs lsdup dirA dirB
	$sourceGS = TK->makeFromCommand( @argv sources)
	$targetGS = TK->makeFromCommand( @argv dest)

	$targetGS->getDescendingIterator();
	foreach( my $g = getNext() )
		$sourceGS->findAllDuplicates( $g );
			my @sizeMatches = $gs->findFilesWithSize( $size );
				@matches->isDuplicateOf( $g )


++++

Everything-is-a-ghost refactor complete
things still not really handled

1) lsdup should work with snapshots
2) findChildren should start with the files found in the descriptor, saving directory hashing
3) we aren't handling the thunks, therefore directories aren't geting hashes
4) support for identification of deleted files
5) support for filesystems that may or may not be online - e.g. volumes that mount or unmount

fields to add to ghosts lastEnumerated, firstStat, isDeleted, 


isHardIgnore needs to have . and .. added (they are creeping in)

++++++++


object store concepts

a blob always has the hash of the file it saves - the header (everything up to the first null) can be anything

++++

RSYNC implementation ideas

gs = ghoststore of source
for each file in ghoststore
	get a full snapshot of the remote directory + its pillage/spare places + aux places
	get a full snapshot of the source directory
	identify corresponding path in target to match rsync exactly
		A. There is a file fT already sitting at TARGET at this filename
			A1. The hash of fT is correct - its what we want
				-- Mark TARGET fT as claimed, no action needed, go to next file
			A2. The hash is not correct
				-- Mark as orphaned, select new orphaning location - during clearance MOVE phase (will be moved out first)
				-- Flow down to Case B
		B. There is no file at the remote location in TARGET tree at that filename, or the file in TARGET is marked orphaned
			A1. There is no hash ANYWHERE at TARGET, PILLAGE, AUX, COPY or ORPHAN
				-- Queue for remote copy, function done
			A2. There is a hash available in PILLAGE that is not claimed
				-- Queue move PILLAGE -> fT, claim it
			A2. There is a hash availble in ORPHAN that is not claimed
				-- Queue clearance move ORPHAN -> fT (assign new location) - DONE  (What about case of a swap..?)
			A3. There is a hash available in TARGET that is claimed
				-- Queue local copy, done
			A4. There is a hash available in AUX
				-- Queue local copy, done
			A5. There is a hash available in PILLAGE that is claimed
				-- Queue local copy, done
			A6. There is a hash available in COPYLIST
				-- Queue local copy, done

First do all clearance moves
Then do all remote copies
Then do all local copies

Internal commands we need:

action_clearanceMove( $s, $d )
	mark $s as empty, $s->deleted()
	mark $d with new ghost by cloning and assigning new LFN

action_remoteCopy( $s, $d )
	mark $d 

action_localCopy


FileOpPlan
	knows how to record activities
	knows how to check for conflicts
	knows how to start work and verify its proceeding as planned
	fires actual actions over to the FSE in sequence
	copy
	move
	copyIntoObjectFormat
	
We need to insist on a contract that STATs are repeated during full hash
this is going to be done anyway by the OS
also a guard if the file changes and the original stat and hash fall out of sync



we need to not be in a situation where the fullhash has changed but
the stat stays the same
scenario - two ghosts pointing to same file
ghost A has Ha and Sa (hash and stat)
ghost B has Hb and Sa

if Hb is later that Ha, we'd be confused - how could both stats be identical but the hash has changed?
contract should be
getStat
getHash
getStat

if stat is the same both times, than take the timestamp and report it for both

also implies the "learn" semantic should always be accompanied by a date
and not generated inside the routines'


(merge case)


Revocation rules
A new stat does not mean that a hash is out of date. The stat date should be used for resolving which of two stats is most accurate. 

stat fields should be taken all or nothing. 

The only cause for revoking a hash is when the size has changed or last modification date has changed. 

The revocation should not be immediate. It can be lazy. Maybe an optional warning. But the next time anyone asks for a hash on that file, we'd be forced to get one. 

Ghosts should never revoke other ghosts. Let that semantic rest with truth keeper. TK will just ask for incorporation in direction it wants. 

If you ask for a ghost that has no authoritative record is should be marked as virtual. 

One sanity check is that the virtual ghost should not be written to disk under normal cases. 



virtual ghosts
==============

gfs ls a b c 

will try to obtain equivilent of the following

gfs ls fs:a fs:b fs:c

but a b and c may not exist!

until a successful stat, they should be marked as a status = v
virtual should not get stored anywhere

whereas if you do a gfs ls . in a directory that has a b and c

in this case, a b c come from a scan, which by definition is not virtual
see?




so we create some virtual ghosts along the way
every time the operation is done, we should commit them (take off their virtual flag?)

maybe we just have a notion of copies or moves, and later we decide if they are local or remote?

need to check $g->isPathUnder($y);

next steps could occur in parallel
	begin copying over files we think are net new in reverse mtime order
	begin verification download (basically, get a snapshot of the remote FS)


PUSH looks different.. Push just says that we're going to move over things we think weren't there
into a "pushed-content" subdirectory
we could get things wrong but better for an offline situation

gsf accept pushed-content-1234 is used to rearrange the tree

maybe we have concept of a library
technically snapshots, orphans, pushes, etc could all be objects

GFS-library/objects
GFS-library/primay
GFS-library/orphans
GFS-library/pushes
GFS-library/snapshots
GFS-library/snapshots/Latest->xxx

need a flag on a blob if is is a moved file, or just some junk that GFS maintained
otherwise, cleanup on aisle three situation is shitty

maybe object syntax is as simple as foo/bar/this@234abc32a993, 
this syntax always means a file with this hash under that tree

how do we create definitive names for things and spread them around?
seems like git just stores things in namespace/xxx format

refs/tags/xyz


GFS objects











