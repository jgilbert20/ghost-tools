

# Compare mode

a new sematic i didn't realize
its not reconstruct
its stronger
it asks "is everything here exactly as it was in the snapshot?"


# Singleton nature

Its canonical that the LFN of a ghost once created must never change.

if you need to reference a new thing, make a new ghost


# determinis and order

ghost iterators now sort their contents why? well, turns out there is some
non-deterministic behavior going on depending on how the values get scrabled
into the cache and i need consistency. ideally though, we need a way of
iterating through the ghost store in an order similiar to the one the user
intendend.

# too much caching

there is a good reason to really restat the entire destination directory
before developing a workplan otherwise operations will start to go wrong when
things aren't where expected. I think its fair to not rely on caching 
when you are actually about to write new stuff.. 

do this at the start, don't wait for some forced cache

# New design idea

GhostStateTransform

populate the entire current state -> assign that to the original state
default is that future state = the current state




gst->assign( g, $lfn )
	# copies the G, puts it at the LFN in the future state
	# now current state != future state

gst->handleFutureStateLFN( $lfn )
	looks at that LFN, and emits instruction to move source G to dest G
	now curState = futureState
	every transformation moves future state to current state

# how does this handle swaps?

setOriginalState( $g )

the end of evrything, we handle orphans

LFN-> [original,current,final]
	points to ghosts


gst->generateWorkplan();

now caller says

for all target LFNs
	gst->setOriginal()
	sets original, current and final to the ghost (all pointers)

for all source LFNs
	gst->assign( $g, lfn)

gst->genWorkplan()
	simplest one is that each item in the workplan is copied over

gst->delete(lfn)
	sets the final to undef

gst->replace(src)
	sets future state to G = copy src;
	we have an ongoing hash of orphans, this adds to the orphans

gst->setOriginal( target/file1{A} )
gst->setOriginal( target/file2{B} )

gst->assign( source/file2{A}, target/file2{A} )
     makes new G target/file2{A} and backpointer to source/file2{A} as upstream
gst->assign( source/file1{B}, target/file1{B} )
	 makes new G target/file1{B} and backpointer to source/file1{B} as upstream

# issue - some of these ghosts will not have hashes and may never need them
# .. I guess assign has to be OK with that?

gst->go()
	gst->Clear()
		do clearances first
		first, we'll see that target/file1 will be replaced
			how? because finalState(LFN) != curState(lfn)
		so we'll call gst->move( target/file1, target/file1.orphan )
		so we'll call gst->move( target/file2, target/file2.orphan )
		now all new positions are cleared
			curState->undef, and i emit onto the work queue
			[MOVE, target/file1, target/file1.orphan]
		now I go through all of the LFNs
	gst->moveWinkinOrphans()
		in cases where curState undef, finalstate() defined
		now I get to look for the best way to fill finalstate
		my first bet will be to hit orphans
		orphans->findDuplicates( $g )
		get a match, now i drop into place
		gst->move( target/file1.orphan, target/file2 )
		gst->move( target/file2.orphan, target/file1 )
			now curState will have those things
	gst->findLocalCopies()
		things that are missing will look that way -- curState blank
			for all LFNs in curState, we stick into a GS
		generateLocalCopiesFromStore(GS)
			so now probe for the curstate
				generate a
					[COPY, source, target ]
				populate the curState 
	gst->generateLongHaulCopies()
		go through all of my curStats
		for each of those, check if targetCache->hasDuplicate()
		hasDuplicate should not force a hash unless the file sizes differ
		if net new, handle the copy
		if not, add to our pending guys()
	gst->generateLocalCopiesFromStore()
		so now probe for the curstate
			generate a
				[COPY, source, target ]
			populate the curState




advantages
	can be undone
	can be interrupted and resumed
	can be parallelized
	clearner to understand

gst->findSuperceeded()
	looks for all currents that are not final

the rule is that things that don't exist don't enter the GST.

so you'd see an "undef" in that location








# Existence

A stat on a file either has to tell you that the file has certain information,
or that it DOESN't have this information (e.g. the file doesn't exists). A
third option is that we cannot know (e.g permission issue.) How should this
data be passed back up the chain?

Simplest way I can think of is a modification to the stat rules. One of the
returned fields is basically a status, which can be considered a code of these
three situations. THe other parameters should be undef'ed. But that raises a
contradition - we'd have undefined data and a valid stat date.

Simplest Solution: Size is undef, mtime is undef, mode is "DELETED" or "NOACCESS"







# RSYNC Emulation

## What does RSync Do?

Rsync has some quirks I'd like to recreate. First, it doesn't matter if target
is there or not, it gets created. Secondly, the trailing slash seems to be irrelevant
to the target.

	rsync -av /bar/foo quxx target
		target/foo
		target/quxx

	rsync -av /bar/foo quxx target/
		target/foo
		target/quxx


The -R switch means that the entire provided path is taken as the root for the transfer

	rsync -av -R /bar/foo quxx target
		target/bar/foo
		target/quxx

	rsync -r dirA dirB destination
		destination/dirA/..
		destination/dirB/..



A trailing slash basically means treat the passed argument as a collection of files

	rsync -r dir-a/ dir-b destination
		destination/filea1
		destination/filea2
		destination/filea3
		destination/dirB/fileb1	
		destination/dirB/fileb2	
		destination/dirB/fileb3

Using this -R switch turns off the trailing slash behavior too

	rsync -r -R dir-a/ dir-b destination
		destination/dirA/filea1
		destination/dirA/filea2
		destination/dirA/filea3
		destination/dirB/fileb1	
		destination/dirB/fileb2	
		destination/dirB/fileb3

This creates some counter-intuitive behavior when the user is thinking about true mirrors

	rsync -r -R dir-a dir-b destination/dir-a
		destination/dir-a/dir-a/filea1
		destination/dir-a/dir-a/filea2
		destination/dir-a/dir-a/filea3

Because presumably you wanted it to make dir-a "look the same".

So in short, a trailing slash on TARGET is ignored
And a trailing slash on a SOURCE is only relevant if -R is off


## Translation to the gfs(1) tool

Lets say that foodir is a directory.

	gfs ls foodir 
	-> one result, representing the entry of foodir itself

	gfs ls foodir/
	-> a result for every entry under foodir

My guess is that its better to emulate the rsync(1) usage than the ls(1) usage
since most of the time you're handling directories of things.

## Playing devil's advocate on this decision

Devil's advocate.. Lets say you do a sync operation

	gfs sync foodir bardir

What is more logical? Would you want to think that foodir and bardir should
look the same afterwards? Or would you think that foodir should be synced
underneath bardir? After all, both are entries, and a sync operation suggests
that one should look like the other. More cognitive load to remember to place
things under other things.

	gfs sync foodir bardir/

To my eyes, the above seems to clearly say "stick foodir under bardir".

So the question is do we create our own new convention?

What about 

	gfs store foodir bardir

This seems to clearly call out for foodir to be stored as a full entry under something else.

So in this alternative scenario, a trailing slash always means "take what is underneath here".

	gfs sync foodir bardir
	# make them the same
	# if bardir had stuff not in foodir, remove it

	gfs sync foodir/ bardir
	# take everything under foodir and make sure each is the same as bardir
	# probably should result in an error - what does it mean to sync 3 things to 1 thing?
	# or maybe we pretent bardir had a trailing slash
	# or maybe this says that bardir should have exactly the contents of foodir in it

	gfs sync foodir bardir/
	# take foodir as an entry and make sure its the same as bardir/foodir
	# don't remove anything from bardir

	gfs sync foodir/ bardir/
	# make sure the contents of foodir appear under bardir.

Decision: NOT MADE - appears we may want flexibility to emulate multiple
formats in the future



# STUFF

immutable - basically means the ghost's truth state cannot change
for instance, in the case of a snapshot

virtual - we don't know if its "real" or not. its a taint check basically

things you pass on the command line should start out virtual. If we cannot look them up from a truth store, it should bomb out.

there are a few cases to consider

	gfs checksum fs:root:filea

		a) filea hits some cache already created
		b) filea doesn't, but when we call g->exists()?, its verified
		c) filea used to exist, but doesn't any longer - when is this figureed out?

	gfs checksum snap:snapfile:filea

		c) filea is not in the snapshot

so the g->exists() semantic is basically - does the FSE "own" this thing?
if the ghost is immutable, it exists by definition
maybe we want the rule to be that we never pass the check to the FSE?

what if the actual file has disappeared?

g->exists() goes to FSE and says "do i have you?"
FSE::snapshot checks its cache, returns true if the ghost's LFN is inside
FSE::fs checks its cache 

maybe we need to have different levels of existence testing()?

exists()
existsSourceVerify()

i think if the user bothers to list something, we can take it as a given  that
its OK to actually source verify it.  To not do that could cause a lot of
problems -- for isntance, if there were a global cache, user could ask to LS
things that were long gone and never be the wiser not expected behavior... so we
can't do that.

So do we pass these functions some sort of time override? We only need one
existence check per invocation (i think.)

maybe exists( <timespec> )?

as soon as something passes the exists test, it should be a candidate for global
caching

If a file is marked as deleted, exists() should return false. But of course, a
source verify on a deleted file needs to be sure that there is not a file
there already. I think we make a new ghost for that file. And we call isSameAs(),
and first that will check the stat.

the question is how should the caller be aware of the deleted file in this
subcase? i've got a ghost, and maybe i think its a deleted thing. i call
exists() to make sure it really is deleted and instead i get a true. should my
ghost now change? probably overthinking this case.

if you have a ghost and its marked as deleted. you soruce verify it, and its
there. refresh its stat, and mark it found. if the hash changes, so be it.
maybe the immutable flag plays a role here. if immutable, we can't source
verify it. 

so resolution: basically, exists() can source verify, and it source verifies
by doing a stat. That stat will bring in new knowlege, and may change flags on
things to bring the ghost to a point where it is consistent with reality.

so what if we have a ghost and our purpose is to test if it conforms to
reality? well, the answer would be to create a new ghost with that LFN and
verify that one. not sure why you'd want this, but it probably will come up.
suggests a semantic where you can ask a ghost to "checkDiscrepencies" and you
get a new ghost back with the actual state.

now another thing has occured to me -what do we do about FSE proliferation?

technically, more than one FSE can be operative on a local file system at a time.

does it matter who or what finalizes these things?

we need to recheck the LSDUP semantics and make sure getFSE() isn't going to get 
completely confused. my guess is that because the ::FS is so thin, it probably doesn't 
matter.  it stores very little state relateive to the ::Snapshiot

but this is an issue when it comes to moving the ghost descriptor code. 

i think we need to make sure that all fs:root ghosts always get the global FS when they ask questions about themselves. And I think this is what the code does now.


# thoughts on the absolute ghost cache (AGC)

## Problem 1

THe absolute cache is an interesting beast and has some peculuarities I need
to work through. First of all, how should it interact with .gfs_ip files? Say
I look for an ghost record and hit the AGC. There is no opportunity for state
to flow up from the .gfs_ip cache into the AGC. And if there is a dirty
record, we'll have to spontaneously make sure that the gfs_ip is loaded, which
itself may disrupt known state..

## Problem 2

When serializing to the AGC, we will write the filenames by asking the ghost
to represent them in an absolute way. There are at least two transformations
we can expect:

	1- The filenames will go to absolute
	2- snapshot roots will get converted to ^234abcdfef things

Now when those AGCs are retrieved, at what point do we "convert" them into
their local versions? Do we ever need to do this?

Lets say we leave them as absolutes. This will mean that anyone who calls
lfn() will see something they may not recognize. But the behavior of the
software may be correct anyway -- its not going to be fed "wrong" information,
just getting data in an unexpected form.

Maybe a better way to think about the AGC is that it is an overarching store
used to populate certain peices of information. If I want a ghost of ./a/test-
file1, I make it and it says local all of its life. But as a pre-loading step,
to "get me started", one source of truth about it is to copy data over from
the AGC. But we can do that with a bunch of "learn" optimizations. I can
g->learnFromOtherGhost(), and when I do that, I just check the AFNs are the
same. If they are, the data is safe to transfer over. The learning algorithim
will be smart to either keep or discard things.

My preference is to deal with .gfs_ip files at termination of the program, so 
lets let the dirty handler take that. 

SO we need really good learn semantics. Maybe we should write those down.

In the case of snapshot, I start with snap:foo:filea. Lets say I want to
operate on that file to do an LS. I'll make a ghost representing
snap:foo:filea. The AFN convert will see it as a ^234db pointer (forcing a
checksum of the file along the way, which is an separate thing), and therefore
we'll potentially hit the global cache, not even reading the file directly.

So decision stands: the AFNs will never directly enter the pool.

# Learn semantics: Time to get these right

There are two different kinds of stat overrides. Some stat overrides imply
immediately that the file is different, or are such a warning sign that they
cannot be ignored.

	- The size has changed (hash by definition cannot be the same)

Other stat overrides merely suggest suspicion.

	- Modification date changes
	- Inode has changed
	- Creation date has changed


Entity(truth name / time name)

statA/B - A=size B=everything else
hash - hash





A(statA/A/1) <- A(statB/B/1) = ERROR

	Error. Times are identical. 

A(statA/A/2) <- A(statB/B/1) = A(statA/A/2)

	Newer stat in learner remains over older one from teacher

A(statA/A/2) <- A(statB/B/1) = A(statA/A/2)

	Newer stat in learner remains over older one from teacher

A(statA/A/2) <- A(statB/B/1) = A(statA/A/2)

	Newer stat in learner remains over older one from teacher

A(statA/A/1) <- A(statB/B/2) = A(statB/B/2)

	Newer stat from teacher enters learner 

Simple cases, old replacing new (as a compound step)

	A(statA/A/1,hashA/1) <- A(statB/B/2,hashB/2) = A(statB/B/2,hashB/2)

Simple case, older data is is ignored

	A(statA/A/3,hashA/3) <- A(statB/B/2,hashB/2) = A(statA/A/3,hashA/3)
	
Stat B is different, hashB is different	

	A(statA/A/1,hashA/1) <- A(statB/B/2,hashB/1) = ERROR - two conflicting hashes w/ same timestamp

	A(statA/1,hashA/2) <- A(statB/B/2,hashB/1) = IMPOSSIBLE - 
		A would never have a stat older than the snapapshot

Harder case, B is more recently stated, but has older hash. Size same

	A(statA/X/2,hashA/2) <- A(statB/X/3,hashB/1) = A(statB/X/3,hashA/2)
	# take stat B as a refresh, allow the older hash to live on

	A(statA/X/2,hashA/2) <- A(statB/Y/3,hashB/1) = A(statB/Y/3,<revoked>)

Now break down into smaller logical steps

	A(statA/X/2,hashA/2) <- A(statB/X/3) = A(statB/X/3,hashA/2)
	# allow the B to come in

	A(statA/X/2,hashA/2) <- A(statB/Y/3) = A(statB/Y/3,<revoked>)
	# Hash "revocation" case

	A(statA/X/2,hashA/2) <- A(hashB/3) = A(statB/Y/3,hashB/3)
	# new has enters as long as its same or newer than the stat on A

Now try it without an explicit recovcation - can it work?

	A(statA/X/2,hashA/2) <- A(statB/Y/4) = A(statB/Y/4,hashA/2)**
	# Hash "revocation" case, A is now dirty

	A(statB/Y/4,hashA/2)** <- A(hashB/3) = A(statB/Y/4,hashA/2)**

	# Entry of a hash older that stat, but newer than hash. Allow it. 

The revocation plays a role here -- it conveys information.  In the case that
the hash and the stat date are identical, we know the ghost is maximally
correct. If the stat date is newer, we cannot distinguish between the case
that innocent data came along and really corrupting data came aloing.

Theoretically, there is nothing "wrong" with keeping the old hash.
Technically, the information is out of date and caller could decide. But we've
lost the change to tell the caller just how badly out of date that information
is.

I think the optimal policy then is to signal this case with recovcation.. If I
ask a ghost for a hash, its the best knowledge of the ghost's current state.
If I learn that my data is wrong, i'm failing in the contract even if I return
the "best known."

Basiclaly, the best known hash of a ghost who has received a size change is no
hash at all.

# The rules in summary now that we have them

RULE #1: A stat can never be older than the hash in a given ghost. 
Cause for immediate program termination.

RULE #2: Two peices of information with the same date must be the same, otherwise ERROR

RULE #3: The date corresponding to the size is lockstep with the date on rest of the stat

RULE #4: A new provided hash will be accepted into a learner as long as its date is newer than both the hash and the stat

RULE #5: A new stat that represents a change in size should invalidate the
existing hash, unless the hash has the exact same date as the stat. (If the existing
hash is newer than the incoming stat, but older than the existing stat, we
have a logical violation of RULE #1)

RULE #6: A dead stat (ERR) always revokes the hash and blanks out size and XXX










## Entries vs. ghosts

An entry is a thing that exists somewhere for the name. A ghost is a view
about state of an entry. There can be many conceptual ghosts pointing at the
same entry. When I call the truthkeeper, my request is to get the most recent
(or best) ghost for that entry. 











things that are not virtual are 'confirmed'.

they could be deleted, present, missing, etc

but to not be virtual, someone somewhere has to either deserialize them
from the past, or find them authoritatively on disk 

that way we don't contaminate the pool of ghosts

if the program that the user is asking to run cannot track down a confirmed ghost,
they can decide what to do (it's basically a file not found.)



# How do ghost work when they cross into the world of snapshots?

hah, interesting issue: what ghosts shpuld we pull from a snapshot? its
written as a bunch of fs:root: things... so  pulling them straight out of the
file contaminates the truth store.

but really, as a snapshot, we need to load it as virtualized objects, in their
world now, they are in a different name space

raises another question: boundary case -if you snapshot a snapshot, what
should happen? - i think the right thing is to either save a snapshot with
rewritten LFNs or to change the LFNs when loading

DECISION: I think its more correct to do on writing, e.g. -> convert  to snapshot LFNs
during the write operation

this requires new FSE semantics
should we think of a snapshot operation as a push into a filesystem?

YES

make a ghost for the snapshot-to-be
mkae FSE
FSE->addObject( $g );
	clone the ghost and re-write the LFN
	make them immutable
FSE->finalize();

we probably will need similiar semantics on object stores



# Interesting hanging chad around ghosts

say you want to write a snapshot to a new place. you get a ghost for
[snap:snapshot-5419.out:] and get the FSE to write to it. You store some
objects in it, and finalize the FSE. But there is a significant gap - what is
the purpose of [snap:snapshot-5419.out:]? Does it need to be finalized?

it is technically a valid path. its a valid thing in the world. its the head
position of a snapshot. its even been read and written to a few times. the
ghost represents a real thing, and you could potentially even want to do an LS
on it later.  so who is supposed to write it to disk?



# root renames on snapshot loads

a strange corner case - you write out a snapshot, a bunch of ghosts.  what is
the proper root of the snapshot file? Its actually not clear what that should
be. for instance:

	gfs snapshot foo bar -> snapshot123.snap

	gfs ls snap:snapshot123.snap:foo snap:snapshot123.snap:bar

	cp snapshot123.snap jeremy.snap

	gfs ls snap:jeremy.snap:foo

Technically, the root here was just a transitory thing. Its basically an local
relative filename. 

Design decision: The written version will always just be "root". 

but on LOADs, we'll rewrite the LFNs/root so they make sense to the user.


# Dealing with the obnoxiousness of absolute paths and snapshots

We decided earlier that the permanent root of a snapshot is the hash of the file
that stores it. This seems to work well for git(1). But what happens with the 
framework wants an absolute LFN before its written? Current design decision is
this is not allowed.

this is a pretty wide ranging issue, turns out. Say I invoke the following

	gfs ls snap:snapshot-5803.out:quxx

technically, "snap:snapshot-5803.out:quxx" will trigger creation of a FSE 
all of these ghosts are sort of not real.

ghosts that are not real should never be attempted to convert to AFNs? is that
true? they certainly shouldn't find their way into a persistent store. if that
were true, the truthkeeper has to learns that it is actualized, and thats when
it does and AFN test.







# What is a ghost?

The basic object is a ghost. A ghost is a description of some object
discovered "for real" in the filesystem somewhere.

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash

my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information. It is
basically serialized ghosts which have very local basenames for paths. 

The ghoststore is an abstraction around a collection of ghosts there can be
different implementations the most basic one is just a hashing container that
is as lazy as possible.


# About LFNS

LFN - local file name - whatever is colloquial to the USER to
describe something. LFNs can never presume to be universally unique between
invocations of GFS because they may be relative paths or tags/references may
change. But the user's maximal intent must be captured in the LFN.

	for instance fs:a depends on your CWD
	for a different PWD, fs:a could refer to a different file

However, there are many different handlers.

	/foo
	fs:/foo
	snap:foo.snap:file123
	remote:jgilbert@auspice.net:file123

In this case, the "LFN" is a composite of a handler, a root and a path. The
user may provide some or all of these peices on the command line. We need to
be be able  to intelligently construct the rest.

LFN's can be fullqualified, which means fully specify the missing strings and
to canonicalized the path.

To determine if two LFNs are the same inside one running instance of GFS, the
only way to do it is to canonicalize that path portion both to remove . and
../ etc. Then you need you make sure to attach the ROOT and the HANDLER.

across instances, the only way is to convert to an AFN (absolute file name)
The AFN is a unique permanent name for the location

FQAFN - fully qualified AFN: handler:root:afn
FQLFN - fully qualified LFN: handler:root:path (can be local)

snapshots will generally represent LFNs, typically from the point of view of
the PWD but their power is that they should NOT be foreced to contain AFNs -
the snapshot could be  reconstructed anywhere and we'll leave it to the user
to either feed a snapshot the AFNs or the LFNS.

in the nested case, assume the following data is stored

	snap:testdir/snapshot.gfssnap:
	snap:testdir/snapshot.gfssnap:a
	snap:testdir/snapshot.gfssnap:b

in this case, the LFN of the snapshot file itself is testdir/snapshot.gfssnap

So LFNs are designed to hold three peices of information, and together forms a
unique namespace.


# What is truth? What is the single source of truth?

Ghost is a description of the potential state of a file.

Currently, contracts look like this

- No obligation for a specific ghost to be up to date
- Only the ghosts from the TK are guarnteed to be single source of truth
- Ghosts find their FSE by looking at their handler value
- Ghosts can be in any number of ghost stores
- TruthKeeper should make all ghosts that are intended to hold present state
- The only reason TK is not used to make ghosts is in cases where you are deliverably
- TK holds all ghost pointers and knows if they are dirty or not
- Generally, the FSE doesn't hold links back to ghosts except in cases where a ghost is a root (like a snapshot)


# design ideas for object stores
object store concepts

a blob always has the hash of the file it saves - the header (everything up to the first null) can be anything


# Revocation rules and timing


we need to not be in a situation where the fullhash has changed but
the stat stays the same
scenario - two ghosts pointing to same file
ghost A has Ha and Sa (hash and stat)
ghost B has Hb and Sa

if Hb is later that Ha, we'd be confused - how could both stats be identical but the hash has changed?
contract should be
getStat
getHash
getStat

if stat is the same both times, than take the timestamp and report it for both

also implies the "learn" semantic should always be accompanied by a date
and not generated inside the routines'

(merge case)

A new stat does not mean that a hash is out of date. The stat date should be used for resolving which of two stats is most accurate. 

stat fields should be taken all or nothing. 

The only cause for revoking a hash is when the size has changed or last modification date has changed. 

The revocation should not be immediate. It can be lazy. Maybe an optional warning. But the next time anyone asks for a hash on that file, we'd be forced to get one. 

Ghosts should never revoke other ghosts. Let that semantic rest with truth keeper. TK will just ask for incorporation in direction it wants. 

If you ask for a ghost that has no authoritative record is should be marked as virtual. 

One sanity check is that the virtual ghost should not be written to disk under normal cases. 




+++++++++++


# gfs snapshot .
# generates a snapshot file

# gfs condense /etc /var snap:barf.gs
# gfs condense snap:barf.gs snap:arf.gs snap:new.gs

# will reconstruct using known afns, keeps orphans
# gfs reconstruct --test snap:barf.gs foo/
# 150 files reconstructed, 4 not located (see gfsrun.3234.orphaned.txt for details)

# make symlinks rather than copy files
# gfs reconstruct --symlink 

# uses xxx and yyy as hash sources
# gfs reconstruct --test snap:barf.gs foo/ --source=xxx --source=yyyy

# verify all descriptors in /etc
# gfs verify /etc

# creates a GFS entity that can be pushed to
# gfs gfs create /etc gfs:foo.gfs

# gfs gfs push /etc gfs:foo.gfs
# makes sure that /etc can be reconstructed by gfs:foo.gfs

# gfs gfs add gfs:goo.
# a .gfs file can contain storage and 




+++++++++++++++++


seems like the best separation of concerns is a new idea:

FSE is really about the interface to a collection of files
TruthKeeper is the module that generates ghosts
it also tracks when ghost records are stale and may need to be pushed out to new places


+++++++++++++++++




Definitions:

	Source: A place to take things from, but must remain untouched
	Auxiliary|Pillage: A place we don't give a shit about and okay to move things

+++++




Thats sort of annoying, right? 

Everything is a ghost refactor. In this design, ghosts are interachanable with filenames
and FSE is really stripped down

M: main
FSE: Filesystem manager
	getStat
	getFullHash
	listDirectory
	copy
	move

G: Ghost
	getSize()
	learnSize() - accomodate potentially new information
	learnStat
TK: Truthkeeper
GS: Ghoststore


1-How do .gcpip files get populated initially?
2-How does .gcip file change when contents is changed?
3-How does a snapshot work?

	gfs snapshot filea fileb

	main: gs = TK->makeGhostsFromCommandline( @ARGV );
			obtainGhostForLFN( shift @ARGV )
				look inside our local run cache for these ghosts first
				if not, pull in the ghoststore of the parent using existing functions
		i = gs->getDescendingIterator(); - populate this GhostIterator with a queue of the roots
		g = i->next();
			pop queue 
			if( g->hasChildren() )
				g->getChildren()
					getChildren finds out the last time dirent was populated
					    if ok, then TK->getCachedChildren( g );
					    	load the .gcip file
					    if not ok, children = FSM->getImmediateChildren( g )
					    		this function opens the directory, does a full scan,
					    		and creates a ghoststore 
					    		calls TK->makeGhostForLFN( "fs:" . thingWeFindInDir)
					    	g->learnChildrenPresent( children );
					    		setChildrenMarkPresent now scans the known children
					    		marking those that aren't found as "not present"
					    		and setting the lastDirEnt date
			onThunk
				g->finalizeChildrenPresent()
					computes the hashes of names and subhashes
					populates lastHashDate() 
					g->learnFullHash(X);
						if different, sets dirty flag
						if date different, sets it, and sets dirty flag
					g->flush();
						Tk->flush() - called on thunk only for directory case
						writes out the gcip file and cleans the dirty flag
		g->forceCharacterize();
			checks all of its fields. For those that don't seem right, 
			we call the FSM -> retrieveFullHashHard
							-> retrieveStatHard




	gfs lsdup dirA dirB
	$sourceGS = TK->makeFromCommand( @argv sources)
	$targetGS = TK->makeFromCommand( @argv dest)

	$targetGS->getDescendingIterator();
	foreach( my $g = getNext() )
		$sourceGS->findAllDuplicates( $g );
			my @sizeMatches = $gs->findFilesWithSize( $size );
				@matches->isDuplicateOf( $g )



###


++++

RSYNC implementation ideas

gs = ghoststore of source
for each file in ghoststore
	get a full snapshot of the remote directory + its pillage/spare places + aux places
	get a full snapshot of the source directory
	identify corresponding path in target to match rsync exactly
		A. There is a file fT already sitting at TARGET at this filename
			A1. The hash of fT is correct - its what we want
				-- Mark TARGET fT as claimed, no action needed, go to next file
			A2. The hash is not correct
				-- Mark as orphaned, select new orphaning location - during clearance MOVE phase (will be moved out first)
				-- Flow down to Case B
		B. There is no file at the remote location in TARGET tree at that filename, or the file in TARGET is marked orphaned
			A1. There is no hash ANYWHERE at TARGET, PILLAGE, AUX, COPY or ORPHAN
				-- Queue for remote copy, function done
			A2. There is a hash available in PILLAGE that is not claimed
				-- Queue move PILLAGE -> fT, claim it
			A2. There is a hash availble in ORPHAN that is not claimed
				-- Queue clearance move ORPHAN -> fT (assign new location) - DONE  (What about case of a swap..?)
			A3. There is a hash available in TARGET that is claimed
				-- Queue local copy, done
			A4. There is a hash available in AUX
				-- Queue local copy, done
			A5. There is a hash available in PILLAGE that is claimed
				-- Queue local copy, done
			A6. There is a hash available in COPYLIST
				-- Queue local copy, done

First do all clearance moves
Then do all remote copies
Then do all local copies

Internal commands we need:

action_clearanceMove( $s, $d )
	mark $s as empty, $s->deleted()
	mark $d with new ghost by cloning and assigning new LFN

action_remoteCopy( $s, $d )
	mark $d 

action_localCopy


FileOpPlan
	knows how to record activities
	knows how to check for conflicts
	knows how to start work and verify its proceeding as planned
	fires actual actions over to the FSE in sequence
	copy
	move
	copyIntoObjectFormat
	
We need to insist on a contract that STATs are repeated during full hash
this is going to be done anyway by the OS
also a guard if the file changes and the original stat and hash fall out of sync





virtual ghosts
==============

gfs ls a b c 

will try to obtain equivilent of the following

gfs ls fs:a fs:b fs:c

but a b and c may not exist!

until a successful stat, they should be marked as a status = v
virtual should not get stored anywhere

whereas if you do a gfs ls . in a directory that has a b and c

in this case, a b c come from a scan, which by definition is not virtual
see?




so we create some virtual ghosts along the way
every time the operation is done, we should commit them (take off their virtual flag?)

maybe we just have a notion of copies or moves, and later we decide if they are local or remote?

need to check $g->isPathUnder($y);

next steps could occur in parallel
	begin copying over files we think are net new in reverse mtime order
	begin verification download (basically, get a snapshot of the remote FS)


PUSH looks different.. Push just says that we're going to move over things we think weren't there
into a "pushed-content" subdirectory
we could get things wrong but better for an offline situation

gsf accept pushed-content-1234 is used to rearrange the tree

maybe we have concept of a library
technically snapshots, orphans, pushes, etc could all be objects

GFS-library/objects
GFS-library/primay
GFS-library/orphans
GFS-library/pushes
GFS-library/snapshots
GFS-library/snapshots/Latest->xxx

need a flag on a blob if is is a moved file, or just some junk that GFS maintained
otherwise, cleanup on aisle three situation is shitty

maybe object syntax is as simple as foo/bar/this@234abc32a993, 
this syntax always means a file with this hash under that tree

how do we create definitive names for things and spread them around?
seems like git just stores things in namespace/xxx format

refs/tags/xyz


GFS objects


# design decision - where should domains be stored? How do they nest?

fs:/a -> /a

do we store fs:/a in the lfn?
or do we split into a separate field like domain?
my temptation is to split them

that decision would result in these definitions:




# so what should be cached?

technically the B file's only LFN inside the invocation is: snap:testdir/snapshot.gfssnap/b
what is the AFN?
I suppose it must be 
/home/jgilbert/testdir/snapshot.gfssnap/b

but what if snapshot.gifssnap moves?

The funny thing in the case of a snapshot is that snapshot.gfssnap is 
probably not going to change

so you'd be good actually saying the AFN is: snap:423fe15a4757767bb4d2a04702f8572d3421179d/a/b

If the snapshot changes, you'd automatically get "revocation" of those AFNs

What about the remote case?

normally i'd talk about files somewhere else like this:

	jgilbert@auspice.net:backups/file-a

	(which is really:)

	jgilbert@auspice.net:/home/jgilbert/backups/file-a

	or

	jgilbert@auspice.net:/tmp/file-b

from the perspective of the local cache on the sending side, the AFN is what?

Ideally, you'd want to know 

	jgilbert@auspice.net:. 

		is the same as 

	jgilbert@auspice.net:/home/jgilbert

but how would i learn anything about that? feels like there is no clean way,
and the system can deal with discrepencies even if it doesn't know this
information. would be nice if there was a way to tell GFS that in the future

ideally when i have a volume tagging process i can say

	gfs volume.add.remote @GRYPHON jgilbert@auspice.net:myvolume

	gfs init GRYPHON

	gfs rsync my-photos/test @GRYPHON

So if we use the GIT strategy of tagging, there is some symbolic link between GRYPHON 
and a data structure with the connection information

@GRYPHON becomes its own virtual path. Assuming that there are no overlaps
@with other paths, or other volumes, the FQAFN would be remote:GRYPHON/my-
@photos/test

technically, the "remote" in this case is superflous because it can be
instantly known that this is a remote path by decoding @GRYPHON. Also we'd
want a way in the future to relocate @GRYPHON to a local path.

so these alias constructs know
	The handler (local, remote, volume, snapshot, etc)
	The remote location if there is one

technically, that means that every alias is a pointer to some object sitting in
an object store (Either one that is "real and dedicate", or one that is "found" )

this implies i can do the following

	gfs alias snap:foo.snap @FOO

	# creates a link saying that FOO -> 423fe15a4757767bb4d2a04702f8572d3421179d
	# and also remembers that 423fe15...21179d is a snapshot

	gfs ls @FOO/bar

	# which as long as I can find 423fe15...21179d, i'll just reach into that snapshot

But my stored FQAFNs.... Should they be:

	snap:423fe15...21179d/a/b/c

Or should they be

	alias:@FOO/bar ?

What if FOO changes in the future? The data that is closest to the "truth" is
the first one. That is also the information that is most reusable. But the second
strategy is probably better because @FOO presumably means something to the user
and the user has some intent to refer to @FOO/bar in the future

this raises another question even more fundamental. what if I do this?

	gfs alias /home/jgilbert/backups @LOCAL_BAKS

and say the content is like this

	/home/jgilbert/backups/file1
	/home/jgilbert/backups/file2
	/home/jgilbert/backups/file3

now I do a scan

	gfs snapshot @LOCAL_BAKS

Technically, I now have three universal names for these three files

	fs:/home/jgilbert/backups/file1 -> checksum 123
	snap:423fe15...21179d/file1     -> checksum 123
	@LOCAL_BAKS/file1               -> checksum 123

I think for the moment, all of these things are OK

A snapshot basically acts like a local alias. and the snapshot is the most permanent thing

if i were to list who has checksum 123, 

I'd want to see 

	snap:423fe15...21179d/file1
	@LOCAL_BAKS/file1

The first one I'd rather NOT see because it's expansion will have an identical expansion

How does this work with objects? Lets assume that when an blob is stored, its checksum is identical to a file that was also stored with that hash.

Implications

	gfs checksum file1
	-> 623fe15...24479d
	# now we know this hash

Next I can copy by hash

	gfs copy blob:423fe15...21179d foo.txt

I go through and find obj: anywhere I can, and copy it as a blob. 

Another form of this:

	gfs copy blob:423fe15...21179d foo/

	# copies into foo/423fe15...21179d.blob

But what is this idea of an object store? The object store theoretically has a name
and a location on disk. And the contract is that anytime you copy a file there, its name is lost and it gets absorbed.

	gfs copy foo.txt objstore:path/to/store
	# takes foo.txt and places it in objstore:path/to/store/obj/42/423fe15...21179d

	# the AFNs involved
	# objstore:/home/jgilbert/path/to/store/obj/42/423fe15...21179d --> 423fe15...21179d

Wait, so what is the point of storing objstore as the "domain" here? what
information does it add to the system?

Interesting pattern - in which cases does the AFN actually "need" a domain?

	Doesn't matter for aliases, regular files (fs:)
	but it does matter for remote
	and it matters for snapshots (i think)
	and its confusing for object stores

for instance, what is the difference between 

	objstore:/home/jgilbert/path/to/store/obj/42/423fe15...21179d
		  fs:/home/jgilbert/path/to/store/obj/42/423fe15...21179d

What data is gained by calling something an objectstore in the AFN?

two obvservations now that i've written this out:

1) AFNs really have only three core types:

	fs paths: /home/jgilbert/la-la-la
	containers: tarballs, snapshots, things that contain files and have an addressing scheme
	aliases: which can theoretically cover the earlier two cases

2) The meta-data requirements are different

	fs paths: don't need to know anything
	containers: need to know what expansion formula to use
	aliases: may need to point to either fs, or containers, or remote things

An alias is basically a container. But i need a level of indirection. The data in an alias
could change (hostname updates, path updates), and the namespace has to be valid.

so maybe the best term for a domain is a namespace

a. one name space is limited by files on your computer - world of absolute pathnames
b. another name space is basically ghosts that live under a single file-like-thing
	theoretically that single file like thing could have different "views"
c. another name space is aliases, which are ghosts that are referenced through a LUT

the prefixes used on the command line could be better thought of as access strategies?

consider a case of (b.) above.

AFN: snap:423fe15...21179d/file1 

but i might also have derrivative things

AFN: gunzip:423fe15...21179d     -> whatever the hash is of the thing unziped
AFN: gzip:423fe15...21179d    -> has of the thing zipped

what if the gzip algorithim changes?

well, each row is a ghost. so technically, each ghost has a lastFullHash date, 
and if that gets too old, you'd have to repeat the operation.

An alias can be said to always start with an AT

@FOO/bar

so lets consider a simple case of LS

	gfs ls @FOO/bar

	first, we make @FOO/bar a ghost G1
	then user calls G1->getHash();
	G1 is going to say, "wait, don't ask me, dereference me first!"
	so G2 = G1->dereference();
	two cases now
		case 1: fs alias
			G2 is going to be /home/foo/bar
			so i give the information about G2 and ascribe it to G1
		case 2: snapshot alias
			G2 is going to be snap:423fe15...21179d/file1
	Either way, G2 is something that i can ask a meaningful question about
	there is actually a question if G1 should even get "stored" anywhere

	in the case that i have no idea what snap:423fe15...21179d/file1 resolves as a hash

	when i call obtain "snap:423fe15...21179d/file1"

	the TK handler makes a virtual ghost with that name
	G2->getHash()

	will get passed to the "snap" FSE 

	SNAP FSE says fine, get me an object named 423fe15...21179d
	presumably this thing appears
	and i slurp it in, and then i'll have my LFN cache, and i look up file1

great lets take another case

	gfs ls snap:foo.snap/file1
	I obtain a ghost G1 for "snap:foo.snap/file1".
	G1->getHash();
		this will be a cache miss (obviously!)
		needs to be some logic to deference rather than giving up early
		so next I call G1->getFSE()
		and the FSE call will make me a new explorer connected to foo.snap
		if one doesn't exist already
		this implies some sort of FSE cache or factory
		Now I call FSE->getFullHash()
			well, if i have an AFN cache, a forced conversion to AFN will give me an
				opportunity to remember this answer since it AFN is snap:2343...43432/file1
			but if not..
			this routine will force enumeration of foo.snap, populating a LFN cache
			all of the ghosts will be stored in the FSE maybe for use later
			these ghosts should be marked as "immutable" somehow
				i'll pull out the ghost, and extract the hash from it, and return to caller

great, lets take another case

	gfs checksum tar:foo.tar/file1

	I obtain a ghost for G1 for "tar:foo.tar/file1"
	I ask, is tar:foo.tar/file1 a directory?
	the ghost code won't know yet,
	so it will ask for a stat on tar:foo.tar/file1
	get me a FSE for tar:foo.tar/file1
	this will navigate to a cahce for tar:foo.tar FSE controller
		i'll now generate an AFN and check it
		but assuming that fails
		I untar the file somewhere
		i'll be generating LFNs that look like this
			tar:foo.tar/a 
			tar:foo.tar/file1
		and i'll get the checksum of that file
			i'll make ghosts for the things i find there 
				and the call to checksum it will pass through to that ghost
					ghost will call me asking for the checksu
					i'll construct a new path with the temporary directory
						and hash that file
		if anyone asks for the AFN
		ghost will call the FSE to get the AFN

THis implies that canonocalization and rel2abs, cwd stuff all should live with the 
FSE, not the ghost code!!!

Lets revisit the alais case in more detail

	gfs checksum @mainbackup/neepfile.txt

	I obtain a ghost for "@mainbackup/neepfile.txt"
	initially marked as virtual, b/c i don't know what it is.
	first, i ask "are you an alias"?
		ghost getFSE returns the alias handling FSE
			fse->getFileType("@mainbackup/neepfile.txt")
				FSE will say yes, you are an alias
					not clear if FSE is a singleton thingy or one per alias
					might be cleaner if each alias consiered to be its own FSE
	now i say, G2 = G1->dereference();
		ghost will call getFSE->dereference("@mainbackup/neepfile.txt")
			FSE will go to its reference database
				learn that "mainbackup" points to object 423fe15...21179d
					so now I pull this object from store
							TK->getGhostForHash( )
						the object is going to say mainbackup = /Volumes/blah/bu
						now FSE calls TK->obtainGhostFor "/Volumes/blah/bu/neepfile.txt"
						return
	great, now I have G2 = "/Volumes/blah/bu/neepfile.txt"
	That clearly is just a regular FS path
	so handled the usual way, (Generate pcip files, etc.)
	not clear how the G2 information gets "absorbed into G1" and committed to disk

another way to think about this is that maybe there is no explicit "dereference"
instead, a ghost that knows it is an alias just passes all questions about itself
to the other thing?
but that would require a ton of proliferated code

maybe the TK just gets aliases and dereferences them after the fact to keep things clean

or there is a function on a ghost called g->absorbAttributesFromReferenceGhost()
and this moves over the hash, size, stat and other information

big question - is the file type of an alias always an alias?
like a symlink?
where you don't know if the symlink is a directory or a file?
maybe its best if we do that
unix seems to cope with that type of situation just fine


we need to figure out semantics for the reference database
we need to store two things permanently:

obj and ref

just like git

we can do this in our working directory
but it would be nice if this was all managed

this also should be some sort of FSE

objectstore

and there is a $coreOBS that is my working directory of shit
and i have an FSE complaint thing that manages it

so if I construct a hash like blob:423fe15,
G1 = tk->getGhostWithHash( "423fe15" )
	this routine is going to get you the most local version it can
	this may involve scanning various object stores it knows about
		its going to eventually end up saying
			$coreOBS->getGhostWithHash( "423fe15")
			the core OBS is special
			when you ask a object store FSE for a hashed thing
			it doesn't have to scan its known ghosts
			instead, it can actually just pick up the file
			it turns that into a ghost
			and someone should call "isAcceptaleCopy" to do a guard test on it

if i construct an lfn like blob:423fe15
G1  = TK->obtainGhostFromLFN( "blob:423fe15" );
	I'll get the ghost right away (its virtual)
	but the second anyone asks me to do something with it,
	i'll get delegated to to a blob FSE
	and the blob FSE knows to go back to truthkeeper
	maybe blobs are ocnsidered aliases (e.g. caller must dereference them)

thats sort of a nice way to handle it

in fact, yes, lets do it that way

maybe truthkeeper has a way when asked to do a getGhistWithHash
of passing that work down to FSEs?

there is also a ghostFromLFN strategy to think about
TK first checks its cache
if it can't find it there, it asks the relevant FSM
which implies that a global function to find FSM for LFN
this lives at the factory level



+++++

# reconstruct-test A B C
#   makes sure that everything in A and B be reconstructed in target C
#   as long as there is at least one valid duplicate, we move on
# 			first, best approach is to look for hashed AFN
#			       ask FSE if it "owns" that file
#				   then call "hasIdenticalContent()"
#               if that doens't work, check the cache of sizes
#					call hasIdenticalContent
#                  as soon as one returns "hasIdenticalContent", we return
#               finally, force the full cache of sizes

# copy-delta A B C
#    Anything in A and B that is not in C is going to get copied over 
#	 into a "delta" directory

# rsync A B C
#	 C is reconfigured to look exactly like A and B. Files that no longer
#    belong can be identified and removed as an option.
#	 this works by MOVING files around inside of C. depending on how
#    slashes are used, we either merge or replace (rsync ffo/ behavior)
#    $fse->move( a, b );
#			the move tool has to move the ghosts too
# 			and that may invalidate certain descriptor files

# what is the means by whcih deleted files are recongized?
# in cases of a full scan, someone has to be smart enough
# to know we are doing a scan that is "full" enough to count,
# and to mark ghosts for deletion

# right now, there is no stat verification when writing a snapshot
# snapshot -verify-stat (can find it, size and lastmod match) --verify-
# hash and when those things are in place, anthing that hasn't been
# checked in the last 24 hours are rejected, unless you say --hit-disk
# which means basically ignore all of the  tools and recheck everything


























