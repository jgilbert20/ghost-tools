















The basic object is a ghost. A ghost is a description of some object discovered "for real" in the filesystem somewhere. 

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash


my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information.

The ghosttree is an abstraction around a collection of ghosts
there can be different implementations
the most basic one is just a hashing container that is as lazy as possible

$GhostTree = getAllGhostsUnderPath();

my @ghosts = GhostTree->ghostsForHash();


+++++++++++


# gfs snapshot .
# generates a snapshot file

# gfs condense /etc /var snap:barf.gs
# gfs condense snap:barf.gs snap:arf.gs snap:new.gs

# will reconstruct using known afns, keeps orphans
# gfs reconstruct --test snap:barf.gs foo/
# 150 files reconstructed, 4 not located (see gfsrun.3234.orphaned.txt for details)

# make symlinks rather than copy files
# gfs reconstruct --symlink 

# uses xxx and yyy as hash sources
# gfs reconstruct --test snap:barf.gs foo/ --source=xxx --source=yyyy

# verify all descriptors in /etc
# gfs verify /etc

# creates a GFS entity that can be pushed to
# gfs gfs create /etc gfs:foo.gfs

# gfs gfs push /etc gfs:foo.gfs
# makes sure that /etc can be reconstructed by gfs:foo.gfs

# gfs gfs add gfs:goo.
# a .gfs file can contain storage and 




+++++++++++++++++


seems like the best separation of concerns is a new idea:

FSE is really about the interface to a collection of files
TruthKeeper is the module that generates ghosts
it also tracks when ghost records are stale and may need to be pushed out to new places


+++++++++++++++++




Definitions:

	Source: A place to take things from, but must remain untouched
	Auxiliary|Pillage: A place we don't give a shit about and okay to move things

+++++




Thats sort of annoying, right? 

Everything is a ghost refactor. In this design, ghosts are interachanable with filenames
and FSE is really stripped down

M: main
FSE: Filesystem manager
	getStat
	getFullHash
	listDirectory
	copy
	move

G: Ghost
	getSize()
	learnSize() - accomodate potentially new information
	learnStat
TK: Truthkeeper
GS: Ghoststore


1-How do .gcpip files get populated initially?
2-How does .gcip file change when contents is changed?
3-How does a snapshot work?

	gfs snapshot filea fileb

	main: gs = TK->makeGhostsFromCommandline( @ARGV );
			obtainGhostForLFN( shift @ARGV )
				look inside our local run cache for these ghosts first
				if not, pull in the ghoststore of the parent using existing functions
		i = gs->getDescendingIterator(); - populate this GhostIterator with a queue of the roots
		g = i->next();
			pop queue 
			if( g->hasChildren() )
				g->getChildren()
					getChildren finds out the last time dirent was populated
					    if ok, then TK->getCachedChildren( g );
					    	load the .gcip file
					    if not ok, children = FSM->getImmediateChildren( g )
					    		this function opens the directory, does a full scan,
					    		and creates a ghoststore 
					    		calls TK->makeGhostForLFN( "fs:" . thingWeFindInDir)
					    	g->learnChildrenPresent( children );
					    		setChildrenMarkPresent now scans the known children
					    		marking those that aren't found as "not present"
					    		and setting the lastDirEnt date
			onThunk
				g->finalizeChildrenPresent()
					computes the hashes of names and subhashes
					populates lastHashDate() 
					g->learnFullHash(X);
						if different, sets dirty flag
						if date different, sets it, and sets dirty flag
					g->flush();
						Tk->flush() - called on thunk only for directory case
						writes out the gcip file and cleans the dirty flag
		g->forceCharacterize();
			checks all of its fields. For those that don't seem right, 
			we call the FSM -> retrieveFullHashHard
							-> retrieveStatHard




	gfs lsdup dirA dirB
	$sourceGS = TK->makeFromCommand( @argv sources)
	$targetGS = TK->makeFromCommand( @argv dest)

	$targetGS->getDescendingIterator();
	foreach( my $g = getNext() )
		$sourceGS->findAllDuplicates( $g );
			my @sizeMatches = $gs->findFilesWithSize( $size );
				@matches->isDuplicateOf( $g )


++++

Everything-is-a-ghost refactor complete
things still not really handled

1) lsdup should work with snapshots
2) findChildren should start with the files found in the descriptor, saving directory hashing
3) we aren't handling the thunks, therefore directories aren't geting hashes
4) support for identification of deleted files
5) support for filesystems that may or may not be online - e.g. volumes that mount or unmount

fields to add to ghosts lastEnumerated, firstStat, isDeleted, 


isHardIgnore needs to have . and .. added (they are creeping in)

++++++++


object store concepts

a blob always has the hash of the file it saves - the header (everything up to the first null) can be anything

++++

RSYNC implementation ideas

gs = ghoststore of source
for each file in ghoststore
	get a full snapshot of the remote directory + its pillage/spare places + aux places
	get a full snapshot of the source directory
	identify corresponding path in target to match rsync exactly
		A. There is a file fT already sitting at TARGET at this filename
			A1. The hash of fT is correct - its what we want
				-- Mark TARGET fT as claimed, no action needed, go to next file
			A2. The hash is not correct
				-- Mark as orphaned, select new orphaning location - during clearance MOVE phase (will be moved out first)
				-- Flow down to Case B
		B. There is no file at the remote location in TARGET tree at that filename, or the file in TARGET is marked orphaned
			A1. There is no hash ANYWHERE at TARGET, PILLAGE, AUX, COPY or ORPHAN
				-- Queue for remote copy, function done
			A2. There is a hash available in PILLAGE that is not claimed
				-- Queue move PILLAGE -> fT, claim it
			A2. There is a hash availble in ORPHAN that is not claimed
				-- Queue clearance move ORPHAN -> fT (assign new location) - DONE
			A3. There is a hash available in TARGET that is claimed
				-- Queue local copy, done
			A4. There is a hash available in AUX
				-- Queue local copy, done
			A5. There is a hash available in PILLAGE that is claimed
				-- Queue local copy, done
			A6. There is a hash available in COPYLIST
				-- Queue local copy, done

First do all clearance moves
Then do all remote copies
Then do all local copies

Internal commands we need:

action_clearanceMove( $s, $d )
	mark $s as empty, $s->deleted()
	mark $d with new ghost by cloning and assigning new LFN

action_remoteCopy( $s, $d )
	mark $d 

action_localCopy

so we create some virtual ghosts along the way
every time the operation is done, we should commit them (take off their virtual flag?)

maybe we just have a notion of copies or moves, and later we decide if they are local or remote?

need to check $g->isPathUnder($y);

next steps could occur in parallel
	begin copying over files we think are net new in reverse mtime order
	begin verification download (basically, get a snapshot of the remote FS)


PUSH looks different.. Push just says that we're going to move over things we think weren't there
into a "pushed-content" subdirectory
we could get things wrong but better for an offline situation

gsf accept pushed-content-1234 is used to rearrange the tree

maybe we have concept of a library

GFS-library/objects
GFS-library/files
GFS-library/orphans
GFS-library/pushes










