
# What is this document

It is a collection of work in progress (or sometimes retired) ideas.. anything that is current has been moved to the full concept file. 

Current goal: Tidy backup and restore. Backup writes new files into a tree and a commit hash of the directory state. Backups can be named. Restore takes the name and reconstructs it somewhere else. Permanent metadata about the state of the backup is checkpointed into a deep tree object so that if metadata is destroyed we can recover it. Work is done when we can back up a directory a few times, then restore it, then blow away metadata, restore that, and then restore another file.

gfs checkpoint dir-a
gfs debug newsnapwrite dir-a
gfs debug newsnapread dir-a
gfs reconstruct foo.snap



- [ ] actually test the new learnAFNlist() functionality with a few different cases to convince myself its stable
- [ ] finish backup action - should write out a deep hash object
- [ ] wire in a reconstruct verb that uses recursion. And we should be able to test it works by clearing the caches, recheckpointing, etc
- [ ] Test speed. 



- [ ] rel2abs is generating paths like ../../../ in the AFN queue, can we actually just say revokes only happen on REAL things?
- [ ] ultimately, checkpointing probably ought to clear out all objects below that path
- [ ] what do we do about revokes during the learn and flush? Hashes will change on the ghosts held by the GDB, but should that trigger revokes? Also sometimes it triggers stats too. This is quite wasteful. 
	- [x] one of these is triggered by learnHash which mkaes a call to canHaveChildren [fixed]
- [x] Have discovered that writes into the object store do not appear to immediately enter the cache. (Not sure why...) - Fixed now
- [x] What "lastEnumeratedDate" do we use when we learn of our contents from a directory object? Hacking this now using lastFullHash date, which seems mostly correct.
- [ ] fix glitch that changing dir contents doesn't appear to revoke hashes
	- [ ] reproduce by cloning dir and moving files and recheckpointing
- [x] add logic to flush and recover directory objects from OJX store.
- [ ] rework snapshot intake and writes
- [ ] now build a proper reconstruct verb 


- [ ] directory size changes should NOT revoke the directory
- [ ] any creation of a new file should revoke parent hash, (call learnContentsChanged) - mostly sure this is true
- [ ] revoke enum (and therefore LC) when file is added
- [ ] revoke enum during mkdirs
- [x] Lower priority: wire in reads of directory caches to properly populate children, find some easy way to get LFNs for children fast



# Brain slow today

X is a ghost. I want to reconstruct X. 

Step 1: Check if I have a definition of X somewhere.
	Case A: Yes, I have X. Next, go through X's children. Anything in X that is a real file should be copied. Anything that is a directory, repeat the other thing.

	Case B: I can't find X. Oh. That sucks. well, that part won't be reconstructed. Better make a note somewhere and try the next shot.








# Thunk design

StoreThunk basically writes a file that is a promise just containing the hash. 

Maybe we write a thunk in cases where a restore / reconstruct cannot find the file. Then we provide some operations

	gfs thunk make xxxx
	gfs thunk restore .
	gfs thunk ls . 

I think the contract is that making a thunk forces the object into the internal repository no matter what. (Maybe a command line overrides this.)

Restoring thunks is a copy operation. We’ll let the user GC to get rid of thunks.

Thunks need to put their contents into a list of protected objects. GFS needs to be really sure that it never deletes something that has a thunk representation.

The list of protected objects could be persisted by a pointer to a block which is a list of hashes pointing to a true or a false, with a header that points to the previous block in the chain. And every so often you do a compact operation.

You’d actually get nice transactional semantics as long as the write to the new HEAD was atomic.

The question is what content does a thunk  show to the layer. I think copying it with a store should pull the full thing. And getchecksum should get the interior of the file the thing it points to? 

Actually I think it's best to treat it as a non file. Once it's been typed inclusion into directory objects or use in store and retrievals ought to be managed as a separate codepath. The default would always be to just treat it as a file with its own hash. 

In some cases if we checkpoint it we'd want the directory objects to pretend the thunk was real. In other cases no. 

We need the infrastructure to handle the idea that managed objects are registered. And we can mark i. Then ghost if they have been absorbed or not. 

I'm not sure if absorbing needs learning rules. But typing does. 

Change to hash should revoke type. 

If we make the checksum stable we can handle finding thunks for any given file really easy. We can I'd all thunks by hashing the hash the sa,e way and look for duplicates. 

If we had a check summing mechanism where the hash didn't start until after the header we could squeeze more functionality out of this trick. 

So to do this, we need a getFileType which returns some sort of structured string. GFS can have its own types. Type is revoked when mode or hash changes. 

I wonder if we should give mobs a namespace that is generated when a file is typed as a MOB. If we did this, it would be simpler to iterate through all objects of type X. 

So when you read foo.mob, it creates a ghost

	mob:thunk:[original hash] -> [hash of thunk]

This means that when the hash is cleared, we may want to revoke these ghosts. Or maybe, since they will be verified anyway, we just leave that up to garbage collection.

I suppose if you can’t find a hardfile with the hash anywhere its safe to purge this mob ghost.

## MOB general

$ghost->isManagedObject();

Primary way to instantiate a MOB from a ghost

	$mo = $ghost->getManagedObject();

Primary way to trigger loading of mob contents

	$ghost->absorbManagedObject();

maybe its absorb that creates the mob lookups

I think each mob should self sign so we can detect corruption

each mob supports a serialize and deserialize syntax? actually, don’t know if that is a good idea. Better to connect the MOB to a file on disk to allow memory mapping, etc. Or maybe we need to support both syntaxes.


# volume design 

When you make a new volume, it will write a volume descriptor object with a random hash of the date, a name and maybe some other characteristics to a file at the root, maybe something like 000_GFS_VOLUME or somesuch. 

When this file is discovered, it will register

	mob:volume_hash:[volume_hash] -> [voldescriptor hash]
	mob:volume_name:myvol         -> [voldescriptor hash]

I guess the root of the object could be inferred from the ghost? How do we ensure a volume is uniquely loaded and not duplicated? We have to keep out any confusion as to where the latest place is. Maybe if at any point we get an isDuplicate() which doesn’t appear in the object store its a problem? I’m not sure we’ve got a great way of knowing if a ghst is part of a volume or object store.

I THINK like most mobs, discovery should intake into the object store and eventually queue onto the authoritative list, or whatever construct we use to know which objects we “own” (and prevent from GC)

So how do we know inside the framework that a particular ghost should be seen as part of a volume (or a object store, for that matter?)

The dumbest way to do this is that we have the paths to the roots of these things, and every time we see an LFN we check it against this list.

Does this support the case where the volume is available over ssh? I THINK so.

so lets say we do a

	gfs copy blah.txt vol:MyVol:neep.txt

I get how this works. The volume handler is going to down reference to the local file system, aking it equivilent

	gfs copy blah.txt fs:root:/volumes/mount/MyVol/neep.txt

But if you executed the second thing directly, how do we update the ghost for ‘vol:MyVol:neep.txt’?

One answer is that we don’t, and we leave it up to checkpointing of a volume to find things like this.

One way to deal might be at the directory level.

If every operation navigated to a path from the root, could it “pick up” this kind of extra data?

So writing fs:root:/volumes/mount/MyVol/neep.txt presumably means we get the MyVol directory, and inside that ghost we’ll have data (i guess??) that has the association registered.

Maybe one approach is to say that there are a surrounding set of observing ghosts to every ghost. When a ghost gets some sort of learn or update, this is passed along to all of the ghosts to take whatever action they need. 

This architecture means that a single FS ghost could be part of multiple volumes, which is architecturally very tidy. Its probably a good trigger point for a lot of other behavior, including revocation of deepScans and enumerations. The question is where in the process it should propogate. The cleanest way to do it might be at flush() time. That way, you have a copy of the before and the after and can be very precise. 

# volume design, idea 2

Here is a bolder way to handle volumes that doesn’t require a proliferation of ghosts the same way. What if the main thing a volume tracks is the directory object of the volume itself? When you checkpoint the volume (maybe “gfs checkpoint --all” could help automatically look for connected volumes for you), it inserts a pointer to that object as the volume. We could also use that opportunity to write to a second object store inside the volume any objects found there. Now when you do a vol:XXX: lookup, you basically start traversing the versioned tree of directory objects. Some rule kicks in that makes them immutable if they can’t be found (or if the volume is believed to be disconnected.) Or maybe they are just always immutable on descent the same way that snapshots are (probably the best option.) 

The challenge with this approach is we will have trouble reporting this situation back to the user without some obnoxious hacjs. Say we store a bunch of files under 

	fs:root:/volumes/mount/MyVol/

In this proposal, no vol: objects are actually created, except maybe one that serves as the head end (vol:NAME:). 

Now, when we lsdups or take other actions, we won’t automatically see the volume affiliations. So we’ll have to query for alternative names (e.g. other top level nodes that point into the directory hash tree) and decide how to present the ghosts. Maybe we can handle that problem with a $ghost->getAliases() which returns all of the other things that point to that AFN? This feels a bit sloppy. Do I have a fast, clean way to know if I have subdirectory object X if hash Y sits underneath?

Similiar problem to commit trees in git - how do i hunt down all of the commits that applied an update to a particular file? Git must just manually chew through the commit chain. I guess the point is that doing this is possible, and if its too slow for practical use, we can think abotu optimizations or caching. 

On the plus side, it means 50% fewer ghosts floating around in the system and much less cleaning up to potentially do when volumes are added or removed. 

Looks like the git guys struggled with this issue too:

http://gitster.livejournal.com/35628.html

Linus even seems to think these things shouldn’t be burdened into commits:

http://article.gmane.org/gmane.comp.version-control.git/217

Starting to read documentation and git source. Seems like this is a really strong committment.



Nice DB, consider as alternative?

https://symas.com/products/lightning-memory-mapped-database/

Bitmap library derrived by git team, and explanation on how it is used for reachability (which I need too!)

https://github.com/lemire/EWAHBoolArray
http://githubengineering.com/counting-objects/
https://docs.google.com/presentation/d/1t2qzSO4z5MbrN21Cz4RrLWz9IZ5ylJNwQassajKyPk8/view#slide=id.g3236f3d6_0_122

So i’m pretty convinced now that commits of the parent directory objects are the way to handle tracking. At least, its a good first start. If the commit object had the date, the name, the source and the destination, everything is good. Maybe a link back to the last backup (of any kind) would be good to create a chain. 

There apparently exist some great, lightning quick algorithims that git has used to efficiently do reachability (for handling pruning down backups.)

So implications: Every checkpoint creates a commit object. YOu can checkpoint at any level of the tree. And every volume has a separate checkpoint object created for it every time you do a major operation. 

# Next level design

So lets carefully walk through the backup case. SOURCE -> TARGET.

	SOURCE
		/                 [6]
		dir-1/            [3]
		dir-1/filea.txt - [1]
		dir-1/fileb.txt - [2]

	TARGET
		/                  [7]
		backups/           [4]
		backups/file-a.txt [1]
		backups/file-c.txt [5]


First, we go through and checkpoint our source. We’ll get a top level directory object [6]. 

Now, we ask if TARGET->crh(Source->toplevel). CRH looks inside target and says if the node and all of its components can be reconstructed. First, we’ll ask if Target [7] can reconstruct [6]. The answer is no. We don’t see [6] anywhere under the target. (The target has 1,4,5,7). So then we go an look for each of the parts of [6] and we’ll get [3]. This also is not under the target. We go down one level more and we have [1] and [2]. We find [1], but not [2]. So now we have a list of things to copy over: [6], [3], [2]. We copy over [2] as a file. 6,3 are metadata only. 

The rule is if I have a metadata object X, Y->CRH(X) is true either if: 
	
	All of the leaves and subnodes of X can be found under Y
	
	OR
	
	X == Y and Y->lastVerifiedSuperParent() = current parent of Y and current parent of Y lastScan in acceptable range. 
	
	OR
	
	X == Y, and Y is a real checkpointed object

Sort of a wierd structure, right?

Maybe its cleaner if we use some reachability matrix?

We need some concept of domain - a named place where things can be stored under. 

So thats all really complex. Simpler might just be that you walk down the SOURCE tree. You either need TARGET with that hash for a node, or TARGET for that hash with a checkpoint date. If not, you need to copy over the node or copy over the metadata (like a directory as an object)

# More killer use cases

With this architecture, we could move to named snapshots:

	gfs checkpoint . -n "bestpics"

This becomes the new head of a “branch” called “bestpics”. 

Now in my virtual filesystem, I have different access points, potentially FUSE accessible

	gfs copy gfs:root:/checkpoints/head/bestpics foo/
	
	add files 1, 2, 3 to foo
	
	cd foo/
	
	gfs checkpoint . -n "bestpics"
	bestpics name updated to [a73bfe3]

This means there will need to be some logic that allows recovery of the previous commit object by path (or even by a dotfile?)

Once I get this right, I actually think this could subsume the volume logic easily.

I also would want an ingest feature that lets me checkout from the global tree, make changes, checkpoint, and then ingest everyting. 

Ingest would mean that I check I have the right number of copies in other places (e.g. persistent place like dropbox) and then actually delete or unlink (unless I am already working in the persistent place, in which case I do nothing.)

So lets say I identify a local store

	/home/jgilbert/gfs
	/home/jgilbert/gfs/objects
	/home/jgilbert/gfs/archive
	/home/jgilbert/gfs/archive/bestpics/file1
	/home/jgilbert/gfs/archive/bestpics/file2
	/home/jgilbert/gfs/archive/bestpics-older/20160405/file1

If each gfs pocket like this had a unique UUID or name or something, than it would be possible to create a universal drive. And every branch name would have 1-2 places it expects to be for replication.

	cd /home/jgilbert
	gfs initvol gfs -n "homedir"
	cd /Volumes/usb-stick 
	gfs initvol my-archive-gfs -n "stick"
	
	cd /home/jgilbert/working-files
	gfs checkpoint . -n "plm-working"
	# creates a snapshot, but nothing is copied
	
	gfs assign-vol plm-working homedir
	gfs update-vol plm-working
	# now it makes a copy to the homedir space

I can unassign a volume to a checkpoint/snapshot, which removes the pointers and makes them eligible for GC. 

So to do all of this, I need the following features working

1. [ ] I must be able to checkpoint an arbitrary tree causing the files to calculate a top level directory hash printed to the user
2. [ ] along the way, directory objects need to be written out into the object store (for now thats where they will live)
3. [ ] I must be able to provide checkout or clone (I dunno WTF to call this operation) a directory hash and get back a complete clone of the tree in question into any directory. Objects for now could just be pulled from the GFS object store or wherever they can be found
4. [ ] I need GFS to be able to recover directory objects not by reading one of those complex snapshot things but instead just by checkpointing a tree that might have directory objects.
5. [ ] Checkpointing should recognize the directory object code and slurp it in (slurpMOB needs to be implemented.)
6. [ ] This means some magic string at the start of the directory object
7. [ ] GFS needs to be able to “discover” this data on demand, so even if it doesn’t recognize a type, it can slurp it in because it sees the hash has something important
8. [ ] All of the above does not need packfiles or anything exotic. I am now 100% convinced that once performance becomes an issue (if it ever does) there are really terrific optimizations that have already been worked out by others
9. [ ] So if this is all working correctly, I should be able to reset GFS entirely, checkpoint an original directory containing stuff (even a former GFS working object store) and then checkout a cached folder
10. [ ] All of the above is a great proof of concept but doesn’t actually create a backup. I need to be able to take a hash and a path and feed it to a command which then sets up a tree that can later fully reconstruct something. We can change the name later but maybe we just say “write-archive”.
11. [ ] This tree needs to walk down a tree of directory objects and not just write the leaves out but also save off the directory objects somewhere.
12. [ ] The dumbest way to do this is just to write those objects at the top level (obj-737abdf83a.gfs)
13. [ ] But an equally good/dumb way is to do it is to create a .gfs-meta/obj-737abdf83a.gfs in each directory.
14. [ ] If we want to be really pedantic, we could actually update the logic such that if a .gfs-meta directory is hashed during a subsequent checkpoint, we ensure that it does not contribute to the parent’s checksum, so that we’ll prevent the infinite cycle of each checkpoint recalculating every directory’s hash over and over again.
15. [ ] We could think about sticking the commit objects into a .gfs-meta too!
16. [ ] And then also maybe the pointer to “latest” (e.g.) latest-picks.ptr, which if it had the right timestamps and magic could be a recoverable way of getting pointers back

Hold on a tick, why don’t we just combine ALL of the above into a checkpoint operation? Checkpoint goes though and populates all of those little .gfs-meta objects. And yes, that creates a massive proliferation of little files, but the truth is that once we do this in the general case, we can optimize it to use a packfile format. So this implies that after checkpointing the interior of a tree, we make a call to 

	getMetadataDirectoryForPath(/x/y/z)

Which returns us a ghost and we start writing these things into that directory object e.g. MDD->addAsObject(X). It also means we need a way of arbitrarily writing a pointer object too. The NAME of the file needs to be the pointer.PTR,

	$dirObject->getPointer( "foo" );
	"searches for foo.PTR" file anywhere under that tree
	once found, checks the MOB signature date, returns the hash
	does a LEARN operation on it
	updates an AFN
	ptr:root:foobar -> [hash of its latest thing], value:[X]

Also implies that we have a way of writing

	$dirObject->writeGhost( $ghost );

which asks that directory for its metadata directory, and its ghoststore.gfs, and then appends the ghost onto it and writes it out. Probably have some sort of cache for now.

And then finally,

	$dirObject->writeMOB( $commit );

Which finds the metadata store for the dirObject and sticks the commit object definiton in there.

Now once this is done, every checkpointed directory becomes self describing. Easy to have future optimizations just write to a packfile for speed and awesomeness.

Also means that the backup operation just technically is like this

	$targetOfBackup = makeGhost(...)
	$targetOfBackup->populateFromCommit( $commit )
	# goes through and makes sure that the commit object
	# itself is written as a MOB, and then walks down making sure there is a copy somewhere 

Okay, so that means we need to be able to do this

	$ gfs checkpoint . 
	No previous commit object for . found, initializing a new one
	Commit: [abc233]
	
	$ gfs verify .
	Comparing current tree vs. its most recent commit object
	Differences found:
	-> file XXX now XXX, was YYY
	->
	
	$ gfs backup-new abc233 /Volumes/foo
	note: /Volumes/foo did not previously set up as a GFS target
	note: created /Volumes/foo/.gfsmeta
	commit abc233 now available inside /Volumes/foo

I’m delighted to see that this fulfills most of the original design spec use cases, including backup, archival and zero internal metadata required to reconstruct

So lets make pointers a ghost and then our generalization is more or less complete.

# GFS vs. Git

Git is awesome. Its such a perfect illustration of the principal of UNIX-like parisomy in software design. It keeps things simple and as a result of upholding this comittment, it remains uber-elegant even as its feature set explodes. Its tempting to just want todo this all in git, but my objectives are different:

The major differences:

- GFS doesn’t want to have any concept of repository: it wants to be flexible with a wide variety of use cases where there is no predefined location for things or objects
- GFS wants to have highly resiliant metadata. Commits, blobs and directories all need to be reconstructable “on demand” from discovered information on disk
- GFS wants to be completely location agnostic - if a verified peice of information is found in an unexpected place, we are OK to deal with it that way.
- GFS wants to accomodate maintaing state where managed files are spread across time and space -- GIT assumes that everything it cares about can always be found united together in one directory. GFS is focused on knowing the multiple locations where something could be found and reconciling them
- GIT is optimziing for source code and versioning tools; GFS is optimizing for a “swiss army knife” approach for backup, file management, volumes, and archival. 
- 


# Reconstruction revisited

Reconstruction is when I am given a set of sources and asked if that entire set of sources could reconstruct the target. 

The sources are a set of ghosts, and the target is presumably a single ghost.

The best possible case is where the target ghost is already known to be at one of the sources. Assuming this is the case with acceptable bounds on deepScan etc, reconstruction would just mean getting rid of anything that isn’t the target.

If the hash doesn’t exist, we go into the next level on the target. Recurively, we look for each item (either blobs or directories). Reconsutrction is assumed to be valid as long as the underlying hashes have reasonable lastScan times. 

With directories, we have to be careful. Two things are needed to reconstruct the hash. First, we need the description of the directory’s state (which is strangely met as long as we can find a file with that hash.) The second is that we need some assurance that this pattern of hashes, down all the way to the leaves, is still on disk. 

lastDeepScan is the latest date for which the fullHash on a directory is valid. This means that although the underlying bytes were not checked, the ghosts for all of the objects do add up to the hash. the lastFastHash is the oldest of all of these byte checks. 

So this means that if a directory ghost has a hash and a valid deepScan, as far as the framework is concerned, that entire block of hashes spanning the entire chain of hashes is present on disk.

There is an interesting optimization. Lets say I have object M, which represents 10,000 hashes. And lets say I have object R that is a subset of those hashes. If I assert that M can reconstruct R, I can save enumeration over those hashes in a really efficent way. Espcially if M sticks around.

For instance, if I take a baseline of a massive file tree. And I emit data somewhere saying that each of those hashes is reconstructed by M, that saves me checking smaller things individually.

A ghost that says X can be reconsturcted by Y is all we need. In the case of directories:

	fs:root:x/y/R    -> [234a]

Means that R is a container for 234a as a stack of hashes.

What if I crated some other block

	store:^ab1234:pointer2   -> [9934a]

This basically says that if I have [ab1234], I could reconstruct 9934a, by using pointer2. This in turn can be validated by a filesystem hash:

	fs:root:/big/file/here     -> [ab1234]

So this means that the AFN->ghost hash can store meta data in a general way that X can reconstruct Y.

In the above example, store: acts like a packfile pointer. If we repack, we simply emit new ghosts and garbage collect the old references. There also might be some way of incrementally building fast paths to objects we use frequently.

So back to the reconstruct mode, as long as I can find an object that meets my scan critiera, if it say it has hash X and we know wht hash X implies hash Y, than that object reconstructs Y. Its sort of a transitive property of CRH();

A reconstruct operation can always boil down to placing ghosts in the place that the target says to put them, and moving the other things out of the way. A reconstruct that cleans is almost as simple as a second pass that just goes through and removes everything found that isn’t in the target.

Can this work with files that are split into tiny peices? Lets say I have a big file

	fs:root:/path/bigfile.dat -> [234ab]

When I split it up, I create a funny little truthy object that contains the following

	split:^234ab:0000-0020    -> [abc423]
	split:^234ab:0021-0040    -> [9c325f]
	split:^234ab:0041-0070    -> [a7ccf3]

The big file just stays where it is. (Nothing changes.) But i’ve now identified smaller peices of that object that could be matched from other files. This meta file say has hash [7672b]

Now say I have a second big file that has many of the same blocks called bigfile2.dat.

	fs:root:/path/bigfile2.dat -> [777e3]

I split into many little peices and I’ll get:

	split:^777e3:0000-0020    -> [abc423]
	split:^777e3:0021-0040    -> [52234a] <--the only difference
	split:^777e3:0041-0070    -> [a7ccf3]

Together, all of this peices sum up (like a directory) to hash [123ab]

Now I guess (and this is really the tricky thing), there needs to be some overarching object that holds these baby peices together. In this case, two objects. 

	merge:^777e3:    -> [123ab]   (for bigfile 2) 
	merge:^234ab:    -> [7672b]   (for bigfile 1)

These two objects say that if you are looking for ^777e3, you can find it by pulling open the ^123ab object. 

Now imagine there is some sort of absorb operation that pulls a file into an objectstore. 

In this case, the operation needs to have backing state (e.g. I store state) for these two ghosts:

	fs:root:/path/bigfile.dat -> [234ab]
	fs:root:/path/bigfile2.dat -> [777e3]

Technically these two files dont’ exist anymore. But if they were tied up in a commit object, we’ll preserve the ghosts of those files.

When we try to reconstruct 777e3, we won’t find it. So we’ll look for merge:^77e3, and get a pointer to a split object. That will then share with us data like this

	split:^777e3:0000-0020    -> [abc423]
	split:^777e3:0021-0040    -> [52234a] <--the only difference
	split:^777e3:0041-0070    -> [a7ccf3]

Now, we could potentially treat this as a directory, which might work better. Lets pretend the split file actually reads like this:

	split::^777e3              -> [123ab]
	split::^777e3/0000-0020    -> [abc423]
	split::^777e3/0021-0040    -> [52234a] <--the only difference
	split::^777e3/0041-0070    -> [a7ccf3]

Now according to the usual rules, if I want an object of 777e3, i can opt to replace it with its children. 123ab is the hash of the childrens descriptor. That assumes I have some clean way to locate the children, but it will be just as clean as the directory approach. 

Remember the corresponding merge object was

	merge:^777e3:    -> [123ab]   (for bigfile 2) 

I actually think this is backwards. The merge object FULFILLS the hash of 777e3. Maybe its better to emit the truth in the oppositite way:

	merge:^123ab:    -> [777e3]   (for bigfile 2) 

That means that the merge object will appear on searches for the hash. So redoing the previoius logic, I seek 777e3. I don’t find a file, but rather a merge object. The merge object gives me a path that i can go to the FSE, and learn that 

	split::^777e3              -> [123ab]

Which now gets me to the 123ab object with the actual splits.

When we read a split object, we could automatically make sure the merge object is created. The two are really just inverses of each other.

This brings me to another point. In fact, its another contadicition about directories in the present implementation. There is no logical separation between the hash of the thing that describes a transformation and the hash of the result of the transformation.

Say I have a directory A. I wonder if its really much cleaner to say

	fs:root:/path/to/A    ->  [f66ed]
	directory:^123bc      ->  [f66ed]

Which basically says that if you need a directory with hash f66ed, you can get it either by reading it from disk right now, or by pulling it from a directory descriptor ^123bc.

In the split/merge case, we have a bigfile2.dat blob with hash 777e3. I want to say you can get it either by reading from disk as a pathname, or by addressing a split object. But the split object does not have the hash 777e3. It has some other hash.

Many operations in GFS seem to have this pattern. Either you get something by reading it from a place, or you get something by applying a transformation over several other things that have a descriptor that ties them together.

In a strange way, the splits are the children of the blob. What if you had a second hash {99494} that could be stored on a ghost. Does this make it cleaner?

	fs:root:/path/bigfile.dat -> [234ab] {}
	merge:234ab               -> [234ab] {55332b}

Where 55332b is the descriptor that can enumerate the children. Since there might be several different ways of enumerating children of an object, you’d not want to actually put this on the other thing. 

I find this extra field overly redudant because the same hash seems to live in a few different places. 

So that brings us back to merge objects then, which describ the result of combining a split object together. ANd then implictly give you the split object’s hash that you then track down, and get children on that object.





Interesting new thought: if we generalized ghosts to be AFN -> state about a file. what if it was AFN -> truth about the AFN?

Then the HASH->AFN map is actually just

  fs:root:numchucks    -> [2342111] 
	blob:2342111 -> (no hash), pointer=fs:root:numchucks

But the lovely thing here now is that the revocation for the blob follows cleaner, learnable rules.

I realy likke this. And it means I can get nearly everything i need out of a KV pair storage system.


# MOBs and Packfiles

So in this emerging architecture, we have split objects which are basically blobs tagged with a bit of extra information, and we have directory objects. How do we efficiently managed these things when they grow into the scale of millions?

Clearly the packfile is the key here. But I think the GFS concept of a packfile is a little different. A packfile needs to be able to hold ghosts as well as MOBs (which are basically blobs.) 

The packfiles have to have a way to quickly check if content is available by hash. There might even be use cases where we’d also want to be able to address things by AFN, but doing that poses some complications. (It would be really nice if all AFNs were themselves blobs but i’m not sure if SHA1 is strong enough to deal with them uniquely like that. Boy that would certainly solve a lot of problems if that were true. Then ghosts become blobs, and the AFN->ghost tree is just a hash->hash pointer table. Maybe this is worth a look. Maybe we just experimentally see if we get hash collisions on small filenames and go from there.)

The dumbest way I can think to implement packfiles is that we have some table up front with a list of hashes and their offsets into the file. Git uses some kind of binary search, which seems reasonable. There might be some advantages to using git’s format directly. In any case, I think we’d want to support object revocation more explicitly. Or have overflow tables so the binary tree of hashes at the start doesn’t have to be known up front.

My version of packfiles has to support ghosts. If I have a ghost that has information about hash XXXX but isn’t actually that hash, how does it get stored and retrieved? One dumb way is to let the content NOT equal the hash pointing to it. Maybe throw a bitflag on the LUT to distinguish between a blob and a descriptor. Of course, that means that a single hash might actually have several different contents. 

Another way is that a packfile could be interleaved in some way.


# KVS and immutability

Is there any way to implement a key value store with immutable hash lookups?

https://en.wikipedia.org/wiki/Persistent_data_structure
https://www.ctl.io/developers/blog/post/immutability
https://joearms.github.io/2015/06/19/Mutable-Value_Chains.html

This kind of magic would actually enable some pretty neat behavior.

Packfiles support blob storage, but not naming. If I want to ask a packfile what it has with a particular name, I am SOL. 

here is a simple hypo. Say I have a file A with the trusted names of hashes in the format

	name -> [hash]

Now, I hash up that entire file A, and stick it in the database. Lets call that the HEAD. 

Now time passes, and I start to want to change the names. I write the new values that I want into a new file B, and I stick along a pointer to the old file A. So the rule is, go to head, see if you can find the thing you want, if not, got to the next object and try there. This could be easily multithreaded with a reduce operation.

If the names were self describing enough, i wouldn’t even need a pointer. You’d just carry an “effective date”.

I think there are libraries to write out immutable Btrees somewhere. There is also this constant database which is pretty popular (Bindings in perl, haskell, etc)

https://cr.yp.to/cdb.html

Btree is harder to find:

https://www.usenix.org/legacy/event/hotstorage11/tech/final_files/Twigg.pdf
https://www.perforce.com/blog/110928/short-history-btree
http://www.hpl.hp.com/techreports/2007/HPL-2007-193.pdf

A nice implementation

https://github.com/orodeh/bt
https://github.com/orodeh/rocksdb

Even a diagram!

http://www.bzero.se/ldapd/btree.html
https://www.usenix.org/legacy/event/lsf07/tech/rodeh.pdf

other b tree
http://fallabs.com/qdbm/plspex.html
http://fallabs.com/kyotocabinet/


Feels like the ideal data structure I want is basically a packfile architecutre for b-trees with COW. I think ZFS does this. I want to insert true K/V data (true = where the K != hash(value)) in a write-only mode and insert it as a immutable object when I’m done. Then I want primitives that let me open a new working file, store new K/V as COW, with pointers back to the old structures for the parts that haven’t changed.

Every so often, I’m happy to checkpoint, meaning that I go through and build a new tree that compacts out all of the old versions. 

Not really clear how optimal this needs to be. I have about 3M files in my collection. Thats 384MB of key values not using any compression. So this whole shebang could be about a few GB. 

Anyway such a structure would let btree information scatter from node to node efficiently and would provide a natural structure for partitioning.

The question I suppose is if it would outpreform simply tree based data structures and blobs in a packfile for the more typical workloads. For instance, what if we serialized ghosts with all of the relevant fields for each directory, and then created a merckle tree a-la git? That would work pretty well EXCEPT for the meta-information about verification. E.g. during checkpointing, massive updates to the lastStat would cause rewriting of the entire tree. UNLESS we had some mechanism for representing cascading updates. E.g. a deepLastStat, and if present on a parent, meant that all of the children were verified stats to that date. That would cut down on the rewrites enormously. Git sort of implicity has this -- a tree objects is taken to have its stats up to date simply because that is how git writes these objects out. This means you can represnt a checkpoint with a bit of data saying: “the oldest stat date of any object in this tree is X, and the oldest fullHash of any object int is tree y is Y” and seal that over all of the old ghosts.

This new idea could potentially be massively more efficient than the B-tree thing. 

But it would take some care and caution. When you pulled out a subpart of the tree to do something with it, that subtree doesn’t stand on its own anymore. You’d have to rewrite it on the spot after extracting it since its parent might override some of its fields.

Another idea is that lastHash and lastStat might just not be thigs you need to keep on an ongoing basis at the ghost level. Sure, you need each instance of GFS to have its own data like this. But the stuff that gets checkpointed and shared could just be a high level “warrant” object that says “hey, i checked the following things as of X date”.







# Volumes

The tricky thing about volumes is that they can be unmounted. And then a ton of ghosts for that volume become invalid. The question is if we maintain two ghosts for both the actual state of the filesystem, as well as the viritual state of the disk. It might be cleaner if we didn’t create fs: objects at all. Theoretically this is possible. if you ask for a fs style ghost underneath a path that is part of a volume, what if the TK actually just passed you back a vol: object? I wonder if frankly most of the code would even care. In cases when the volume is offline, the volume ghosts would (somehow) be immutable. They could filfill searches for the hash they point to. Another way to handle this is for volume LFNs to be fs: handlers, but the ghosts are marked in such a way that unmounting the volume does not cause any revocation. 


# special revocation rules


scenario to clean up 
mkdir bak
checkpoint bak
backup into bak
rm -rf bak
mkdir bak
NO Checkpointing
run abother backup: gfs backup run dir-a bak --stats

what's happening here is that bak has a checksum in the database from the last time it was enumerated. we do a directory scan that comes up with a different hash. When that occurs, we should revoke the contents on the directory. But right now we don’t, and when we call hasAcceptableEnum() the check fails because of the mismatch. 

reading the contents of the directory is akin to source verification in the new world order. But the current implementation doesn’t handle it that way -- it ALWAYS goes to the original.

We need to refactor the hasAcceptableEnumeration code to handle this idea that the directory state works differnetly now.

When you call getChildren or getChildrenLFNs, we should pull in children from the directory object, if it exists. This provides the answer to the LFNS (and thus the children.) If we cannot locate the directory object, we can keep the hash, but we must pass to source verify. If hasAcceptableEnumeration passes the date check and we have the file, we are essentially good to go, and we just return what we have. If for some reason, the date check isn’t good or the backing file is not loadable, we should scan the actual disk with readdir. At this point, a few different things could happen. First, the new contents could have a different hash, and we call “learnContentsChanged” on the directory. Incidentally, we’ll also discover deleted files and other such things which should propogate. Second situation: the contents could be the same, but we’ll up the enumeration date to the most recent read. In cases where the type of the file changes, we need to deal with this too (e.g. a directory has become a file, or a file has become a directory)

I think as part of this cleanup, we might want to think about a formal type tag sitting on the ghost. A stat has to be able to learn that a type tag has changed. learnStat should ALWAYS update the type tag to its correct value. And in cases where the type tag changes, propogate a change event.

Types:

f - file
x - deleted file
d - directory

In cases where a directory is now found to be a file, we need to learnDeleted on the directory. Curiously, i’m not sure that type is super important right nw. I could probably defer this bit until we actually have other types (like commits?)

and learn deleted on the directory may need to cascade down to revoke ghosts. Either that or we just wait for the usual checks for “getDuplicates” to cascade. But it cleaner if we do the dirty work. Not sure if that is strictly needed. If we revoke the deepScan, the user will get around to this eventually by checkpointing, etc. But what if the removal is discovered on a volume? Does checkpoint have a way of revoking all of those pesky hash->AFN things? Another way of handling could be with a deliberate flush: if you checkpoint X, you drop all objects formerly under that part of the tree from both tables. Basically a forced form of garbage collection.

If we had multiple hash->AFN lookup mechanisms, revocation could simply delete out the one that doesn’t exist, or mark it dead. So for instance, if each volume had its own hash->AFN LUT we could get away with that.




# The duality (and triality) problem of ghosts (6/22)

[This is a current description as of 6/22]

I like ghosts. But I’m starting to see the concept is a bit overloaded. 

Today the ghost represents the potential or current state of a file or path on disk, along with exact metadata to know how recent that picture is. It also can contain children. The ghost can either operate in a mode where it is a cache of a backing store (basically the underlying disk/filesystem) or it is frozen in time.

Therein is the contradictions. We’re using ghosts to fulfil many different purposes:

- Ensuring that we have fast, cached (but consistent within timebounds) access to current filesystem state inside the codebase that persists over time
- Storing “frozen” contents of directories (tree objects), in which case the ghost is super stripped down
- Handling multiple potential sources of truth about reality (e.g. a remote node) so that we can be eventually consistent. For instance, the ghost structure could let a sending node keep track of what it thinks has been written to a remote node, and then a remote node can “push” back updated state. The transactional nature of the ghosts means that many different truth-gathers can be active, and each arrive at a useful picture of what is where
- An abstraction layer around where something is - the ghost can be local, remote, S3, etc and the underlying code should be able to deal with it. There are no assumptions above the FSE layer about where a ghost is and how to retrieve it.

The ghost storage infrastucture (TK) handles two different problems for us:

- Obtaining the latest ghost for a given file on disk
- Finding all known ghosts with a particular hash

The challenge is that ghosts can be immutable (frozen), and their semantics inside the “always current” ghost world are hard. 

Here is a nasty corner case: Lets say we want to know if a backup performed long ago contained a particular hash. Where is the single source of truth about the past? 

Right now there really is none, we just sort of pretend that the ghosts are always current unless they are not, and then they are not managed by any central architecture.

I wonder if this domain concept saves us a bit from this. If we have a domain of “current”, the backing store is always the filesystem. But other domains could represent past state.

The problem is that domains will have a super high overlap of ghosts and other metadata. 

Another problem with ghosts is that there is no mutable state for the contents of their directories. This forces us to rely on enumeration from other sources (with verification from the checksum) not the ghost itself.

It seems philosophically perplexing that a ghost that can update at any time somehow connects-to and uses a directory listing that is effectively immutable. Its also really strange, architecturally at least, that the contents of directories don’t persist in the unless they are fully checkpointed (e.g. have valid hashes.) Maybe its a good thing. Its means we cannot have a definitive and immutable perspective of a directory unless its entire contents is fully enumerated and hashed.

So it means that an mutable ghost directory effectively “points” at an immutable managed object. And the second contents change underneath it, it looses its hash and effectively the link to its full enumeration. 

Again, I think this is all correct, but I’m trying to get comfortable with it.


# New concept: Domain (6/22)

A very prototypical idea. But what if we could define a “domain” which is a set of related blobs and metadata that we track and is persisted on disk? I’m thinking this is sort of like a packfile in git. The domain has primitives “AFNforHash()” and “GhostForAFN()”. 

Whats nice about this structure is that the system can know the data returned are versioned and specific inside the scope.

I’m thinking this would be the ideal backing store for a volume.

Is there a compact way to “version” a domain? E.g. some sort of copy on write mechanic? If each backup was a domain, we’d have a massive amount of overlap between them. 

You’d want to be able to fork a domain, and inside that new domain, it only writes new objects specific to that stream.

Its sort of like a multi-version keystore with at least one consistent index on an interior field (hash).

I’d also want some ability to store larger bits of data (a la packfile) inside the domain.

/foo/bar.txt + effective date X ---> Object
each object inside is basically a hash to another data structure that is compactable (e.g. some sort of extents mechanism.)

That means that some data may be revoked. E.g. once an entry is removed, it still needs and entry there to prevent from being recalled.

I think datomic does this.

I’d really want pointer reassignment to work well. I’m not sure I can pin down why exactly. But this idea that I have a master list of backups and that list may change is driving me nuts.

http://www.benhallbenhall.com/2013/06/version-data-keys-nosql-storage-environment/
http://stackoverflow.com/questions/22920240/what-nosql-database-categories-support-versioning








# How does Bup Handle splits?

have discovered that BUP does write out git compatable objects, but git has no provisions for joining up files.

	bup index . 
	bup save -n foo .
	cd ~/.bup  
	git --work-tree=/tmp/foo checkout foo
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/2ea61
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/3224c
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/34c24


 






# Hashes and interior objects

Lets say I have two directories A and B

	a/file1.txt
	b/file1.txt.gz

I want to do a test to see if all of A is contained in B?

Lets pretend we have a function called “canReconstructHash()” CRH. How would CRH know to avoid a backup? Theoretically, CRH should return true when file1.txt is examined against B.

So where would this data come from? Lets assume these checksums:

	a/file1.txt      [fc123]
	b/file1.txt.gz   [def00]

Lets take a very simple case.

	gfs contentscan b/file.txt.gz

The content scan pops open the gzip file, and checksums the thing it points to. Now our global AFN cache is aware that the checksum is present. The following AFN is generated

	split:gz|^def00:    [fc123]

This enters our cache, and now CRH will work.

So where do we persist this information? 

A parallel situation exists for directories. If I have a directory of hash 299932, how do I efficiently know if it can be reconstructed on B? Assuming my backup is checkpointed, the directory is now part of the AFN cache.

But lets say all of the files are there, but the directory itself is not on the remote side? It could be reconstructed. Is there an efficient way to test for this? The rule for CRH on a directory is that I either have to have the hash of the whole thing, or the hash of its blob like things (the rest I can construct if needed from the metadata.)

The second problem is manifest files. Right now, a backup operation presumably is going to write a file for every file and directory present in the backup set. I also think that the LFNs should be the file versions that were known back then. But this means that even very small changes to a large directory tree will always generate very large manifest files.

You’d ideally want some compact file format that can be quickly scanned to locate if a hash is in or out. And you’d want the manifest files to only generate entries for the objects not in the previous manifest file. Or maybe all of them not in a baselined file.

On restore, you simply reference all of the manifest files so that the missing hashes, etc can be recovered from the thin object manifests (the deltas.)

So in a naieve implementation, the backup system first checks that the baseline manifest on the server matches its own, and then does the diff, and sends over the smaller manifest file.

But pruning is going to suck a lot in this case. 

There is a certain logic to git making a “file” for each protected hash. Then the approach to pruning is simple. You write out a file of all of hashes of things you care about. And any hash inside that directory that doesn’t match, you get rid of. That purges both blobs and tree objects. A bloom filter might make this even easier.

I could do that with GFS. Every directory could have a small object file. It could even have the same checksum as the directory does. And then we know that any FILE or DIRECTORY with a given hash can reconstruct a directory of a given hash.

Another way is that we constantly rewrite the manifest file. There is some compaction.

In the case of tar metadata, we could have a directory object that is the contents of the tar file itself. Basically this file ties together the hash of the tarfile with the hashes of its components. It can be regenerated at will. Loading this file places the tarfile contents into the global cache.

But this needs some kind of way of finding the file, prefereably on demand.

What if we had a structure where the **filename** of the object was the hash of the thing it accounted for? Now at least, its really easy to search and doesn’t need to be necessarily penetrated (open+slurped) unless a question comes in to resolve it.

So if 100 files are needed to construct 1 file, you tkae the hash of that one file and make that the filename

But if 1 file expands to 100 files, you DO need to do some kind of deep content scan to pull those 100 files into your cache for the reverse lookup. (You wouldn’t want to have 100 files.)

Also not clear how this might tie into packfiles which seem so uber cool. 

And what if you want a file that has multiple entries (e.g. 100 files -> 1 file, 1000 files -> 1 file) so you don’t have so many opens and cloeses?

How do you find this object and manage it?

Another open question: What exactly is a backup? Is it a snapshot? Or a collection of file versions? Theoretically, it could just be a pointer to the hash of the top level nodes. 

We also need some way of handling volumes. We want the behavior that a copy of the contents of the volume are stored in durable places both on the volume and on a variety of other "archival" stores.


# East Cambridge Design 

The proper persistence behavior for directories is to flush a full directory listing to a disk as a file in the object store. The contents should be the file versions exactly as they were at that time.  Any time we call $g->flush(), this should happen.

Proliferation will be addressed with garbage collection.

ANy containered objects should do this, and that is the backing store for the ghost to get its contents in cases where the contents may no longer exist via readdir(). This case is most likely to occur when the ghosts are made immutable (as file versions.)

When you read a snapshot file for a restore, we need to make sure we create all of these little directory objects too. (Those objects can fortunately be detached from the snapshot and be first class objects.)

This hopefully helps avoid pulling the entire snapshot into memory.

This will create snap:2343df:a/b/c objects in your AFN cache.

Now you can do an ls

	gfs ls snap:foo.snap:a/b/c

And when you need the contents of c, you’ll have the hash. Now you look up that object.

You can imagine having some kind of really cool $ghost->reconstruct() thing that just figures out how to do it with a bunch of store operations.

Now here is the f’in wierd part. When a ghost is immutable, do we have to distinguish it better? The “c” that came out of the snapshot is a different “c” than a different snapshot? Even if their contents and modtimes are the same? I think the answer surprisingly is no. If we’ve commited in our signature for a directory that “c” of hash 2342c means a certain thing, it should ALWAYS mean that certain thing.

So how do we reconnect with UUID and sequence number later? Well, we’ve learned from the snapshot into the global AFN directory. But many of those files actually don’t exist.

So on restore, I think we create the new ghosts using the snapshot as a baseline ghost, and that will pick up this data.

Can we avoid many file accesses to get data for each directory? Several answers here. First, many of those file accesses will collapse when we get packfiles. Secondly, we could create a very simple additional database that maps hash -> AFNs of children. Or maybe AFN->AFNs of children might be even better. 

So when we read a snapshot file, we will be creating new directory objects now, not just files. We’ll check that the read contents match checksums. And then the persistance code will suck all of that stuff in. A snapshot is sort of an example of a deep tree persistence object. We will want similiar things for tar file enumerations.

So that further implies that we create a more generalized idea of a managed object.

	ManagedObject::DeepTree::Snapshot
	ManagedObject::Directory

Managed objects should know how to load from files and save to files.

I think this is better/cleaner than the ghoststore approach. But maybe we’ll merge or combined approaches. GS were sort of written with the idea of fully enumerated things and iterators, and now that my ghost architecture had containers, they are less important. My future snapshot writer needs something different.

	my $s = ManagedObject::DeepTree::Snapshot->new();
	$s->setRoot( $rootGhost );
	$s->readFromFile(); 
	$s->saveToFile( "foo/bar.txt" );
	$s->fsck()
	my $root = $s->getRoot();
	$root->getChildren();

So is a snapshot a source of truth? I think answer is no. I think instead it declares a namespace. Lets say that the snapshot file s.snap has hash [3ff39]. The LFNS are going to look like

	snap:s.snap:a/b/c

And the AFN is

	snap:3ff39:a/b/c

When you call get children on the LFN “snap:s.snap:a/b/c”, we’re going to have two strategies. First, the c object has a hash. We can easily look up that hash in our object store. This gets us the LFN names, and we use those to pull in new ghosts. We can also convert to AFN, and we’ll get the same hash. Its probably dangerous to actually pull in the ghosts from the file. 

now we have a new problem. ghosts that are part of snapshot are immutable. So we’re going to have to somehow figure out how you recall a immutable ghost. Its reasonable to think that all recalled versions of that AFN should be identical. We also have to make sure that the final ghost is marked as immutable, and only learns from immutable sources.

Basically, you want logic that learnFromGhost() will populate the immutable flag, preventing further learns. I’m not sure if the code does this. So it will need to.

Now we have a way of doing fast recursive LS of snapshots, where the recursive data will pull from the snapshot.



# South Cambridge Design 6/18

In the above design, more of the reconstruction dynamcis move to tree operations as opposed to flat lists of nodes. An interesting question will come up as we move to a world where reconstruction is based on ghosts. We will need to go through a list of ghosts in BFS order, calling things like this:

	let M be the ghost where were are testing for reconstructability
	foreach n: all nodes at this node level 
		m->canReconstructHash(n)

In the ideal case, M=N, and the tree doesn't have to decend any further, saving a massive amount of subtree comparisons. 

But when M !=N, we need to dig down deeper into M, cutting into smaller and smaller search trees. 

One clear approach is to have M be fully enumerated into some hash lookup table. Then the hash LUT just checks to be sure that the node is actually underneath M. In most cases, this can be verified by AFN comparisons. But not always. For instance, split files, or tar files will not support this easily. 

Git handles this using packfiles of closely related objects, and then scans them in MRU order. Not sure if this will universlaly work in our cases.


# Car dealership thoughts

Snapshots are important - they are how backups get saved and restored. I need to get them right in order to have full backup and restore functionality. 

A little stymied. What is a snaphot object exactly? I used to have a nice clean definition -- an addressable file type. E.g you ask for thing X, and it opens a file and gets it for you.

But now they are acting a lot more like pointers. Imagine a really clean backup that has nothing changed. It basically a date and the hash of the directory object. Of course, to make them more portable, you have to write out some associated metadata (potentially every ghost underneath.) But I see no reason to make each of these objects some special namespace to themselves.

They are sort of like commit objects now. Meaning they need to have their own little data store. 

To play this out, lets map it out a bit more. 

I backup a directory, and I get a signed new object in some namespace. 

	commit:93994234234

The name doesn't even really matter. It could just be the hash of that object. 

Does it get its own FSE handler? Maybe a good idea since you might want say restore a partial part of the tree. 

So this is the long term direction to head.

But it does raise some obnoxious issues. In writing the metadata store, we'll have the versions of the ghosts which may differ from their latest versions. Might be nice to rename them in some other namespace so we can address the older versions.

Maybe this is how we handle fileversion objects?

I need a fast clean decision for the initial implementation.

The simplest thing is that unpacking the metadata store just is equivlilent to learn operations on existing ghosts. So that's what we'll do.

Still some funky issues.. directories that come out of these tree things are technically immutable. We don't ever want them refreshing their contents from the central store.

so we have a contradiction. On the one hand, we want to persist all directories into the repository of objects. But when we load them as ghost, they are subject to refresh rules. Say i aske for the directory that represents a backup i want to restore. it needs to be handed to be as an immutable object. getting the children can only happen when i pull contents from the repository (or if I happen to have another directory with that hash laying around, but this really an edge case.) So getChildrenLFN on immutable needs different behavior. 

or maybe we just need a more generalized way of naming and retrieving immutale objects?

# more thoughts - a wierd implication of directories as "first class" objects

If i have an object like vol:foo:bar/two/three, what actually gets stored? I wonder if the idea of a sub directory should be universal between FSEs. So if directory X is moved to a volume, its hash should stay the same, right? That means that when you look to enumerate childred of an object, the children should inherit the FSE of the parent node. 

that then implies that a volume object is really just a pointer in the hash database to the top node of the directory plus some information on tagging. When an online operation occurs to a volume, we update the volume pointer to whatever is our newest expression of top-level state. 

then when we disconnect, lsdup and other actions will still work. Lets say i lsdup a file that is on a volume. I'll find a copy of that object in a few different AFNs. assuming i can identify at least one of those AFNs as a volume object, i'll recover it as immutable, and that will report back out to the duplication code.

The point I'm struggling with in this is that it means a ton of extra ghosts are going to persist around. If I back up a file 6 times, and it is on 2 volumues, does that mean I have
	
	1 ghost for the actual file
	2 ghosts for the actual file on the volumes
	2 ghosts for the immutable volume representation of the file
	6 ghosts for the snapshot

That feels like a shit-ton of metadata. Probably some need for cullung or garbage collection if that is actually the correct design. How is it incrementally recovered? What needs to stay?

Incremental recovery of the actual stuff isn't a problem. The ghosts for the immutable volume representation can be resolves from the ghost pointers from the top level volume node. I suppose the same is true for the snapshot. Fully enumerating the volume will recreate the intervening ghosts.

If we move to a more distributed store for finding ghosts for hashes, (and ghosts for AFNs) -- rather than having them all jammed into a BDB -- which I think we're going to have to do sooner or later, I think the design becomes simpler.

So my decision right now is to allow the ghost proliferation to occur, at least for the time being. We'll cound on GDB being super fast, and eventually we'll get this stuff compacted. Git shows up that compaction is possible.

# How does a split work in the East Cambridge design?

Another interesting question is what exactly is a split? We're representing the idea that 10 files construct 1 file. Lets start from the reconstruction case first. Say I'm told to recover a blob with checksum X. I first go into my AFN for Hash database. I'll get a bunch of fs: objects, but say none of them are available. I'll also get a split:[X] AFN. This AFN will have the hash of the file i'm trying to reconstruct. But its descriptor file will NOT have that same hash, unlike a directory.

SO now I need a level of logical mapping. I have to take that AFN split:[x] and i guess its got to point somehow using a mechanism i haveb't invented yet into a descriptor file. And inside that descriptor file i learn the ghosts for each of the splits. We also want that descriptor file to support many:many.

So on the split:[x] I call -> getChildren(), and we'll pull into that descriptor file and it will immutable, unflushable generate ghosts for the idea of the peices of the file i can then use. This file doesn't have to be very complex actually. It might just literally be a binary list of hashes. But I can build ghosts out of those, I think. 

One of the mechanisms to consider is maybe the split DOESN"T appear in the hash->AFN space. but I also know that i can use namespaces like snap:[HASH] as a path to reconsturcting something. Rule: if its a blob, and I can't find it in the hash->AFN space, also look for split:[X] objects.




# virtual directory problem

I'm backing up directory a/foo b/bar and c/tar.

Doing, this, I need to create a snapshot object representing this state. But there is no actual live directory that is the combination of these things. So I need some way to create a virtual directory.

Decision: directories are directories no matter what FSE generates them. For time being, i will use snap: and make sure enumeration follows the handler of the parent. This may be a hack I regret, so I'll have to change it later.


# Directories and "store"

If we are committed to directories as first class objects, we need to be able to fire "store" events at them to get them in other places. This is especially important with backups. But like many of my recent forays into “directores = 1st class”, it comes along with an unusual and frustrating set of coralaries that have unintuitive ramifications.

Consider this example: I have a directory dir-a that I want to store in another location. Some of the files in dir-a are already at that location somewhere. Some of the other files are not.

First, I look for dir-a’s hash in the remote location. Not seeing it, I should not just move dir-a over wholesale along with all of its contents. Instead,  I should mkdir at the remote location. ANd now consider, for each child, if each of them can be reconstructed on the other side. If they can, we skip. If they can’t, we copy that one over too.

But the confusing thing is that at the end of the day, I will not have actually copied over the directory physically on disk. I will have copied over enough that can-reconstruct will pass.

This is all sort of muddled in my brain. I can’t tell if its because these are new confusing thoughts, or if its because I’m loosing interest trying to solve these puzzles. I feel like there is a general purpose operation here that is eluding my brain and preventing me from coming up with a solid bulletproof abstraction for this.





 
 









# New semantics for directories (Ideas prior to east-cambridge design)

In an ideal backup scenario, I obtain a hash representing a particular directory. Primary source verification occurs by getting the hashes of all directories underneath. Therefore “has hash duplicate” still works. As long as the directory is EXACTLY the way it was, we are all good.

This model starts to break down when it comes to multiple snapshots or whatever you want to call them. 

In that case, I need to represent that having a copy of hash XYZ  is equivilent to having the TREE object for that directory, the tree object for all subdirectories, and every file referenced in between.

Similiar case for splits and merges. I can say I have a file if either:

- I have an exact has duplicate
- I have a metadata file saying that the file can be reconstructed with parts P1-Pn, and each of P1-Pn’s hashes can be located somewhere.
- I have a metadata object saying that the file is a split from some other object Q, and I can find and checksum Q.

canReconstructHash() is a semantic that could apply to any ghost or path. (I think.)

if I ask canReconstructHash() on a fs:: type object that is a FILE, the best I can do is see if that file is the hash. My answer is a definitive YES/NO.

if I ask canReconstructHash() on a fs:: directory object, I can answer YES if calling it on all of my children is a YES. If my enumeration date is within the guard time, I can answer definitive NO. Otherwise, its a PROVISIONAL NO.

lets say we have some understanding of an object with an object TYPE known. Say an object is just a file that starts with GFS magic.

if I ask if I ask canReconstructHash() on a object “XXXX”? It depends if i trust that object. Say I slurp in that object into some sort of provisional cache object. And say it passes whatever sort of validation I care to put on it. The object might refer to local or remotely verifiable things. canReconstructHash() can be called on each of those things.

if i have a ghost

	split:tar|tarfile_fc123)|a/b/c/ -> ab6004

than I can reconstruct XYZ as long as I have an object fc123. 

That means fc123->canReconstructHash( abs6004 ) should be true, just like a directory.

I can cache my answers. A->CRH(B) can be cached as a pathway of hashes. This is nice because it locates an answer but I can check it at any time, if needed, untaring the file, pulling the remote, etc.

what about a merge?

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002

This object preferably has the hash fc123 when queried. 

So how does this work? I have objects on disk whose SHA hashes will NOT match the ghosts the point to them. 

Two ways to handle this. Either i have a logical hash separate from a physical hash. Or I’m just cool with physical hashes being a lie for certain metadata objects that I maintain anyway. 

Actually, for tamper-proofing, maybe its worth considering a compound hash?

Or maybe we go back to the idea of pointers for this sort of thing.

The file listed here describes out a few files can be joined together to form fc123. For efficiency, we probably want flexibility to put mutliple files in this object

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002
	merge:origfile_cce99:xxx6003
	merge:origfile_cce99:xxx6004

call this a merge index file.  Say it has a hash of d99abc.

d99abc->canReconstruct(fc123) is TRUE as long as we can find abs6001 in other places.

It would be nice if splits could somehow store along with them the things they restore. But they can’t store pointers to each other -- that would be impossible to calculate a hash that incldues another hash in advance (assuming SHA stays unbroken)

so how do we know to find this merge file and learn how to reconstruct fc123?

I need somewhere to “learn” when I scan the merge index file, i generate these AFNS

	split:d99abc:fc123

This bit of information is always source reconstructable (amazingly enough), just by re-reading the merge file.

So mergers actually can be handled by storing splits.

Bizzare!

In theory, a directory is also a split

	directory [hash=ABC]
		file 1 [hash=01]  
		file 2 [hash=02]

When we make this into ghosts

	fs:root:directory        : ABC
	fs:root:directory/file1  : 01
	fs:root:directory/file2  : 02

However, if we go to a strict split syntx, its also storing

	split:ABC:01   -> 01
	split:ABC:02   -> 02

Lets get the terminology exactly right. 

Consider this LFN:

	split:X:path -> HASH 

This LFN stores the idea that if you split up the thing pointed to at X, using information in path() you can get to an object called HASH.

Consider the alternative

	merge:X:path -> HASH

This says that in order to get thing X, I need a peice at path(), and it should have hash HASH

In this case, [merge:X:path]->CRH(X) is false. But a merge index file FOO is assumed to be a complete set of mappings. Would be nice to also verify that at the ghost level. So the rule is that if we intake the whole chunk of the merge file, we’ll give ourselves permission to enter these new AFNS.

So the MIF needs some sort of magic keyword at the top. And we need some concept of “gives closure to”.  For instance, a gfs_ip file for a directory can be verified as complete, at least at a point in time, by seeing that the sum of the key hash information matches the directory. 

Maybe the MIF works the same way? Or maybe its more syntactically pure if we call it a SIF. 

	merge:origfile_fc123:CLOSURE -> hashOfSignatures(abs6001,abs6002)
	split:origfile_fc123:(1/2) -> abs6001
	split:origfile_fc123:(2/2) -> abs6002

This does actually seem a lot better and mirrors the way that a directory works.


A packfile is some special concept of a of a split with syntax to tell how to reconstruct something. 


  
I think I need to implement this whole thing the “slow” way before I can figure out how to do it the “fast” way

So step one is getting to the point where we have an easy way to invoke CRH. The recon could just print yes or no if target is sufficent or “contains” the source.

	gfs recon ^fd35 ^2dfse
	gfs recon source target

How do we automagically learn and absorb merge index files?

If you pass me obtainGhostByHash( ^fds34 ), I’ll check my truth sources. I’ll get several objects. One of them will be for the things in the filesystem. The second will be the thing for the merge: file.

The recon should support
- Finding a file inside of directory object
- Finding a file inside a nested directory object
- finding a subdirectory inside of a directory

TODO We need to deal with deletes better - if we realize something is deleted, its parent should have its hash cleared.

Source: a294d0c46d9768647f53e58fc69ebb0600c84236
several times got it.







# cache

WACKY IDEA- What if cache poisoning was resolved by LEARN ALWAYS taking the MOST conservative date? No. that would cause needless source verifications. But what about the case of enumeration? Taking the oldest would force full enumeration that should trickle back to all caches. And enumeration is cheap. 

is there a difference from a cache perspective of "latest playback".. e.g. a think that we learn that says - someone in the cache chain hasn't seen a playback of the full directory in XXX days?

OTOH, we can always ask individual truth keepers what their enumeration date is.

where i am right now is that hash checking is working well. But there needs to be some kind of checkpoint verb. and we need a basic restore mechanic.
lets get this all working so we can start to understand performance issues and think through fixes. 

we don't seem to have an acceptable forceEnumeration. The date is the OLDEST date that contribute to that tree. Need to construct a test schenario where this falls out of date, and then where a readdir scan reverifies it. That will actually fix 9/10 of the cache poisoning problem.

need some simple db verification tools. Make sure all AFNs from the hash cache are in the other thing and visca versa. 


# The ghost backup situation

I want to backup object X, which is a file. How do I know if it is present on the backup directory?

- Worse case: I traverse every node, looking for a hash duplicate. With caching, this actually may just require a relatively modest load
- Better case: I find some truth engine that supporst canReconstructHash().

CRH() dumb implementation is to select an entire swath of filenames using the B-Tree and insert them into the hash store every time. But how do we know all subchildren are loaded? I think it is safe to assume that in a consistent system, if they are in the AFN->Ghost store, they are also in the HASH->AFN store. We can create tools to do a consistent check for this (and probably should.) If we stored something on the parent that told us the size and number of files, we could do a fast iteration over the exploded tree, and verify that information. Presumably with cursors that could be made to be faster than hitting the database so many times.

A deeper scan aims for a full enumeration, but has to use ONLY the ghost data inside that store, so that gaps can be identified.

	GDB->verifyFullDIrectoryEnumeration( $g );

This implies there is some sort of flag-- when a whole block of things come over into a cache, I can mark them as authoritative and that effectively the entire cache is up to date.

This seal on the rows would be broken on insert. 

I don't need to be too worried about false-positives since those can be verified. But false negatives would suck a bit.

Maybe we push this back to the user.  We do some spot checks on a few directories at random. If they don't seem enumerated, we issue a warning and tell the user to run the checkpointing tools again.

A really advanced idea is to have a CRH cache. I bet there is some hyper-fast file format that could be used. Or maybe an in memory hash is not that bad? Perl can deal with really gnarly hash sizes and these checksums can be packed into a file for uber-fast lookup.

	# the problem of a cache of a cahce of this kind it hat the "full deal" is what matters.
	# it seems to take about 1.4 user seconds to enumerate 1.36M rows in BDB
	# that tells me we've got a lot of room to grow here

Seems like the best thing to do in V1 is assume "user must checkpoint" behavior.

Checkpointing requires a full stat, and a hash if the stat revoked our hash.
it also should make sure any objects described (snapshots, directories, tarfiles, whatever) are pulled into the cache.

reasonable to warn that the current target of the backup has not been checkpointed in XXX days. 












# File Versions - how should they work?

We need some way to address file versions. 

I can copy older versions of files by referring to tree state objects or by dates. 

	Cp a b

Copies file a to position b

	Cp a --state-before timespec

This looks for a copy of a that was the most recent before a certain date. 

We also need a concept of a snapshot where it's a set of file versions. And it's okay for them to be tied to an absolute afb. 

Also nice if the ghost points to that snapshot like isMemberOf 

That way even if the record of the snapshot is lost we are okay.

This means that learn may revoke the version which should clear out memberOf. Any change to core stat or hash should do this. 

If 5 files are needed to construct file x nice to have source afn. This could enable striping. 

So the original afn stands. But we gain 5 new split objects each pointing back to the hash they reconstruct. So this can actually live in the afn. But nice if splits carry along the ghost of the thing they reconstruct. 

Same logic for tar files and one to manies. 

The hash of tar file is the afn. 

But nice if we can store the path of the tar file. Wait. Why? We can reverse look that up. Hash to file names are assumed to be o(1) basically

So the split can be subclassed to handle par

And the merge can beb




# Lineage

GFS has a concept of files having a limited-use universal name. When a file is turned into a ghost, GFS tries to find a UUID for it. The UUID is a combination of the filename, size, modification date and inode plus some random data. When the file changes or moves, it will obviously get a new UUID. But if GFS can figure out where the file came from, it will mark an upstream UUID and an upstream checksum. This lets it understand lineage.

Say that a file ~/work-dir/jeremy.txt changes. GFS will look for the most recent ghost for that file. That recent ghost becomes the upstream ghosts.

This is how lshistory works. When you want the history of file X, it attempts to walk through a tree of upstream pointers, gathering the other file versions. 

When a file is backed up, it stores its original source LFN, creating a family of related files. It also flags the ghost of any file version to get saved along with.  

GFS doesn’t need this data to be correct to make backups, but it does need it to provide the most intelligble reports on the histories of files and directories. 

fields to add

UUID
upstreamUUID
lineageUUID
lineageHash
upstreamHash
effectiveDate

WAIT, we don’t actually need all of these.

ghost->assignUUID();

any time a ghost is saved with a change (effectiveDate increases), it could get dropped into the FVDB.

iside learn is a “triggerNewIdentity()” which roles over the UUID. But when is this called? cna’t think of a single case. if it has the same AFN, we’d always want it to pick up previous. 

during pre-save, any node without a lineage could get one. look in the FVDB for the same AFN. or same inode and size (later point). 

Only write to FVDB if has stat and hash, and if effective date has changed.

effective date is propogated when marking dirty. dirty also ups the sequence number.

learn should always take the max of the two sequence numbers, and absorb a UUID if one is not already assigned.

UUID should be assigned during pre-save.



fileversions dont’ learn. we just take them all together.

That means that AFN+effective_date is unique inside the FVDB. 
 
so.. should now a backed up thing should know what AFN it backed up??? 

It doesn’t need to. lshistory enumerates all ghosts with that AFN. that tells us the checksums.

as long as the FVDB entries get passed along to the backup location in a way they can be re-integrated, we’re fine.

and i think they are!! why? because the snapshot stored there could be a bunch of non:SNAP things. which owuld be implicit file versions.

so we got all of this with just effectiveDate, sequence # and UUID.

what if user clears .gfs
then it assings a new UUID. that might not match the older ones.
that is true. but a new operation could theoretically link them.  we can write this idea into a formerUUID field, which now creates a forward-looking link between the new UUID and the old one.

is there a generalization of this mechanism for splits and merges?

say 5 different versions of bigfile.PSD are stored in one packfile

or bigfile.PSD is split across 5 different files

maybe that creates a use for “upstream AFN, upstream UUID”??






in my ideal tool, i'd do the following

	gfs init-volume /              laptop-root
	gfs init-volume /Volumes/usb   usb1-spare

	mkdir ~/working-stuff
	add lots of things under here

Now, to "run" a backup...

	gfs push . 

	# Looks for things in laptop-root its never seen before, and then fishes around
	#for a place to stash them. 

	./working-stuff/a,b,c -> usb1-spare/backup-today-20160601

And i can check on replication status whenever I want...

	gfs check .

	15 files have stashed copies on usb1-spare (trusted, but currently offline)
	4 files are novel and unreplicated anywhere else

The key thing is that GFS stays flexible where and how these other things are
saved and stored. And it doesn't create symlinks or other messy things like
that.

GFS also tries to stay flexibile about how blobs are backed up. The lowest
common denominator of a backed up file is just a user-level copy of the file.



# 




# One to Many references

there is probably also a type of thing called an opener.. 

expand:foo/bar.gz:bar

The AFN for an opener is the hash of the object being openeed

expand:234fc234ffeeaab3234b:













So really base case, lets implement 

- singleton object
gfs init 
  - makes all of the directories and stuff
gfs store filex
  - exercises the storeGhost API (which can be hackist, no GST required.)
gfs obj-setname foo bar
  - exercises pointer logic
  



Side note: We need a way of defining a ghoststore that is "backed" by a file such that slurping is completely lazy and uses some sort of native indexing tool. Hate the idea that a million rows of a ghoststore might have to be read just to get some random value. 







# great idea here: (copypaste from BUP technical notes)


bup makes accommodations for the expected "worst-case" filesystem timestamp resolution -- currently one second; examples include VFAT, ext2, ext3, small ext4, etc. Since bup cannot know the filesystem timestamp resolution, and could be traversing multiple filesystems during any given run, it always assumes that the resolution may be no better than one second.

As a practical matter, this means that index updates are a bit imprecise, and so bup save may occasionally record filesystem changes that you didn't expect. That's because, during an index update, if bup encounters a path whose actual timestamps are more recent than one second before the update started, bup will set the index timestamps for that path (mtime and ctime) to exactly one second before the run, -- effectively capping those values.





# Compare mode

a new sematic i didn't realize
its not reconstruct
its stronger
it asks "is everything here exactly as it was in the snapshot?"


# thoughts on the absolute ghost cache (AGC)

## Problem 1

THe absolute cache is an interesting beast and has some peculuarities I need
to work through. First of all, how should it interact with .gfs_ip files? Say
I look for an ghost record and hit the AGC. There is no opportunity for state
to flow up from the .gfs_ip cache into the AGC. And if there is a dirty
record, we'll have to spontaneously make sure that the gfs_ip is loaded, which
itself may disrupt known state..

## Problem 2

When serializing to the AGC, we will write the filenames by asking the ghost
to represent them in an absolute way. There are at least two transformations
we can expect:

	1- The filenames will go to absolute
	2- snapshot roots will get converted to ^234abcdfef things

Now when those AGCs are retrieved, at what point do we "convert" them into
their local versions? Do we ever need to do this?

Lets say we leave them as absolutes. This will mean that anyone who calls
lfn() will see something they may not recognize. But the behavior of the
software may be correct anyway -- its not going to be fed "wrong" information,
just getting data in an unexpected form.

Maybe a better way to think about the AGC is that it is an overarching store
used to populate certain peices of information. If I want a ghost of ./a/test-
file1, I make it and it says local all of its life. But as a pre-loading step,
to "get me started", one source of truth about it is to copy data over from
the AGC. But we can do that with a bunch of "learn" optimizations. I can
g->learnFromOtherGhost(), and when I do that, I just check the AFNs are the
same. If they are, the data is safe to transfer over. The learning algorithim
will be smart to either keep or discard things.

My preference is to deal with .gfs_ip files at termination of the program, so 
lets let the dirty handler take that. 

SO we need really good learn semantics. Maybe we should write those down.

In the case of snapshot, I start with snap:foo:filea. Lets say I want to
operate on that file to do an LS. I'll make a ghost representing
snap:foo:filea. The AFN convert will see it as a ^234db pointer (forcing a
checksum of the file along the way, which is an separate thing), and therefore
we'll potentially hit the global cache, not even reading the file directly.

So decision stands: the AFNs will never directly enter the pool.






## Entries vs. ghosts

An entry is a thing that exists somewhere for the name. A ghost is a view
about state of an entry. There can be many conceptual ghosts pointing at the
same entry. When I call the truthkeeper, my request is to get the most recent
(or best) ghost for that entry. 











things that are not virtual are 'confirmed'.

they could be deleted, present, missing, etc

but to not be virtual, someone somewhere has to either deserialize them
from the past, or find them authoritatively on disk 

that way we don't contaminate the pool of ghosts

if the program that the user is asking to run cannot track down a confirmed ghost,
they can decide what to do (it's basically a file not found.)



# How do ghost work when they cross into the world of snapshots?

hah, interesting issue: what ghosts shpuld we pull from a snapshot? its
written as a bunch of fs:root: things... so  pulling them straight out of the
file contaminates the truth store.

but really, as a snapshot, we need to load it as virtualized objects, in their
world now, they are in a different name space

raises another question: boundary case -if you snapshot a snapshot, what
should happen? - i think the right thing is to either save a snapshot with
rewritten LFNs or to change the LFNs when loading

DECISION: I think its more correct to do on writing, e.g. -> convert  to snapshot LFNs
during the write operation

this requires new FSE semantics
should we think of a snapshot operation as a push into a filesystem?

YES

make a ghost for the snapshot-to-be
mkae FSE
FSE->addObject( $g );
	clone the ghost and re-write the LFN
	make them immutable
FSE->finalize();

we probably will need similiar semantics on object stores








# What is a ghost?

The basic object is a ghost. A ghost is a description of some object
discovered "for real" in the filesystem somewhere.

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash

my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information. It is
basically serialized ghosts which have very local basenames for paths. 

The ghoststore is an abstraction around a collection of ghosts there can be
different implementations the most basic one is just a hashing container that
is as lazy as possible.


# About LFNS

LFN - local file name - whatever is colloquial to the USER to
describe something. LFNs can never presume to be universally unique between
invocations of GFS because they may be relative paths or tags/references may
change. But the user's maximal intent must be captured in the LFN.

	for instance fs:a depends on your CWD
	for a different PWD, fs:a could refer to a different file

However, there are many different handlers.

	/foo
	fs:/foo
	snap:foo.snap:file123
	remote:jgilbert@auspice.net:file123

In this case, the "LFN" is a composite of a handler, a root and a path. The
user may provide some or all of these peices on the command line. We need to
be be able  to intelligently construct the rest.

LFN's can be fullqualified, which means fully specify the missing strings and
to canonicalized the path.

To determine if two LFNs are the same inside one running instance of GFS, the
only way to do it is to canonicalize that path portion both to remove . and
../ etc. Then you need you make sure to attach the ROOT and the HANDLER.

across instances, the only way is to convert to an AFN (absolute file name)
The AFN is a unique permanent name for the location

FQAFN - fully qualified AFN: handler:root:afn
FQLFN - fully qualified LFN: handler:root:path (can be local)

snapshots will generally represent LFNs, typically from the point of view of
the PWD but their power is that they should NOT be foreced to contain AFNs -
the snapshot could be  reconstructed anywhere and we'll leave it to the user
to either feed a snapshot the AFNs or the LFNS.

in the nested case, assume the following data is stored

	snap:testdir/snapshot.gfssnap:
	snap:testdir/snapshot.gfssnap:a
	snap:testdir/snapshot.gfssnap:b

in this case, the LFN of the snapshot file itself is testdir/snapshot.gfssnap

So LFNs are designed to hold three peices of information, and together forms a
unique namespace.















