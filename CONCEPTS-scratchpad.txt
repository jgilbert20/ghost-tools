
# What is this document

It is a collection of work in progress (or sometimes retired) ideas.. anything that is current has been moved to the full concept file. 

- [ ] directory size changes should NOT revoke the directory
- [ ] any creation of a new file should revoke parent hash, (call learnContentsChanged)
- [ ] revoke enum (and therefore LC) when file is added
- [ ] revoke enum during mkdirs
- [ ] Lower priority: wire in reads of directory caches to properly populate children, find some easy way to get LFNs for children fast




# Hashes and interior objects

Lets say I have two directories A and B

	a/file1.txt
	b/file1.txt.gz

I want to do a test to see if all of A is contained in B?

Lets pretend we have a function called “canReconstructHash()” CRH. How would CRH know to avoid a backup? Theoretically, CRH should return true when file1.txt is examined against B.

So where would this data come from? Lets assume these checksums:

	a/file1.txt      [fc123]
	b/file1.txt.gz   [def00]

Lets take a very simple case.

	gfs contentscan b/file.txt.gz

The content scan pops open the gzip file, and checksums the thing it points to. Now our global AFN cache is aware that the checksum is present. The folling AFN is generated

	split:gz|^def00:    [fc123]

This enters our cache, and now CRH will work.

So where do we persist this information? 

A parallel situation exists for directories. If I have a directory of hash 299932, how do I efficiently know if it can be reconstructed on B? Assuming my backup is checkpointed, the directory is now part of the AFN cache.

But lets say all of the files are there, but the directory itself is not on the remote side? It could be reconstructed. Is there an efficient way to test for this? The rule for CRH on a directory is that I either have to have the hash of the whole thing, or the hash of its blob like things (the rest I can construct if needed from the metadata.)

The second problem is manifest files. Right now, a backup operation presumably is going to write a file for every file and directory present in the backup set. I also think that the LFNs should be the file versions that were known back then. But this means that even very small changes to a large directory tree will always generate very large manifest files.

You’d ideally want some compact file format that can be quickly scanned to locate if a hash is in or out. And you’d want the manifest files to only generate entries for the objects not in the previous manifest file. Or maybe all of them not in a baselined file.

On restore, you simply reference all of the manifest files so that the missing hashes, etc can be recovered from the thin object manifests (the deltas.)

So in a naieve implementation, the backup system first checks that the baseline manifest on the server matches its own, and then does the diff, and sends over the smaller manifest file.

But pruning is going to suck a lot in this case. 

There is a certain logic to git making a “file” for each protected hash. Then the approach to pruning is simple. You write out a file of all of hashes of things you care about. And any hash inside that directory that doesn’t match, you get rid of. That purges both blobs and tree objects. A bloom filter might make this even easier.

I could do that with GFS. Every directory could have a small object file. It could even have the same checksum as the directory does. And then we know that any FILE or DIRECTORY with a given hash can reconstruct a directory of a given hash.

Another way is that we constantly rewrite the manifest file. There is some compaction.

In the case of tar metadata, we could have a directory object that is the contents of the tar file itself. Basically this file ties together the hash of the tarfile with the hashes of its components. It can be regenerated at will. Loading this file places the tarfile contents into the global cache.

But this needs some kind of way of finding the file, prefereably on demand.

What if we had a structure where the **filename** of the object was the hash of the thing it accounted for? Now at least, its really easy to search and doesn’t need to be necessarily penetrated (open+slurped) unless a question comes in to resolve it.

So if 100 files are needed to construct 1 file, you tkae the hash of that one file and make that the filename

But if 1 file expands to 100 files, you DO need to do some kind of deep content scan to pull those 100 files into your cache for the reverse lookup. (You wouldn’t want to have 100 files.)

Also not clear how this might tie into packfiles which seem so uber cool. 

And what if you want a file that has multiple entries (e.g. 100 files -> 1 file, 1000 files -> 1 file) so you don’t have so many opens and cloeses?

How do you find this object and manage it?

Another open question: What exactly is a backup? Is it a snapshot? Or a collection of file versions? Theoretically, it could just be a pointer to the has of the top level nodes. 
 









# New semantics for directories (slightly different design)

In an ideal backup scenario, I obtain a hash representing a particular directory. Primary source verification occurs by getting the hashes of all directories underneath. Therefore “has hash duplicate” still works. As long as the directory is EXACTLY the way it was, we are all good.

This model starts to break down when it comes to multiple snapshots or whatever you want to call them. 

In that case, I need to represent that having a copy of hash XYZ  is equivilent to having the TREE object for that directory, the tree object for all subdirectories, and every file referenced in between.

Similiar case for splits and merges. I can say I have a file if either:

- I have an exact has duplicate
- I have a metadata file saying that the file can be reconstructed with parts P1-Pn, and each of P1-Pn’s hashes can be located somewhere.
- I have a metadata object saying that the file is a split from some other object Q, and I can find and checksum Q.

canReconstructHash() is a semantic that could apply to any ghost or path. (I think.)

if I ask canReconstructHash() on a fs:: type object that is a FILE, the best I can do is see if that file is the hash. My answer is a definitive YES/NO.

if I ask canReconstructHash() on a fs:: directory object, I can answer YES if calling it on all of my children is a YES. If my enumeration date is within the guard time, I can answer definitive NO. Otherwise, its a PROVISIONAL NO.

lets say we have some understanding of an object with an object TYPE known. Say an object is just a file that starts with GFS magic.

if I ask if I ask canReconstructHash() on a object “XXXX”? It depends if i trust that object. Say I slurp in that object into some sort of provisional cache object. And say it passes whatever sort of validation I care to put on it. The object might refer to local or remotely verifiable things. canReconstructHash() can be called on each of those things.

if i have a ghost

	split:tar|tarfile_fc123)|a/b/c/ -> ab6004

than I can reconstruct XYZ as long as I have an object fc123. 

That means fc123->canReconstructHash( abs6004 ) should be true, just like a directory.

I can cache my answers. A->CRH(B) can be cached as a pathway of hashes. This is nice because it locates an answer but I can check it at any time, if needed, untaring the file, pulling the remote, etc.

what about a merge?

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002

This object preferably has the hash fc123 when queried. 

So how does this work? I have objects on disk whose SHA hashes will NOT match the ghosts the point to them. 

Two ways to handle this. Either i have a logical hash separate from a physical hash. Or I’m just cool with physical hashes being a lie for certain metadata objects that I maintain anyway. 

Actually, for tamper-proofing, maybe its work considering a compound hash?

Or maybe we go back to the idea of pointers for this sort of thing.

The file listed here describes out a few files can be joined together to form fc123. For efficiency, we probably want flexibility to put mutliple files in this object

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002
	merge:origfile_cce99:xxx6003
	merge:origfile_cce99:xxx6004

call this a merge index file.  Say it has a hash of d99abc.

d99abc->canReconstruct(fc123) is TRUE as long as we can find abs6001 in other places.

It would be nice if splits could somehow store along with them the things they restore. But they can’t store pointers to each other -- that would be impossible to calculate a hash that incldues another hash in advance (assuming SHA stays unbroken)

so how do we know to find this merge file and learn how to reconstruct fc123?

I need somewhere to “learn” when I scan the merge index file, i generate these AFNS

	split:d99abc:fc123

This bit of information is always source reconstructable (amazingly enough), just by re-reading the merge file.

So mergers actually can be handled by storing splits.

Bizzare!

In theory, a directory is also a split

	directory [hash=ABC]
		file 1 [hash=01]  
		file 2 [hash=02]

When we make this into ghosts

	fs:root:directory        : ABC
	fs:root:directory/file1  : 01
	fs:root:directory/file2  : 02

However, if we go to a strict split syntx, its also storing

	split:ABC:01   -> 01
	split:ABC:02   -> 02

Lets get the terminology exactly right. 

Consider this LFN:

	split:X:path -> HASH 

This LFN stores the idea that if you split up the thing pointed to at X, using information in path() you can get to an object called HASH.

Consider the alternative

	merge:X:path -> HASH

This says that in order to get thing X, I need a peice at path(), and it should have hash HASH

In this case, [merge:X:path]->CRH(X) is false. But a merge index file FOO is assumed to be a complete set of mappings. Would be nice to also verify that at the ghost level. So the rule is that if we intake the whole chunk of the merge file, we’ll give ourselves permission to enter these new AFNS.

So the MIF needs some sort of magic keyword at the top. And we need some concept of “gives closure to”.  For instance, a gfs_ip file for a directory can be verified as complete, at least at a point in time, by seeing that the sum of the key hash information matches the directory. 

Maybe the MIF works the same way? Or maybe its more syntactically pure if we call it a SIF. 

	merge:origfile_fc123:CLOSURE -> hashOfSignatures(abs6001,abs6002)
	split:origfile_fc123:(1/2) -> abs6001
	split:origfile_fc123:(2/2) -> abs6002

This does actually seem a lot better and mirrors the way that a directory works.


A packfile is some special concept of a of a split with syntax to tell how to reconstruct something. 


  
I think I need to implement this whole thing the “slow” way before I can figure out how to do it the “fast” way

So step one is getting to the point where we have an easy way to invoke CRH. The recon could just print yes or no if target is sufficent or “contains” the source.

	gfs recon ^fd35 ^2dfse
	gfs recon source target

How do we automagically learn and absorb merge index files?

If you pass me obtainGhostByHash( ^fds34 ), I’ll check my truth sources. I’ll get several objects. One of them will be for the things in the filesystem. The second will be the thing for the merge: file.

The recon should support
- Finding a file inside of directory object
- Finding a file inside a nested directory object
- finding a subdirectory inside of a directory

Okay, so version one of this 

TODO We need to deal with deletes better - if we realize something is deleted, its parent should have its hash cleared.

Source: a294d0c46d9768647f53e58fc69ebb0600c84236
several times got it.







# cache

WACKY IDEA- What if cache poisoning was resolved by LEARN ALWAYS taking the MOST conservative date? No. that would cause needless source verifications. But what about the case of enumeration? Taking the oldest would force full enumeration that should trickle back to all caches. And enumeration is cheap. 

is there a difference from a cache perspective of "latest playback".. e.g. a think that we learn that says - someone in the cache chain hasn't seen a playback of the full directory in XXX days?

OTOH, we can always ask individual truth keepers what their enumeration date is.

where i am right now is that hash checking is working well. But there needs to be some kind of checkpoint verb. and we need a basic restore mechanic.
lets get this all working so we can start to understand performance issues and think through fixes. 

we don't seem to have an acceptable forceEnumeration. The date is the OLDEST date that contribute to that tree. Need to construct a test schenario where this falls out of date, and then where a readdir scan reverifies it. That will actually fix 9/10 of the cache poisoning problem.

need some simple db verification tools. Make sure all AFNs from the hash cache are in the other thing and visca versa. 


# The ghost backup situation

I want to backup object X, which is a file. How do I know if it is present on the backup directory?

- Worse case: I traverse every node, looking for a hash duplicate. With caching, this actually may just require a relatively modest load
- Better case: I find some truth engine that supporst canReconstructHash().

CRH() dumb implementation is to select an entire swath of filenames using the B-Tree and insert them into the hash store every time. But how do we know all subchildren are loaded? I think it is safe to assume that in a consistent system, if they are in the AFN->Ghost store, they are also in the HASH->AFN store. We can create tools to do a consistent check for this (and probably should.) If we stored something on the parent that told us the size and number of files, we could do a fast iteration over the exploded tree, and verify that information. Presumably with cursors that could be made to be faster than hitting the database so many times.

A deeper scan aims for a full enumeration, but has to use ONLY the ghost data inside that store, so that gaps can be identified.

	GDB->verifyFullDIrectoryEnumeration( $g );

This implies there is some sort of flag-- when a whole block of things come over into a cache, I can mark them as authoritative and that effectively the entire cache is up to date.

This seal on the rows would be broken on insert. 

I don't need to be too worried about false-positives since those can be verified. But false negatives would suck a bit.

Maybe we push this back to the user.  We do some spot checks on a few directories at random. If they don't seem enumerated, we issue a warning and tell the user to run the checkpointing tools again.

A really advanced idea is to have a CRH cache. I bet there is some hyper-fast file format that could be used. Or maybe an in memory hash is not that bad? Perl can deal with really gnarly hash sizes and these checksums can be packed into a file for uber-fast lookup.

	# the problem of a cache of a cahce of this kind it hat the "full deal" is what matters.
	# it seems to take about 1.4 user seconds to enumerate 1.36M rows in BDB
	# that tells me we've got a lot of room to grow here

Seems like the best thing to do in V1 is assume "user must checkpoint" behavior.

Checkpointing requires a full stat, and a hash if the stat revoked our hash.
it also should make sure any objects described (snapshots, directories, tarfiles, whatever) are pulled into the cache.

reasonable to warn that the current target of the backup has not been checkpointed in XXX days. 






# Original notes from duplicateUnderPath

We have to decide at this point if the universal cache is up to date. 
Each FSE might handle this differently.

I think the check would recurively look at subdirectories? Or maybe just
simple time based? I'd like to find some way to prevent the need for the
user to be smart enough to know that a checkpoint is needed.

One way to handle this is there is some kind of
"hasAcceptableDirscan()". If dirscan is popualted for an AFN, we could avoid
the full traversal. But an acceptable dirscan on the .gfs_ip doesn't mean the
one inside the cache is up to date.

you'd want two fields (most recent enumer, and oldest enum.) 

maybe there is a recursive way. I can "test my own" understanding.
first, i get the GFS_IP for the current directory
its going to have some stats in it about sizes and file counts and whatnot.
and i compare that to my own db enumeration.

BDB may or may not be quick enough to allow this..?
but doesn't its Btree have some sort of "# records between?"
that would be optimal
that is the check?

the problem of a cache of a cahce of this kind it hat the "full deal" is what matters.
it seems to take about 1.4 user seconds to enumerate 1.36M rows in BDB
that tells me we've got a lot of room to grow here








# TruthKeeper evolved

You wnat multiple directories of ghost information -
	e.g. a backing store using BDB, .gfs_ip

THe truthkeeper ALWAYS makes the ghost first during obtain. Then state is gathered from the others each in turn. 

When a ->save() is issued, the ghost is dropped out of the caches,
and the ->saveGhost() message is given to each GDB

We need some mechanism to be sure the ghosts are actually finalized on save, otherwise operations that create millions of ghost are going to drive crazy amounts of memory use.

Ghost act like a transaction, and you can play them forward.

obtainGhostForLFN
obtainFomerGhostForLFN( priorToDate );
obtainFomerGhostForLFN( versionSpec );

We can also ask

obtainGhostsUnderLFN( LFN )
canObtainGhostsUnderLFN()
	- checks if the cache is fully populated
obtainGhostsForHash()




# bup - had many of the same ideas as JG! but more nicely done, espciecially around indexing and splits

https://raw.githubusercontent.com/bup/bup/master/DESIGN


# Backup tool issues

http://burp.grke.org/images/burp2-report.pdf

# attric denmo

https://debian-administration.org/article/712/An_introduction_to_the_attic_backup_program



# notes on filename compression


http://codegolf.stackexchange.com/questions/4771/text-compression-and-decompression-nevermore

http://www.mit.edu/afs.new/athena/astaff/source/src-9.3/third/findutils/locate/frcode.cache


# packfile perl

https://github.com/jacquesg/p5-Git-Raw/tree/master/xs




# Notes on easy native C interfaces

http://www.thegeekstuff.com/2012/03/swig-perl-examples/


gcc -fpic -c -Dbool=char -I/System/Library/Perl/5.18/darwin-thread-multi-2level/CORE  area_wrap.c area.c -D_GNU_SOURCE

gcc -shared area.o area_wrap.o -o area.so


	gcc -c `perl -MConfig -e 'print join(" ", @Config{qw(ccflags optimize cccdlflags)}, "-I$Config{archlib}/CORE")'`  area.c area_wrap.cache

http://www.swig.org/tutorial.html

- replace .so with dylib

	gcc `perl -MConfig -e 'print $Config{lddlflags}'` area.o area_wrap.o -o area.dylib

Works

	#!/usr/bin/perl
	use strict;
	use warnings;
	use area;
	my $area_of_cir = area::area_of_circle(5);
	my $area_of_squ = area::area_of_square(5);
	print "Area of Circle: $area_of_cir\n";
	print "Area of Square: $area_of_squ\n";
	print "$area::pi\n";



# File Versions - how should they work?

We need some way to address file versions. 

I can copy older versions of files by referring to tree state objects or by dates. 

	Cp a b

Copies file a to position b

	Cp a --state-before timespec

This looks for a copy of a that was the most recent before a certain date. 

We also need a concept of a snapshot where it's a set of file versions. And it's okay for them to be tied to an absolute afb. 

Also nice if the ghost points to that snapshot like isMemberOf 

That way even if the record of the snapshot is lost we are okay.

This means that learn may revoke the version which should clear out memberOf. Any change to core stat or hash should do this. 

If 5 files are needed to construct file x nice to have source afn. This could enable striping. 

So the original afn stands. But we gain 5 new split objects each pointing back to the hash they reconstruct. So this can actually live in the afn. But nice if splits carry along the ghost of the thing they reconstruct. 

Same logic for tar files and one to manies. 

The hash of tar file is the afn. 

But nice if we can store the path of the tar file. Wait. Why? We can reverse look that up. Hash to file names are assumed to be o(1) basically

So the split can be subclassed to handle par

And the merge can beb




# Lineage

GFS has a concept of files having a limited-use universal name. When a file is turned into a ghost, GFS tries to find a UUID for it. The UUID is a combination of the filename, size, modification date and inode plus some random data. When the file changes or moves, it will obviously get a new UUID. But if GFS can figure out where the file came from, it will mark an upstream UUID and an upstream checksum. This lets it understand lineage.

Say that a file ~/work-dir/jeremy.txt changes. GFS will look for the most recent ghost for that file. That recent ghost becomes the upstream ghosts.

This is how lshistory works. When you want the history of file X, it attempts to walk through a tree of upstream pointers, gathering the other file versions. 

When a file is backed up, it stores its original source LFN, creating a family of related files. It also flags the ghost of any file version to get saved along with.  

GFS doesn’t need this data to be correct to make backups, but it does need it to provide the most intelligble reports on the histories of files and directories. 

fields to add

UUID
upstreamUUID
lineageUUID
lineageHash
upstreamHash
effectiveDate

WAIT, we don’t actually need all of these.

ghost->assignUUID();

any time a ghost is saved with a change (effectiveDate increases), it could get dropped into the FVDB.

iside learn is a “triggerNewIdentity()” which roles over the UUID. But when is this called? cna’t think of a single case. if it has the same AFN, we’d always want it to pick up previous. 

during pre-save, any node without a lineage could get one. look in the FVDB for the same AFN. or same inode and size (later point). 

Only write to FVDB if has stat and hash, and if effective date has changed.

effective date is propogated when marking dirty. dirty also ups the sequence number.

learn should always take the max of the two sequence numbers, and absorb a UUID if one is not already assigned.

UUID should be assigned during pre-save.



fileversions dont’ learn. we just take them all together.

That means that AFN+effective_date is unique inside the FVDB. 
 
so.. should now a backed up thing should know what AFN it backed up??? 

It doesn’t need to. lshistory enumerates all ghosts with that AFN. that tells us the checksums.

as long as the FVDB entries get passed along to the backup location in a way they can be re-integrated, we’re fine.

and i think they are!! why? because the snapshot stored there could be a bunch of non:SNAP things. which owuld be implicit file versions.

so we got all of this with just effectiveDate, sequence # and UUID.

what if user clears .gfs
then it assings a new UUID. that might not match the older ones.
that is true. but a new operation could theoretically link them.  we can write this idea into a formerUUID field, which now creates a forward-looking link between the new UUID and the old one.

is there a generalization of this mechanism for splits and merges?

say 5 different versions of bigfile.PSD are stored in one packfile

or bigfile.PSD is split across 5 different files

maybe that creates a use for “upstream AFN, upstream UUID”??






in my ideal tool, i'd do the following

	gfs init-volume /              laptop-root
	gfs init-volume /Volumes/usb   usb1-spare

	mkdir ~/working-stuff
	add lots of things under here

Now, to "run" a backup...

	gfs push . 

	# Looks for things in laptop-root its never seen before, and then fishes around
	#for a place to stash them. 

	./working-stuff/a,b,c -> usb1-spare/backup-today-20160601

And i can check on replication status whenever I want...

	gfs check .

	15 files have stashed copies on usb1-spare (trusted, but currently offline)
	4 files are novel and unreplicated anywhere else

The key thing is that GFS stays flexible where and how these other things are
saved and stored. And it doesn't create symlinks or other messy things like
that.

GFS also tries to stay flexibile about how blobs are backed up. The lowest
common denominator of a backed up file is just a user-level copy of the file.



# 




# One to Many references

there is probably also a type of thing called an opener.. 

expand:foo/bar.gz:bar

The AFN for an opener is the hash of the object being openeed

expand:234fc234ffeeaab3234b:













So really base case, lets implement 

- singleton object
gfs init 
  - makes all of the directories and stuff
gfs store filex
  - exercises the storeGhost API (which can be hackist, no GST required.)
gfs obj-setname foo bar
  - exercises pointer logic
  



Side note: We need a way of defining a ghoststore that is "backed" by a file such that slurping is completely lazy and uses some sort of native indexing tool. Hate the idea that a million rows of a ghoststore might have to be read just to get some random value. 







# great idea here: (copypaste from BUP technical notes)


bup makes accommodations for the expected "worst-case" filesystem timestamp resolution -- currently one second; examples include VFAT, ext2, ext3, small ext4, etc. Since bup cannot know the filesystem timestamp resolution, and could be traversing multiple filesystems during any given run, it always assumes that the resolution may be no better than one second.

As a practical matter, this means that index updates are a bit imprecise, and so bup save may occasionally record filesystem changes that you didn't expect. That's because, during an index update, if bup encounters a path whose actual timestamps are more recent than one second before the update started, bup will set the index timestamps for that path (mtime and ctime) to exactly one second before the run, -- effectively capping those values.





# Compare mode

a new sematic i didn't realize
its not reconstruct
its stronger
it asks "is everything here exactly as it was in the snapshot?"


# Singleton nature

Its canonical that the LFN of a ghost once created must never change.

if you need to reference a new thing, make a new ghost


# determinis and order

ghost iterators now sort their contents why? well, turns out there is some
non-deterministic behavior going on depending on how the values get scrabled
into the cache and i need consistency. ideally though, we need a way of
iterating through the ghost store in an order similiar to the one the user
intendend.

# too much caching

there is a good reason to really restat the entire destination directory
before developing a workplan otherwise operations will start to go wrong when
things aren't where expected. I think its fair to not rely on caching 
when you are actually about to write new stuff.. 

do this at the start, don't wait for some forced cache



now another thing has occured to me -what do we do about FSE proliferation?

technically, more than one FSE can be operative on a local file system at a time.

does it matter who or what finalizes these things?

we need to recheck the LSDUP semantics and make sure getFSE() isn't going to get 
completely confused. my guess is that because the ::FS is so thin, it probably doesn't 
matter.  it stores very little state relateive to the ::Snapshiot

but this is an issue when it comes to moving the ghost descriptor code. 

i think we need to make sure that all fs:root ghosts always get the global FS when they ask questions about themselves. And I think this is what the code does now.


# thoughts on the absolute ghost cache (AGC)

## Problem 1

THe absolute cache is an interesting beast and has some peculuarities I need
to work through. First of all, how should it interact with .gfs_ip files? Say
I look for an ghost record and hit the AGC. There is no opportunity for state
to flow up from the .gfs_ip cache into the AGC. And if there is a dirty
record, we'll have to spontaneously make sure that the gfs_ip is loaded, which
itself may disrupt known state..

## Problem 2

When serializing to the AGC, we will write the filenames by asking the ghost
to represent them in an absolute way. There are at least two transformations
we can expect:

	1- The filenames will go to absolute
	2- snapshot roots will get converted to ^234abcdfef things

Now when those AGCs are retrieved, at what point do we "convert" them into
their local versions? Do we ever need to do this?

Lets say we leave them as absolutes. This will mean that anyone who calls
lfn() will see something they may not recognize. But the behavior of the
software may be correct anyway -- its not going to be fed "wrong" information,
just getting data in an unexpected form.

Maybe a better way to think about the AGC is that it is an overarching store
used to populate certain peices of information. If I want a ghost of ./a/test-
file1, I make it and it says local all of its life. But as a pre-loading step,
to "get me started", one source of truth about it is to copy data over from
the AGC. But we can do that with a bunch of "learn" optimizations. I can
g->learnFromOtherGhost(), and when I do that, I just check the AFNs are the
same. If they are, the data is safe to transfer over. The learning algorithim
will be smart to either keep or discard things.

My preference is to deal with .gfs_ip files at termination of the program, so 
lets let the dirty handler take that. 

SO we need really good learn semantics. Maybe we should write those down.

In the case of snapshot, I start with snap:foo:filea. Lets say I want to
operate on that file to do an LS. I'll make a ghost representing
snap:foo:filea. The AFN convert will see it as a ^234db pointer (forcing a
checksum of the file along the way, which is an separate thing), and therefore
we'll potentially hit the global cache, not even reading the file directly.

So decision stands: the AFNs will never directly enter the pool.






## Entries vs. ghosts

An entry is a thing that exists somewhere for the name. A ghost is a view
about state of an entry. There can be many conceptual ghosts pointing at the
same entry. When I call the truthkeeper, my request is to get the most recent
(or best) ghost for that entry. 











things that are not virtual are 'confirmed'.

they could be deleted, present, missing, etc

but to not be virtual, someone somewhere has to either deserialize them
from the past, or find them authoritatively on disk 

that way we don't contaminate the pool of ghosts

if the program that the user is asking to run cannot track down a confirmed ghost,
they can decide what to do (it's basically a file not found.)



# How do ghost work when they cross into the world of snapshots?

hah, interesting issue: what ghosts shpuld we pull from a snapshot? its
written as a bunch of fs:root: things... so  pulling them straight out of the
file contaminates the truth store.

but really, as a snapshot, we need to load it as virtualized objects, in their
world now, they are in a different name space

raises another question: boundary case -if you snapshot a snapshot, what
should happen? - i think the right thing is to either save a snapshot with
rewritten LFNs or to change the LFNs when loading

DECISION: I think its more correct to do on writing, e.g. -> convert  to snapshot LFNs
during the write operation

this requires new FSE semantics
should we think of a snapshot operation as a push into a filesystem?

YES

make a ghost for the snapshot-to-be
mkae FSE
FSE->addObject( $g );
	clone the ghost and re-write the LFN
	make them immutable
FSE->finalize();

we probably will need similiar semantics on object stores








# What is a ghost?

The basic object is a ghost. A ghost is a description of some object
discovered "for real" in the filesystem somewhere.

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash

my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information. It is
basically serialized ghosts which have very local basenames for paths. 

The ghoststore is an abstraction around a collection of ghosts there can be
different implementations the most basic one is just a hashing container that
is as lazy as possible.


# About LFNS

LFN - local file name - whatever is colloquial to the USER to
describe something. LFNs can never presume to be universally unique between
invocations of GFS because they may be relative paths or tags/references may
change. But the user's maximal intent must be captured in the LFN.

	for instance fs:a depends on your CWD
	for a different PWD, fs:a could refer to a different file

However, there are many different handlers.

	/foo
	fs:/foo
	snap:foo.snap:file123
	remote:jgilbert@auspice.net:file123

In this case, the "LFN" is a composite of a handler, a root and a path. The
user may provide some or all of these peices on the command line. We need to
be be able  to intelligently construct the rest.

LFN's can be fullqualified, which means fully specify the missing strings and
to canonicalized the path.

To determine if two LFNs are the same inside one running instance of GFS, the
only way to do it is to canonicalize that path portion both to remove . and
../ etc. Then you need you make sure to attach the ROOT and the HANDLER.

across instances, the only way is to convert to an AFN (absolute file name)
The AFN is a unique permanent name for the location

FQAFN - fully qualified AFN: handler:root:afn
FQLFN - fully qualified LFN: handler:root:path (can be local)

snapshots will generally represent LFNs, typically from the point of view of
the PWD but their power is that they should NOT be foreced to contain AFNs -
the snapshot could be  reconstructed anywhere and we'll leave it to the user
to either feed a snapshot the AFNs or the LFNS.

in the nested case, assume the following data is stored

	snap:testdir/snapshot.gfssnap:
	snap:testdir/snapshot.gfssnap:a
	snap:testdir/snapshot.gfssnap:b

in this case, the LFN of the snapshot file itself is testdir/snapshot.gfssnap

So LFNs are designed to hold three peices of information, and together forms a
unique namespace.


# What is truth? What is the single source of truth?

Ghost is a description of the potential state of a file.

Currently, contracts look like this

- No obligation for a specific ghost to be up to date
- Only the ghosts from the TK are guarnteed to be single source of truth
- Ghosts find their FSE by looking at their handler value
- Ghosts can be in any number of ghost stores
- TruthKeeper should make all ghosts that are intended to hold present state
- The only reason TK is not used to make ghosts is in cases where you are deliverably
- TK holds all ghost pointers and knows if they are dirty or not
- Generally, the FSE doesn't hold links back to ghosts except in cases where a ghost is a root (like a snapshot)


# design ideas for object stores
object store concepts

a blob always has the hash of the file it saves - the header (everything up to the first null) can be anything


# Revocation rules and timing


we need to not be in a situation where the fullhash has changed but
the stat stays the same
scenario - two ghosts pointing to same file
ghost A has Ha and Sa (hash and stat)
ghost B has Hb and Sa

if Hb is later that Ha, we'd be confused - how could both stats be identical but the hash has changed?
contract should be
getStat
getHash
getStat

if stat is the same both times, than take the timestamp and report it for both

also implies the "learn" semantic should always be accompanied by a date
and not generated inside the routines'

(merge case)

A new stat does not mean that a hash is out of date. The stat date should be used for resolving which of two stats is most accurate. 

stat fields should be taken all or nothing. 

The only cause for revoking a hash is when the size has changed or last modification date has changed. 

The revocation should not be immediate. It can be lazy. Maybe an optional warning. But the next time anyone asks for a hash on that file, we'd be forced to get one. 

Ghosts should never revoke other ghosts. Let that semantic rest with truth keeper. TK will just ask for incorporation in direction it wants. 

If you ask for a ghost that has no authoritative record is should be marked as virtual. 

One sanity check is that the virtual ghost should not be written to disk under normal cases. 




+++++++++++





+++++++++++++++++


seems like the best separation of concerns is a new idea:

FSE is really about the interface to a collection of files
TruthKeeper is the module that generates ghosts
it also tracks when ghost records are stale and may need to be pushed out to new places


+++++++++++++++++




Definitions:

	Source: A place to take things from, but must remain untouched
	Auxiliary|Pillage: A place we don't give a shit about and okay to move things

+++++




Thats sort of annoying, right? 

Everything is a ghost refactor. In this design, ghosts are interachanable with filenames
and FSE is really stripped down

M: main
FSE: Filesystem manager
	getStat
	getFullHash
	listDirectory
	copy
	move

G: Ghost
	getSize()
	learnSize() - accomodate potentially new information
	learnStat
TK: Truthkeeper
GS: Ghoststore


1-How do .gcpip files get populated initially?
2-How does .gcip file change when contents is changed?
3-How does a snapshot work?

	gfs snapshot filea fileb

	main: gs = TK->makeGhostsFromCommandline( @ARGV );
			obtainGhostForLFN( shift @ARGV )
				look inside our local run cache for these ghosts first
				if not, pull in the ghoststore of the parent using existing functions
		i = gs->getDescendingIterator(); - populate this GhostIterator with a queue of the roots
		g = i->next();
			pop queue 
			if( g->hasChildren() )
				g->getChildren()
					getChildren finds out the last time dirent was populated
					    if ok, then TK->getCachedChildren( g );
					    	load the .gcip file
					    if not ok, children = FSM->getImmediateChildren( g )
					    		this function opens the directory, does a full scan,
					    		and creates a ghoststore 
					    		calls TK->makeGhostForLFN( "fs:" . thingWeFindInDir)
					    	g->learnChildrenPresent( children );
					    		setChildrenMarkPresent now scans the known children
					    		marking those that aren't found as "not present"
					    		and setting the lastDirEnt date
			onThunk
				g->finalizeChildrenPresent()
					computes the hashes of names and subhashes
					populates lastHashDate() 
					g->learnFullHash(X);
						if different, sets dirty flag
						if date different, sets it, and sets dirty flag
					g->flush();
						Tk->flush() - called on thunk only for directory case
						writes out the gcip file and cleans the dirty flag
		g->forceCharacterize();
			checks all of its fields. For those that don't seem right, 
			we call the FSM -> retrieveFullHashHard
							-> retrieveStatHard




	gfs lsdup dirA dirB
	$sourceGS = TK->makeFromCommand( @argv sources)
	$targetGS = TK->makeFromCommand( @argv dest)

	$targetGS->getDescendingIterator();
	foreach( my $g = getNext() )
		$sourceGS->findAllDuplicates( $g );
			my @sizeMatches = $gs->findFilesWithSize( $size );
				@matches->isDuplicateOf( $g )



###


++++

RSYNC implementation ideas

gs = ghoststore of source
for each file in ghoststore
	get a full snapshot of the remote directory + its pillage/spare places + aux places
	get a full snapshot of the source directory
	identify corresponding path in target to match rsync exactly
		A. There is a file fT already sitting at TARGET at this filename
			A1. The hash of fT is correct - its what we want
				-- Mark TARGET fT as claimed, no action needed, go to next file
			A2. The hash is not correct
				-- Mark as orphaned, select new orphaning location - during clearance MOVE phase (will be moved out first)
				-- Flow down to Case B
		B. There is no file at the remote location in TARGET tree at that filename, or the file in TARGET is marked orphaned
			A1. There is no hash ANYWHERE at TARGET, PILLAGE, AUX, COPY or ORPHAN
				-- Queue for remote copy, function done
			A2. There is a hash available in PILLAGE that is not claimed
				-- Queue move PILLAGE -> fT, claim it
			A2. There is a hash availble in ORPHAN that is not claimed
				-- Queue clearance move ORPHAN -> fT (assign new location) - DONE  (What about case of a swap..?)
			A3. There is a hash available in TARGET that is claimed
				-- Queue local copy, done
			A4. There is a hash available in AUX
				-- Queue local copy, done
			A5. There is a hash available in PILLAGE that is claimed
				-- Queue local copy, done
			A6. There is a hash available in COPYLIST
				-- Queue local copy, done

First do all clearance moves
Then do all remote copies
Then do all local copies

Internal commands we need:

action_clearanceMove( $s, $d )
	mark $s as empty, $s->deleted()
	mark $d with new ghost by cloning and assigning new LFN

action_remoteCopy( $s, $d )
	mark $d 

action_localCopy


FileOpPlan
	knows how to record activities
	knows how to check for conflicts
	knows how to start work and verify its proceeding as planned
	fires actual actions over to the FSE in sequence
	copy
	move
	copyIntoObjectFormat
	
We need to insist on a contract that STATs are repeated during full hash
this is going to be done anyway by the OS
also a guard if the file changes and the original stat and hash fall out of sync





virtual ghosts
==============

gfs ls a b c 

will try to obtain equivilent of the following

gfs ls fs:a fs:b fs:c

but a b and c may not exist!

until a successful stat, they should be marked as a status = v
virtual should not get stored anywhere

whereas if you do a gfs ls . in a directory that has a b and c

in this case, a b c come from a scan, which by definition is not virtual
see?




so we create some virtual ghosts along the way
every time the operation is done, we should commit them (take off their virtual flag?)

maybe we just have a notion of copies or moves, and later we decide if they are local or remote?

need to check $g->isPathUnder($y);

next steps could occur in parallel
	begin copying over files we think are net new in reverse mtime order
	begin verification download (basically, get a snapshot of the remote FS)


PUSH looks different.. Push just says that we're going to move over things we think weren't there
into a "pushed-content" subdirectory
we could get things wrong but better for an offline situation

gsf accept pushed-content-1234 is used to rearrange the tree

maybe we have concept of a library
technically snapshots, orphans, pushes, etc could all be objects

GFS-library/objects
GFS-library/primay
GFS-library/orphans
GFS-library/pushes
GFS-library/snapshots
GFS-library/snapshots/Latest->xxx

need a flag on a blob if is is a moved file, or just some junk that GFS maintained
otherwise, cleanup on aisle three situation is shitty

maybe object syntax is as simple as foo/bar/this@234abc32a993, 
this syntax always means a file with this hash under that tree

how do we create definitive names for things and spread them around?
seems like git just stores things in namespace/xxx format

refs/tags/xyz


GFS objects

What about the remote case?

normally i'd talk about files somewhere else like this:

	jgilbert@auspice.net:backups/file-a

	(which is really:)

	jgilbert@auspice.net:/home/jgilbert/backups/file-a

	or

	jgilbert@auspice.net:/tmp/file-b

from the perspective of the local cache on the sending side, the AFN is what?

Ideally, you'd want to know 

	jgilbert@auspice.net:. 

		is the same as 

	jgilbert@auspice.net:/home/jgilbert

but how would i learn anything about that? feels like there is no clean way,
and the system can deal with discrepencies even if it doesn't know this
information. would be nice if there was a way to tell GFS that in the future

ideally when i have a volume tagging process i can say

	gfs volume.add.remote @GRYPHON jgilbert@auspice.net:myvolume

	gfs init GRYPHON

	gfs rsync my-photos/test @GRYPHON

So if we use the GIT strategy of tagging, there is some symbolic link between GRYPHON 
and a data structure with the connection information

@GRYPHON becomes its own virtual path. Assuming that there are no overlaps
@with other paths, or other volumes, the FQAFN would be remote:GRYPHON/my-
@photos/test

technically, the "remote" in this case is superflous because it can be
instantly known that this is a remote path by decoding @GRYPHON. Also we'd
want a way in the future to relocate @GRYPHON to a local path.

so these alias constructs know
	The handler (local, remote, volume, snapshot, etc)
	The remote location if there is one

technically, that means that every alias is a pointer to some object sitting in
an object store (Either one that is "real and dedicate", or one that is "found" )

this implies i can do the following

	gfs alias snap:foo.snap @FOO

	# creates a link saying that FOO -> 423fe15a4757767bb4d2a04702f8572d3421179d
	# and also remembers that 423fe15...21179d is a snapshot

	gfs ls @FOO/bar

	# which as long as I can find 423fe15...21179d, i'll just reach into that snapshot

But my stored FQAFNs.... Should they be:

	snap:423fe15...21179d/a/b/c

Or should they be

	alias:@FOO/bar ?

What if FOO changes in the future? The data that is closest to the "truth" is
the first one. That is also the information that is most reusable. But the second
strategy is probably better because @FOO presumably means something to the user
and the user has some intent to refer to @FOO/bar in the future

this raises another question even more fundamental. what if I do this?

	gfs alias /home/jgilbert/backups @LOCAL_BAKS

and say the content is like this

	/home/jgilbert/backups/file1
	/home/jgilbert/backups/file2
	/home/jgilbert/backups/file3

now I do a scan

	gfs snapshot @LOCAL_BAKS

Technically, I now have three universal names for these three files

	fs:/home/jgilbert/backups/file1 -> checksum 123
	snap:423fe15...21179d/file1     -> checksum 123
	@LOCAL_BAKS/file1               -> checksum 123

I think for the moment, all of these things are OK

A snapshot basically acts like a local alias. and the snapshot is the most permanent thing

if i were to list who has checksum 123, 

I'd want to see 

	snap:423fe15...21179d/file1
	@LOCAL_BAKS/file1

The first one I'd rather NOT see because it's expansion will have an identical expansion

How does this work with objects? Lets assume that when an blob is stored, its checksum is identical to a file that was also stored with that hash.

Implications

	gfs checksum file1
	-> 623fe15...24479d
	# now we know this hash

Next I can copy by hash

	gfs copy blob:423fe15...21179d foo.txt

I go through and find obj: anywhere I can, and copy it as a blob. 

Another form of this:

	gfs copy blob:423fe15...21179d foo/

	# copies into foo/423fe15...21179d.blob

But what is this idea of an object store? The object store theoretically has a name
and a location on disk. And the contract is that anytime you copy a file there, its name is lost and it gets absorbed.

	gfs copy foo.txt objstore:path/to/store
	# takes foo.txt and places it in objstore:path/to/store/obj/42/423fe15...21179d

	# the AFNs involved
	# objstore:/home/jgilbert/path/to/store/obj/42/423fe15...21179d --> 423fe15...21179d

Wait, so what is the point of storing objstore as the "domain" here? what
information does it add to the system?

Interesting pattern - in which cases does the AFN actually "need" a domain?

	Doesn't matter for aliases, regular files (fs:)
	but it does matter for remote
	and it matters for snapshots (i think)
	and its confusing for object stores

for instance, what is the difference between 

	objstore:/home/jgilbert/path/to/store/obj/42/423fe15...21179d
		  fs:/home/jgilbert/path/to/store/obj/42/423fe15...21179d

What data is gained by calling something an objectstore in the AFN?

two obvservations now that i've written this out:

1) AFNs really have only three core types:

	fs paths: /home/jgilbert/la-la-la
	containers: tarballs, snapshots, things that contain files and have an addressing scheme
	aliases: which can theoretically cover the earlier two cases

2) The meta-data requirements are different

	fs paths: don't need to know anything
	containers: need to know what expansion formula to use
	aliases: may need to point to either fs, or containers, or remote things

An alias is basically a container. But i need a level of indirection. The data in an alias
could change (hostname updates, path updates), and the namespace has to be valid.

so maybe the best term for a domain is a namespace

a. one name space is limited by files on your computer - world of absolute pathnames
b. another name space is basically ghosts that live under a single file-like-thing
	theoretically that single file like thing could have different "views"
c. another name space is aliases, which are ghosts that are referenced through a LUT

the prefixes used on the command line could be better thought of as access strategies?

consider a case of (b.) above.

AFN: snap:423fe15...21179d/file1 

but i might also have derrivative things

AFN: gunzip:423fe15...21179d     -> whatever the hash is of the thing unziped
AFN: gzip:423fe15...21179d    -> has of the thing zipped

what if the gzip algorithim changes?

well, each row is a ghost. so technically, each ghost has a lastFullHash date, 
and if that gets too old, you'd have to repeat the operation.

An alias can be said to always start with an AT

@FOO/bar

so lets consider a simple case of LS

	gfs ls @FOO/bar

	first, we make @FOO/bar a ghost G1
	then user calls G1->getHash();
	G1 is going to say, "wait, don't ask me, dereference me first!"
	so G2 = G1->dereference();
	two cases now
		case 1: fs alias
			G2 is going to be /home/foo/bar
			so i give the information about G2 and ascribe it to G1
		case 2: snapshot alias
			G2 is going to be snap:423fe15...21179d/file1
	Either way, G2 is something that i can ask a meaningful question about
	there is actually a question if G1 should even get "stored" anywhere

	in the case that i have no idea what snap:423fe15...21179d/file1 resolves as a hash

	when i call obtain "snap:423fe15...21179d/file1"

	the TK handler makes a virtual ghost with that name
	G2->getHash()

	will get passed to the "snap" FSE 

	SNAP FSE says fine, get me an object named 423fe15...21179d
	presumably this thing appears
	and i slurp it in, and then i'll have my LFN cache, and i look up file1

great lets take another case

	gfs ls snap:foo.snap/file1
	I obtain a ghost G1 for "snap:foo.snap/file1".
	G1->getHash();
		this will be a cache miss (obviously!)
		needs to be some logic to deference rather than giving up early
		so next I call G1->getFSE()
		and the FSE call will make me a new explorer connected to foo.snap
		if one doesn't exist already
		this implies some sort of FSE cache or factory
		Now I call FSE->getFullHash()
			well, if i have an AFN cache, a forced conversion to AFN will give me an
				opportunity to remember this answer since it AFN is snap:2343...43432/file1
			but if not..
			this routine will force enumeration of foo.snap, populating a LFN cache
			all of the ghosts will be stored in the FSE maybe for use later
			these ghosts should be marked as "immutable" somehow
				i'll pull out the ghost, and extract the hash from it, and return to caller

great, lets take another case

	gfs checksum tar:foo.tar/file1

	I obtain a ghost for G1 for "tar:foo.tar/file1"
	I ask, is tar:foo.tar/file1 a directory?
	the ghost code won't know yet,
	so it will ask for a stat on tar:foo.tar/file1
	get me a FSE for tar:foo.tar/file1
	this will navigate to a cahce for tar:foo.tar FSE controller
		i'll now generate an AFN and check it
		but assuming that fails
		I untar the file somewhere
		i'll be generating LFNs that look like this
			tar:foo.tar/a 
			tar:foo.tar/file1
		and i'll get the checksum of that file
			i'll make ghosts for the things i find there 
				and the call to checksum it will pass through to that ghost
					ghost will call me asking for the checksu
					i'll construct a new path with the temporary directory
						and hash that file
		if anyone asks for the AFN
		ghost will call the FSE to get the AFN

THis implies that canonocalization and rel2abs, cwd stuff all should live with the 
FSE, not the ghost code!!!

Lets revisit the alais case in more detail

	gfs checksum @mainbackup/neepfile.txt

	I obtain a ghost for "@mainbackup/neepfile.txt"
	initially marked as virtual, b/c i don't know what it is.
	first, i ask "are you an alias"?
		ghost getFSE returns the alias handling FSE
			fse->getFileType("@mainbackup/neepfile.txt")
				FSE will say yes, you are an alias
					not clear if FSE is a singleton thingy or one per alias
					might be cleaner if each alias consiered to be its own FSE
	now i say, G2 = G1->dereference();
		ghost will call getFSE->dereference("@mainbackup/neepfile.txt")
			FSE will go to its reference database
				learn that "mainbackup" points to object 423fe15...21179d
					so now I pull this object from store
							TK->getGhostForHash( )
						the object is going to say mainbackup = /Volumes/blah/bu
						now FSE calls TK->obtainGhostFor "/Volumes/blah/bu/neepfile.txt"
						return
	great, now I have G2 = "/Volumes/blah/bu/neepfile.txt"
	That clearly is just a regular FS path
	so handled the usual way, (Generate pcip files, etc.)
	not clear how the G2 information gets "absorbed into G1" and committed to disk

another way to think about this is that maybe there is no explicit "dereference"
instead, a ghost that knows it is an alias just passes all questions about itself
to the other thing?
but that would require a ton of proliferated code

maybe the TK just gets aliases and dereferences them after the fact to keep things clean

or there is a function on a ghost called g->absorbAttributesFromReferenceGhost()
and this moves over the hash, size, stat and other information

big question - is the file type of an alias always an alias?
like a symlink?
where you don't know if the symlink is a directory or a file?
maybe its best if we do that
unix seems to cope with that type of situation just fine


we need to figure out semantics for the reference database
we need to store two things permanently:

obj and ref

just like git

we can do this in our working directory
but it would be nice if this was all managed

this also should be some sort of FSE

objectstore

and there is a $coreOBS that is my working directory of shit
and i have an FSE complaint thing that manages it

so if I construct a hash like blob:423fe15,
G1 = tk->getGhostWithHash( "423fe15" )
	this routine is going to get you the most local version it can
	this may involve scanning various object stores it knows about
		its going to eventually end up saying
			$coreOBS->getGhostWithHash( "423fe15")
			the core OBS is special
			when you ask a object store FSE for a hashed thing
			it doesn't have to scan its known ghosts
			instead, it can actually just pick up the file
			it turns that into a ghost
			and someone should call "isAcceptaleCopy" to do a guard test on it

if i construct an lfn like blob:423fe15
G1  = TK->obtainGhostFromLFN( "blob:423fe15" );
	I'll get the ghost right away (its virtual)
	but the second anyone asks me to do something with it,
	i'll get delegated to to a blob FSE
	and the blob FSE knows to go back to truthkeeper
	maybe blobs are ocnsidered aliases (e.g. caller must dereference them)

thats sort of a nice way to handle it

in fact, yes, lets do it that way

maybe truthkeeper has a way when asked to do a getGhistWithHash
of passing that work down to FSEs?

there is also a ghostFromLFN strategy to think about
TK first checks its cache
if it can't find it there, it asks the relevant FSM
which implies that a global function to find FSM for LFN
this lives at the factory level



+++++

# what is the means by whcih deleted files are recongized?
# in cases of a full scan, someone has to be smart enough
# to know we are doing a scan that is "full" enough to count,
# and to mark ghosts for deletion

# right now, there is no stat verification when writing a snapshot
# snapshot -verify-stat (can find it, size and lastmod match) --verify-
# hash and when those things are in place, anthing that hasn't been
# checked in the last 24 hours are rejected, unless you say --hit-disk
# which means basically ignore all of the  tools and recheck everything











# how do we stick virtual things into the hash?
# for instance, where or how do we store the idea that a .tar file has certain contents
# or that an rsync directory (jgilbert@auspice.net:a/b/c) contains a file?
# almost like a normal path a/b/c is basically an implicit "local:a/b/c"..
# the local handler knows to write .gfs_info files
# whereas the remote one knows not to
# each FS type offeres a get, move, copy, retrieve full checksum, stat, etc function
# so now we can handle S3, Rsync, etc

# how does that tie in with tags?
# how do I say anything under /Volume/Foo should be addressed as FOODRIVE:
# clearly this only matters during AFN conversion
# disktag:FOODRIVE|/one/two/three
#
# if that is the AFN recorded, then the system will think that is there
# we just need to write a record like /Volume/Foo -> FOODRIVE
# and also write a /Volume/Foo/GFS_ID.txt file
# then a dereference just requires that we verify that the /Volume/Foo/GFS_ID can still be reached
# maybe there is a tool like "bless" which does this for you
# so the rel->AFN conversion would either need to rely on this mapping
# or traverse back up to make sure that all ../GFS_ID.txt files were registered

# is a snapshot treated as a file collection as a target? if it is, we have to emphasize 
# that it is a ghost. e.g. you can enumerate, but the fact that snapshot:4444/sdf contains
# this file version could never be used as a proxy.
# so when you "get all AFNs for hash", you'd have to pass to a FShandler for the snapshot





# Database layer use cases
# 1) quickly give me ghost information for a path
#      AFN -> GhostSerialized as absolute
# 2) quickly give me the AFNs associated with a hash or size
#      hash -> AFN 0..n   (needed for LSDup)
#	   size -> AFN 0..n   (also possibly needed for LSDup)

# so when something is stored in location X or found in location X, 
# what permanent record is created?
# maybe we create spontaneous snapshot records that can be loaded back in?
#
#    gfs rsync /source jgilbert@asdf:foobar/
# 
# All files written to foobar drops a file in .gfs/receipts/rsync-$$-date.snapshot
# With the note: "This snapshot record was written by GFS on date XXXX"















