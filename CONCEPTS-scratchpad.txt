
# What is this document

It is a collection of work in progress (or sometimes retired) ideas.. anything that is current has been moved to the full concept file. 

Current goal: Tidy backup and restore. Backup writes new files into a tree and a commit hash of the directory state. Backups can be named. Restore takes the name and reconstructs it somewhere else. Permanent metadata about the state of the backup is checkpointed into a deep tree object so that if metadata is destroyed we can recover it. Work is done when we can back up a directory a few times, then restore it, then blow away metadata, restore that, and then restore another file.

- [ ] actually test the new learnAFNlist() functionality with a few different cases to convince myself its stable
- [ ] finish backup action - should write out a deep hash object
- [ ] wire in a reconstruct verb that uses recursion. And we should be able to test it works by clearing the caches, recheckpointing, etc
- [ ] rel2abs is generating paths like ../../../ in the AFN queue, can we actually just say revokes only happen on REAL things?
- [ ] ultimately, checkpointing probably ought to clear out all objects below that path
- [ ] what do we do about revokes during the learn and flush? Hashes will change on the ghosts held by the GDB, but should that trigger revokes? Also sometimes it triggers stats too. This is quite wasteful. 
	- [x] one of these is triggered by learnHash which mkaes a call to canHaveChildren [fixed]
- [x] Have discovered that writes into the object store do not appear to immediately enter the cache. (Not sure why...) - Fixed now
- [x] What "lastEnumeratedDate" do we use when we learn of our contents from a directory object? Hacking this now using lastFullHash date, which seems mostly correct.
- [ ] fix glitch that changing dir contents doesn't appear to revoke hashes
	- [ ] reproduce by cloning dir and moving files and recheckpointing
- [x] add logic to flush and recover directory objects from OJX store.
- [ ] rework snapshot intake and writes
- [ ] now build a proper reconstruct verb 


- [ ] directory size changes should NOT revoke the directory
- [ ] any creation of a new file should revoke parent hash, (call learnContentsChanged) - mostly sure this is true
- [ ] revoke enum (and therefore LC) when file is added
- [ ] revoke enum during mkdirs
- [x] Lower priority: wire in reads of directory caches to properly populate children, find some easy way to get LFNs for children fast



# special revocation rules


scenario to clean up 
mkdir bak
checkpoint bak
backup into bak
rm -rf bak
mkdir bak
NO Checkpointing
run abother backup: gfs backup run dir-a bak --stats

what's happening here is that bak has a checksum in the database from the last time it was enumerated. we do a directory scan that comes up with a different hash. When that occurs, we should revoke the contents on the directory. But right now we don’t, and when we call hasAcceptableEnum() the check fails because of the mismatch. 

reading the contents of the directory is akin to source verification in the new world order. But the current implementation doesn’t handle it that way -- it ALWAYS goes to the original.

We need to refactor the hasAcceptableEnumeration code to handle this idea that the directory state works differnetly now.

When you call getChildren or getChildrenLFNs, we should pull in children from the directory object, if it exists. This provides the answer to the LFNS (and thus the children.) If we cannot locate the directory object, we can keep the hash, but we must pass to source verify. If hasAcceptableEnumeration passes the date check and we have the file, we are essentially good to go, and we just return what we have. If for some reason, the date check isn’t good or the backing file is not loadable, we should scan the actual disk with readdir. At this point, a few different things could happen. First, the new contents could have a different hash, and we call “learnContentsChanged” on the directory. Incidentally, we’ll also discover deleted files and other such things which should propogate. Second situation: the contents could be the same, but we’ll up the enumeration date to the most recent read. In cases where the type of the file changes, we need to deal with this too (e.g. a directory has become a file, or a file has become a directory)

I think as part of this cleanup, we might want to think about a formal type tag sitting on the ghost. A stat has to be able to learn that a type tag has changed. learnStat should ALWAYS update the type tag to its correct value. And in cases where the type tag changes, propogate a change event.

Types:

f - file
x - deleted file
d - directory

In cases where a directory is now found to be a file, we need to learnDeleted on the directory. Curiously, i’m not sure that type is super important right nw. I could probably defer this bit until we actually have other types (like commits?)

and learn deleted on the directory may need to cascade down to revoke ghosts. Either that or we just wait for the usual checks for “getDuplicates” to cascade. But it cleaner if we do the dirty work. Not sure if that is strictly needed. If we revoke the deepScan, the user will get around to this eventually by checkpointing, etc. But what if the removal is discovered on a volume? Does checkpoint have a way of revoking all of those pesky hash->AFN things? Another way of handling could be with a deliberate flush: if you checkpoint X, you drop all objects formerly under that part of the tree from both tables. Basically a forced form of garbage collection.

If we had multiple hash->AFN lookup mechanisms, revocation could simply delete out the one that doesn’t exist, or mark it dead. So for instance, if each volume had its own hash->AFN LUT we could get away with that.


# What is a managed object exactly?

[Current as of 6/22/2016]

A managed object is a blob of information and metadata created by GFS, typically stored as a file on disk. They are typed and can be identified with a magic tag at the start of the file. The first use case of MOBs is a deep tree snapshot. But other uses are anticipated, for instance as “commits” (named backups).

MOBs may refer to zero or more sub-objects as hash pointers. This means that a GC operation can be performed to make sure that the MOB’s metadata is reconstructable, and also to be able to purge dangling references. For instance a list of known backups might be stored as a MOB, where each backup is another MOB, a pointer to the head directory of the backup.

MOBs are frequently carriers of ghost information, typically information locked down in place in time. Normally this ghost information is returned as immutable objects into the GFS layer. 

My design intent is that managed objects can live anywhere the user wants them to live. Also I think they are effectively immutable. They are identified by their hash. 



# The duality (and triality) problem of ghosts (6/22)

[This is a current description as of 6/22]

I like ghosts. But I’m starting to see the concept is a bit overloaded. 

Today the ghost represents the potential or current state of a file or path on disk, along with exact metadata to know how recent that picture is. It also can contain children. The ghost can either operate in a mode where it is a cache of a backing store (basically the underlying disk/filesystem) or it is frozen in time.

Therein is the contradictions. We’re using ghosts to fulfil many different purposes:

- Ensuring that we have fast, cached (but consistent within timebounds) access to current filesystem state inside the codebase that persists over time
- Storing “frozen” contents of directories (tree objects), in which case the ghost is super stripped down
- Handling multiple potential sources of truth about reality (e.g. a remote node) so that we can be eventually consistent. For instance, the ghost structure could let a sending node keep track of what it thinks has been written to a remote node, and then a remote node can “push” back updated state. The transactional nature of the ghosts means that many different truth-gathers can be active, and each arrive at a useful picture of what is where
- An abstraction layer around where something is - the ghost can be local, remote, S3, etc and the underlying code should be able to deal with it. There are no assumptions above the FSE layer about where a ghost is and how to retrieve it.

The ghost storage infrastucture (TK) handles two different problems for us:

- Obtaining the latest ghost for a given file on disk
- Finding all known ghosts with a particular hash

The challenge is that ghosts can be immutable (frozen), and their semantics inside the “always current” ghost world are hard. 

Here is a nasty corner case: Lets say we want to know if a backup performed long ago contained a particular hash. Where is the single source of truth about the past? 

Right now there really is none, we just sort of pretend that the ghosts are always current unless they are not, and then they are not managed by any central architecture.

I wonder if this domain concept saves us a bit from this. If we have a domain of “current”, the backing store is always the filesystem. But other domains could represent past state.

The problem is that domains will have a super high overlap of ghosts and other metadata. 

Another problem with ghosts is that there is no mutable state for the contents of their directories. This forces us to rely on enumeration from other sources (with verification from the checksum) not the ghost itself.

It seems philosophically perplexing that a ghost that can update at any time somehow connects-to and uses a directory listing that is effectively immutable. Its also really strange, architecturally at least, that the contents of directories don’t persist in the unless they are fully checkpointed (e.g. have valid hashes.) Maybe its a good thing. Its means we cannot have a definitive and immutable perspective of a directory unless its entire contents is fully enumerated and hashed.

So it means that an mutable ghost directory effectively “points” at an immutable managed object. And the second contents change underneath it, it looses its hash and effectively the link to its full enumeration. 

Again, I think this is all correct, but I’m trying to get comfortable with it.

# Ghost contracts: What is truth? What is the single source of truth?

[Current descriptions as of 6/22/2016]

Ghost is a description of the potential state of a file.

Currently, contracts look like this

- No obligation for a specific ghost to be up to date
- Only the ghosts from the TK are guarnteed to be single source of truth
- Ghosts find their FSE by looking at their handler value
- Ghosts can be in any number of ghost stores
- TruthKeeper should make all ghosts that are intended to hold present state
- The only reason TK is not used to make ghosts is in cases where you are deliberatly constructing older things
- TK holds all ghost pointers and knows if they are dirty or not
- Generally, the FSE doesn't hold links back to ghosts except in cases where a ghost is a root (like a snapshot)

Its canonical that the LFN of a ghost once created must never change. If you need to reference a new thing, make a new ghost and copy over the data.


# TruthKeeper evolved: Everything is learned

[Current design]

You wnat multiple directories of ghost information -
	e.g. a backing store using BDB, .gfs_ip

THe truthkeeper ALWAYS makes the ghost first during obtain. Then state is gathered from the others each in turn via new learn operations.

When a ->save() is issued, the ghost is dropped out of the caches, and the ->saveGhost() message is given to each GDB

We need some mechanism to be sure the ghosts are actually finalized on save, otherwise operations that create millions of ghost are going to drive crazy amounts of memory use.




# New concept: Domain (6/22)

A very prototypical idea. But what if we could define a “domain” which is a set of related blobs and metadata that we track and is persisted on disk? I’m thinking this is sort of like a packfile in git. The domain has primitives “AFNforHash()” and “GhostForAFN()”. 

Whats nice about this structure is that the system can know the data returned are versioned and specific inside the scope.

I’m thinking this would be the ideal backing store for a volume.


# How does Bup Handle splits?

have discovered that BUP does write out git compatable objects, but git has no provisions for joining up files.

	bup index . 
	bup save -n foo .
	cd ~/.bup  
	git --work-tree=/tmp/foo checkout foo
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/2ea61
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/3224c
	./Users/jgilbert/PhotoFSX/LR-PhotoFS-LT/Vermont 2015 Unsorted/jag-151010-20875.CR2.bup/0e21bce/4bf95b/0fa36d/34c24


 






# Hashes and interior objects

Lets say I have two directories A and B

	a/file1.txt
	b/file1.txt.gz

I want to do a test to see if all of A is contained in B?

Lets pretend we have a function called “canReconstructHash()” CRH. How would CRH know to avoid a backup? Theoretically, CRH should return true when file1.txt is examined against B.

So where would this data come from? Lets assume these checksums:

	a/file1.txt      [fc123]
	b/file1.txt.gz   [def00]

Lets take a very simple case.

	gfs contentscan b/file.txt.gz

The content scan pops open the gzip file, and checksums the thing it points to. Now our global AFN cache is aware that the checksum is present. The following AFN is generated

	split:gz|^def00:    [fc123]

This enters our cache, and now CRH will work.

So where do we persist this information? 

A parallel situation exists for directories. If I have a directory of hash 299932, how do I efficiently know if it can be reconstructed on B? Assuming my backup is checkpointed, the directory is now part of the AFN cache.

But lets say all of the files are there, but the directory itself is not on the remote side? It could be reconstructed. Is there an efficient way to test for this? The rule for CRH on a directory is that I either have to have the hash of the whole thing, or the hash of its blob like things (the rest I can construct if needed from the metadata.)

The second problem is manifest files. Right now, a backup operation presumably is going to write a file for every file and directory present in the backup set. I also think that the LFNs should be the file versions that were known back then. But this means that even very small changes to a large directory tree will always generate very large manifest files.

You’d ideally want some compact file format that can be quickly scanned to locate if a hash is in or out. And you’d want the manifest files to only generate entries for the objects not in the previous manifest file. Or maybe all of them not in a baselined file.

On restore, you simply reference all of the manifest files so that the missing hashes, etc can be recovered from the thin object manifests (the deltas.)

So in a naieve implementation, the backup system first checks that the baseline manifest on the server matches its own, and then does the diff, and sends over the smaller manifest file.

But pruning is going to suck a lot in this case. 

There is a certain logic to git making a “file” for each protected hash. Then the approach to pruning is simple. You write out a file of all of hashes of things you care about. And any hash inside that directory that doesn’t match, you get rid of. That purges both blobs and tree objects. A bloom filter might make this even easier.

I could do that with GFS. Every directory could have a small object file. It could even have the same checksum as the directory does. And then we know that any FILE or DIRECTORY with a given hash can reconstruct a directory of a given hash.

Another way is that we constantly rewrite the manifest file. There is some compaction.

In the case of tar metadata, we could have a directory object that is the contents of the tar file itself. Basically this file ties together the hash of the tarfile with the hashes of its components. It can be regenerated at will. Loading this file places the tarfile contents into the global cache.

But this needs some kind of way of finding the file, prefereably on demand.

What if we had a structure where the **filename** of the object was the hash of the thing it accounted for? Now at least, its really easy to search and doesn’t need to be necessarily penetrated (open+slurped) unless a question comes in to resolve it.

So if 100 files are needed to construct 1 file, you tkae the hash of that one file and make that the filename

But if 1 file expands to 100 files, you DO need to do some kind of deep content scan to pull those 100 files into your cache for the reverse lookup. (You wouldn’t want to have 100 files.)

Also not clear how this might tie into packfiles which seem so uber cool. 

And what if you want a file that has multiple entries (e.g. 100 files -> 1 file, 1000 files -> 1 file) so you don’t have so many opens and cloeses?

How do you find this object and manage it?

Another open question: What exactly is a backup? Is it a snapshot? Or a collection of file versions? Theoretically, it could just be a pointer to the hash of the top level nodes. 

We also need some way of handling volumes. We want the behavior that a copy of the contents of the volume are stored in durable places both on the volume and on a variety of other "archival" stores.


# East Cambridge Design 

The proper persistence behavior for directories is to flush a full directory listing to a disk as a file in the object store. The contents should be the file versions exactly as they were at that time.  Any time we call $g->flush(), this should happen.

Proliferation will be addressed with garbage collection.

ANy containered objects should do this, and that is the backing store for the ghost to get its contents in cases where the contents may no longer exist via readdir(). This case is most likely to occur when the ghosts are made immutable (as file versions.)

When you read a snapshot file for a restore, we need to make sure we create all of these little directory objects too. (Those objects can fortunately be detached from the snapshot and be first class objects.)

This hopefully helps avoid pulling the entire snapshot into memory.

This will create snap:2343df:a/b/c objects in your AFN cache.

Now you can do an ls

	gfs ls snap:foo.snap:a/b/c

And when you need the contents of c, you’ll have the hash. Now you look up that object.

You can imagine having some kind of really cool $ghost->reconstruct() thing that just figures out how to do it with a bunch of store operations.

Now here is the f’in wierd part. When a ghost is immutable, do we have to distinguish it better? The “c” that came out of the snapshot is a different “c” than a different snapshot? Even if their contents and modtimes are the same? I think the answer surprisingly is no. If we’ve commited in our signature for a directory that “c” of hash 2342c means a certain thing, it should ALWAYS mean that certain thing.

So how do we reconnect with UUID and sequence number later? Well, we’ve learned from the snapshot into the global AFN directory. But many of those files actually don’t exist.

So on restore, I think we create the new ghosts using the snapshot as a baseline ghost, and that will pick up this data.

Can we avoid many file accesses to get data for each directory? Several answers here. First, many of those file accesses will collapse when we get packfiles. Secondly, we could create a very simple additional database that maps hash -> AFNs of children. Or maybe AFN->AFNs of children might be even better. 

So when we read a snapshot file, we will be creating new directory objects now, not just files. We’ll check that the read contents match checksums. And then the persistance code will suck all of that stuff in. A snapshot is sort of an example of a deep tree persistence object. We will want similiar things for tar file enumerations.

So that further implies that we create a more generalized idea of a managed object.

	ManagedObject::DeepTree::Snapshot
	ManagedObject::Directory

Managed objects should know how to load from files and save to files.

I think this is better/cleaner than the ghoststore approach. But maybe we’ll merge or combined approaches. GS were sort of written with the idea of fully enumerated things and iterators, and now that my ghost architecture had containers, they are less important. My future snapshot writer needs something different.

	my $s = ManagedObject::DeepTree::Snapshot->new();
	$s->setRoot( $rootGhost );
	$s->readFromFile(); 
	$s->saveToFile( "foo/bar.txt" );
	$s->fsck()
	my $root = $s->getRoot();
	$root->getChildren();

So is a snapshot a source of truth? I think answer is no. I think instead it declares a namespace. Lets say that the snapshot file s.snap has hash [3ff39]. The LFNS are going to look like

	snap:s.snap:a/b/c

And the AFN is

	snap:3ff39:a/b/c

When you call get children on the LFN “snap:s.snap:a/b/c”, we’re going to have two strategies. First, the c object has a hash. We can easily look up that hash in our object store. This gets us the LFN names, and we use those to pull in new ghosts. We can also convert to AFN, and we’ll get the same hash. Its probably dangerous to actually pull in the ghosts from the file. 

now we have a new problem. ghosts that are part of snapshot are immutable. So we’re going to have to somehow figure out how you recall a immutable ghost. Its reasonable to think that all recalled versions of that AFN should be identical. We also have to make sure that the final ghost is marked as immutable, and only learns from immutable sources.

Basically, you want logic that learnFromGhost() will populate the immutable flag, preventing further learns. I’m not sure if the code does this. So it will need to.

Now we have a way of doing fast recursive LS of snapshots, where the recursive data will pull from the snapshot.



# South Cambridge Design 6/18

In the above design, more of the reconstruction dynamcis move to tree operations as opposed to flat lists of nodes. An interesting question will come up as we move to a world where reconstruction is based on ghosts. We will need to go through a list of ghosts in BFS order, calling things like this:

	let M be the ghost where were are testing for reconstructability
	foreach n: all nodes at this node level 
		m->canReconstructHash(n)

In the ideal case, M=N, and the tree doesn't have to decend any further, saving a massive amount of subtree comparisons. 

But when M !=N, we need to dig down deeper into M, cutting into smaller and smaller search trees. 

One clear approach is to have M be fully enumerated into some hash lookup table. Then the hash LUT just checks to be sure that the node is actually underneath M. In most cases, this can be verified by AFN comparisons. But not always. For instance, split files, or tar files will not support this easily. 

Git handles this using packfiles of closely related objects, and then scans them in MRU order. Not sure if this will universlaly work in our cases.


# Car dealership thoughts

Snapshots are important - they are how backups get saved and restored. I need to get them right in order to have full backup and restore functionality. 

A little stymied. What is a snaphot object exactly? I used to have a nice clean definition -- an addressable file type. E.g you ask for thing X, and it opens a file and gets it for you.

But now they are acting a lot more like pointers. Imagine a really clean backup that has nothing changed. It basically a date and the hash of the directory object. Of course, to make them more portable, you have to write out some associated metadata (potentially every ghost underneath.) But I see no reason to make each of these objects some special namespace to themselves.

They are sort of like commit objects now. Meaning they need to have their own little data store. 

To play this out, lets map it out a bit more. 

I backup a directory, and I get a signed new object in some namespace. 

	commit:93994234234

The name doesn't even really matter. It could just be the hash of that object. 

Does it get its own FSE handler? Maybe a good idea since you might want say restore a partial part of the tree. 

So this is the long term direction to head.

But it does raise some obnoxious issues. In writing the metadata store, we'll have the versions of the ghosts which may differ from their latest versions. Might be nice to rename them in some other namespace so we can address the older versions.

Maybe this is how we handle fileversion objects?

I need a fast clean decision for the initial implementation.

The simplest thing is that unpacking the metadata store just is equivlilent to learn operations on existing ghosts. So that's what we'll do.

Still some funky issues.. directories that come out of these tree things are technically immutable. We don't ever want them refreshing their contents from the central store.

so we have a contradiction. On the one hand, we want to persist all directories into the repository of objects. But when we load them as ghost, they are subject to refresh rules. Say i aske for the directory that represents a backup i want to restore. it needs to be handed to be as an immutable object. getting the children can only happen when i pull contents from the repository (or if I happen to have another directory with that hash laying around, but this really an edge case.) So getChildrenLFN on immutable needs different behavior. 

or maybe we just need a more generalized way of naming and retrieving immutale objects?

# more thoughts - a wierd implication of directories as "first class" objects

If i have an object like vol:foo:bar/two/three, what actually gets stored? I wonder if the idea of a sub directory should be universal between FSEs. So if directory X is moved to a volume, its hash should stay the same, right? That means that when you look to enumerate childred of an object, the children should inherit the FSE of the parent node. 

that then implies that a volume object is really just a pointer in the hash database to the top node of the directory plus some information on tagging. When an online operation occurs to a volume, we update the volume pointer to whatever is our newest expression of top-level state. 

then when we disconnect, lsdup and other actions will still work. Lets say i lsdup a file that is on a volume. I'll find a copy of that object in a few different AFNs. assuming i can identify at least one of those AFNs as a volume object, i'll recover it as immutable, and that will report back out to the duplication code.

The point I'm struggling with in this is that it means a ton of extra ghosts are going to persist around. If I back up a file 6 times, and it is on 2 volumues, does that mean I have
	
	1 ghost for the actual file
	2 ghosts for the actual file on the volumes
	2 ghosts for the immutable volume representation of the file
	6 ghosts for the snapshot

That feels like a shit-ton of metadata. Probably some need for cullung or garbage collection if that is actually the correct design. How is it incrementally recovered? What needs to stay?

Incremental recovery of the actual stuff isn't a problem. The ghosts for the immutable volume representation can be resolves from the ghost pointers from the top level volume node. I suppose the same is true for the snapshot. Fully enumerating the volume will recreate the intervening ghosts.

If we move to a more distributed store for finding ghosts for hashes, (and ghosts for AFNs) -- rather than having them all jammed into a BDB -- which I think we're going to have to do sooner or later, I think the design becomes simpler.

So my decision right now is to allow the ghost proliferation to occur, at least for the time being. We'll cound on GDB being super fast, and eventually we'll get this stuff compacted. Git shows up that compaction is possible.

# How does a split work in the East Cambridge design?

Another interesting question is what exactly is a split? We're representing the idea that 10 files construct 1 file. Lets start from the reconstruction case first. Say I'm told to recover a blob with checksum X. I first go into my AFN for Hash database. I'll get a bunch of fs: objects, but say none of them are available. I'll also get a split:[X] AFN. This AFN will have the hash of the file i'm trying to reconstruct. But its descriptor file will NOT have that same hash, unlike a directory.

SO now I need a level of logical mapping. I have to take that AFN split:[x] and i guess its got to point somehow using a mechanism i haveb't invented yet into a descriptor file. And inside that descriptor file i learn the ghosts for each of the splits. We also want that descriptor file to support many:many.

So on the split:[x] I call -> getChildren(), and we'll pull into that descriptor file and it will immutable, unflushable generate ghosts for the idea of the peices of the file i can then use. This file doesn't have to be very complex actually. It might just literally be a binary list of hashes. But I can build ghosts out of those, I think. 

One of the mechanisms to consider is maybe the split DOESN"T appear in the hash->AFN space. but I also know that i can use namespaces like snap:[HASH] as a path to reconsturcting something. Rule: if its a blob, and I can't find it in the hash->AFN space, also look for split:[X] objects.




# virtual directory problem

I'm backing up directory a/foo b/bar and c/tar.

Doing, this, I need to create a snapshot object representing this state. But there is no actual live directory that is the combination of these things. So I need some way to create a virtual directory.

Decision: directories are directories no matter what FSE generates them. For time being, i will use snap: and make sure enumeration follows the handler of the parent. This may be a hack I regret, so I'll have to change it later.


# Directories and "store"

If we are committed to directories as first class objects, we need to be able to fire "store" events at them to get them in other places. This is especially important with backups. But like many of my recent forays into “directores = 1st class”, it comes along with an unusual and frustrating set of coralaries that have unintuitive ramifications.

Consider this example: I have a directory dir-a that I want to store in another location. Some of the files in dir-a are already at that location somewhere. Some of the other files are not.

First, I look for dir-a’s hash in the remote location. Not seeing it, I should not just move dir-a over wholesale along with all of its contents. Instead,  I should mkdir at the remote location. ANd now consider, for each child, if each of them can be reconstructed on the other side. If they can, we skip. If they can’t, we copy that one over too.

But the confusing thing is that at the end of the day, I will not have actually copied over the directory physically on disk. I will have copied over enough that can-reconstruct will pass.

This is all sort of muddled in my brain. I can’t tell if its because these are new confusing thoughts, or if its because I’m loosing interest trying to solve these puzzles. I feel like there is a general purpose operation here that is eluding my brain and preventing me from coming up with a solid bulletproof abstraction for this.





 
 









# New semantics for directories (slightly different design)

In an ideal backup scenario, I obtain a hash representing a particular directory. Primary source verification occurs by getting the hashes of all directories underneath. Therefore “has hash duplicate” still works. As long as the directory is EXACTLY the way it was, we are all good.

This model starts to break down when it comes to multiple snapshots or whatever you want to call them. 

In that case, I need to represent that having a copy of hash XYZ  is equivilent to having the TREE object for that directory, the tree object for all subdirectories, and every file referenced in between.

Similiar case for splits and merges. I can say I have a file if either:

- I have an exact has duplicate
- I have a metadata file saying that the file can be reconstructed with parts P1-Pn, and each of P1-Pn’s hashes can be located somewhere.
- I have a metadata object saying that the file is a split from some other object Q, and I can find and checksum Q.

canReconstructHash() is a semantic that could apply to any ghost or path. (I think.)

if I ask canReconstructHash() on a fs:: type object that is a FILE, the best I can do is see if that file is the hash. My answer is a definitive YES/NO.

if I ask canReconstructHash() on a fs:: directory object, I can answer YES if calling it on all of my children is a YES. If my enumeration date is within the guard time, I can answer definitive NO. Otherwise, its a PROVISIONAL NO.

lets say we have some understanding of an object with an object TYPE known. Say an object is just a file that starts with GFS magic.

if I ask if I ask canReconstructHash() on a object “XXXX”? It depends if i trust that object. Say I slurp in that object into some sort of provisional cache object. And say it passes whatever sort of validation I care to put on it. The object might refer to local or remotely verifiable things. canReconstructHash() can be called on each of those things.

if i have a ghost

	split:tar|tarfile_fc123)|a/b/c/ -> ab6004

than I can reconstruct XYZ as long as I have an object fc123. 

That means fc123->canReconstructHash( abs6004 ) should be true, just like a directory.

I can cache my answers. A->CRH(B) can be cached as a pathway of hashes. This is nice because it locates an answer but I can check it at any time, if needed, untaring the file, pulling the remote, etc.

what about a merge?

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002

This object preferably has the hash fc123 when queried. 

So how does this work? I have objects on disk whose SHA hashes will NOT match the ghosts the point to them. 

Two ways to handle this. Either i have a logical hash separate from a physical hash. Or I’m just cool with physical hashes being a lie for certain metadata objects that I maintain anyway. 

Actually, for tamper-proofing, maybe its work considering a compound hash?

Or maybe we go back to the idea of pointers for this sort of thing.

The file listed here describes out a few files can be joined together to form fc123. For efficiency, we probably want flexibility to put mutliple files in this object

	merge:origfile_fc123:abs6001
	merge:origfile_fc123:abs6002
	merge:origfile_cce99:xxx6003
	merge:origfile_cce99:xxx6004

call this a merge index file.  Say it has a hash of d99abc.

d99abc->canReconstruct(fc123) is TRUE as long as we can find abs6001 in other places.

It would be nice if splits could somehow store along with them the things they restore. But they can’t store pointers to each other -- that would be impossible to calculate a hash that incldues another hash in advance (assuming SHA stays unbroken)

so how do we know to find this merge file and learn how to reconstruct fc123?

I need somewhere to “learn” when I scan the merge index file, i generate these AFNS

	split:d99abc:fc123

This bit of information is always source reconstructable (amazingly enough), just by re-reading the merge file.

So mergers actually can be handled by storing splits.

Bizzare!

In theory, a directory is also a split

	directory [hash=ABC]
		file 1 [hash=01]  
		file 2 [hash=02]

When we make this into ghosts

	fs:root:directory        : ABC
	fs:root:directory/file1  : 01
	fs:root:directory/file2  : 02

However, if we go to a strict split syntx, its also storing

	split:ABC:01   -> 01
	split:ABC:02   -> 02

Lets get the terminology exactly right. 

Consider this LFN:

	split:X:path -> HASH 

This LFN stores the idea that if you split up the thing pointed to at X, using information in path() you can get to an object called HASH.

Consider the alternative

	merge:X:path -> HASH

This says that in order to get thing X, I need a peice at path(), and it should have hash HASH

In this case, [merge:X:path]->CRH(X) is false. But a merge index file FOO is assumed to be a complete set of mappings. Would be nice to also verify that at the ghost level. So the rule is that if we intake the whole chunk of the merge file, we’ll give ourselves permission to enter these new AFNS.

So the MIF needs some sort of magic keyword at the top. And we need some concept of “gives closure to”.  For instance, a gfs_ip file for a directory can be verified as complete, at least at a point in time, by seeing that the sum of the key hash information matches the directory. 

Maybe the MIF works the same way? Or maybe its more syntactically pure if we call it a SIF. 

	merge:origfile_fc123:CLOSURE -> hashOfSignatures(abs6001,abs6002)
	split:origfile_fc123:(1/2) -> abs6001
	split:origfile_fc123:(2/2) -> abs6002

This does actually seem a lot better and mirrors the way that a directory works.


A packfile is some special concept of a of a split with syntax to tell how to reconstruct something. 


  
I think I need to implement this whole thing the “slow” way before I can figure out how to do it the “fast” way

So step one is getting to the point where we have an easy way to invoke CRH. The recon could just print yes or no if target is sufficent or “contains” the source.

	gfs recon ^fd35 ^2dfse
	gfs recon source target

How do we automagically learn and absorb merge index files?

If you pass me obtainGhostByHash( ^fds34 ), I’ll check my truth sources. I’ll get several objects. One of them will be for the things in the filesystem. The second will be the thing for the merge: file.

The recon should support
- Finding a file inside of directory object
- Finding a file inside a nested directory object
- finding a subdirectory inside of a directory

Okay, so version one of this 

TODO We need to deal with deletes better - if we realize something is deleted, its parent should have its hash cleared.

Source: a294d0c46d9768647f53e58fc69ebb0600c84236
several times got it.







# cache

WACKY IDEA- What if cache poisoning was resolved by LEARN ALWAYS taking the MOST conservative date? No. that would cause needless source verifications. But what about the case of enumeration? Taking the oldest would force full enumeration that should trickle back to all caches. And enumeration is cheap. 

is there a difference from a cache perspective of "latest playback".. e.g. a think that we learn that says - someone in the cache chain hasn't seen a playback of the full directory in XXX days?

OTOH, we can always ask individual truth keepers what their enumeration date is.

where i am right now is that hash checking is working well. But there needs to be some kind of checkpoint verb. and we need a basic restore mechanic.
lets get this all working so we can start to understand performance issues and think through fixes. 

we don't seem to have an acceptable forceEnumeration. The date is the OLDEST date that contribute to that tree. Need to construct a test schenario where this falls out of date, and then where a readdir scan reverifies it. That will actually fix 9/10 of the cache poisoning problem.

need some simple db verification tools. Make sure all AFNs from the hash cache are in the other thing and visca versa. 


# The ghost backup situation

I want to backup object X, which is a file. How do I know if it is present on the backup directory?

- Worse case: I traverse every node, looking for a hash duplicate. With caching, this actually may just require a relatively modest load
- Better case: I find some truth engine that supporst canReconstructHash().

CRH() dumb implementation is to select an entire swath of filenames using the B-Tree and insert them into the hash store every time. But how do we know all subchildren are loaded? I think it is safe to assume that in a consistent system, if they are in the AFN->Ghost store, they are also in the HASH->AFN store. We can create tools to do a consistent check for this (and probably should.) If we stored something on the parent that told us the size and number of files, we could do a fast iteration over the exploded tree, and verify that information. Presumably with cursors that could be made to be faster than hitting the database so many times.

A deeper scan aims for a full enumeration, but has to use ONLY the ghost data inside that store, so that gaps can be identified.

	GDB->verifyFullDIrectoryEnumeration( $g );

This implies there is some sort of flag-- when a whole block of things come over into a cache, I can mark them as authoritative and that effectively the entire cache is up to date.

This seal on the rows would be broken on insert. 

I don't need to be too worried about false-positives since those can be verified. But false negatives would suck a bit.

Maybe we push this back to the user.  We do some spot checks on a few directories at random. If they don't seem enumerated, we issue a warning and tell the user to run the checkpointing tools again.

A really advanced idea is to have a CRH cache. I bet there is some hyper-fast file format that could be used. Or maybe an in memory hash is not that bad? Perl can deal with really gnarly hash sizes and these checksums can be packed into a file for uber-fast lookup.

	# the problem of a cache of a cahce of this kind it hat the "full deal" is what matters.
	# it seems to take about 1.4 user seconds to enumerate 1.36M rows in BDB
	# that tells me we've got a lot of room to grow here

Seems like the best thing to do in V1 is assume "user must checkpoint" behavior.

Checkpointing requires a full stat, and a hash if the stat revoked our hash.
it also should make sure any objects described (snapshots, directories, tarfiles, whatever) are pulled into the cache.

reasonable to warn that the current target of the backup has not been checkpointed in XXX days. 






# Original notes from duplicateUnderPath

We have to decide at this point if the universal cache is up to date. 
Each FSE might handle this differently.

I think the check would recurively look at subdirectories? Or maybe just
simple time based? I'd like to find some way to prevent the need for the
user to be smart enough to know that a checkpoint is needed.

One way to handle this is there is some kind of
"hasAcceptableDirscan()". If dirscan is popualted for an AFN, we could avoid
the full traversal. But an acceptable dirscan on the .gfs_ip doesn't mean the
one inside the cache is up to date.

you'd want two fields (most recent enumer, and oldest enum.) 

maybe there is a recursive way. I can "test my own" understanding.
first, i get the GFS_IP for the current directory
its going to have some stats in it about sizes and file counts and whatnot.
and i compare that to my own db enumeration.

BDB may or may not be quick enough to allow this..?
but doesn't its Btree have some sort of "# records between?"
that would be optimal
that is the check?

the problem of a cache of a cahce of this kind it hat the "full deal" is what matters.
it seems to take about 1.4 user seconds to enumerate 1.36M rows in BDB
that tells me we've got a lot of room to grow here








# File Versions - how should they work?

We need some way to address file versions. 

I can copy older versions of files by referring to tree state objects or by dates. 

	Cp a b

Copies file a to position b

	Cp a --state-before timespec

This looks for a copy of a that was the most recent before a certain date. 

We also need a concept of a snapshot where it's a set of file versions. And it's okay for them to be tied to an absolute afb. 

Also nice if the ghost points to that snapshot like isMemberOf 

That way even if the record of the snapshot is lost we are okay.

This means that learn may revoke the version which should clear out memberOf. Any change to core stat or hash should do this. 

If 5 files are needed to construct file x nice to have source afn. This could enable striping. 

So the original afn stands. But we gain 5 new split objects each pointing back to the hash they reconstruct. So this can actually live in the afn. But nice if splits carry along the ghost of the thing they reconstruct. 

Same logic for tar files and one to manies. 

The hash of tar file is the afn. 

But nice if we can store the path of the tar file. Wait. Why? We can reverse look that up. Hash to file names are assumed to be o(1) basically

So the split can be subclassed to handle par

And the merge can beb




# Lineage

GFS has a concept of files having a limited-use universal name. When a file is turned into a ghost, GFS tries to find a UUID for it. The UUID is a combination of the filename, size, modification date and inode plus some random data. When the file changes or moves, it will obviously get a new UUID. But if GFS can figure out where the file came from, it will mark an upstream UUID and an upstream checksum. This lets it understand lineage.

Say that a file ~/work-dir/jeremy.txt changes. GFS will look for the most recent ghost for that file. That recent ghost becomes the upstream ghosts.

This is how lshistory works. When you want the history of file X, it attempts to walk through a tree of upstream pointers, gathering the other file versions. 

When a file is backed up, it stores its original source LFN, creating a family of related files. It also flags the ghost of any file version to get saved along with.  

GFS doesn’t need this data to be correct to make backups, but it does need it to provide the most intelligble reports on the histories of files and directories. 

fields to add

UUID
upstreamUUID
lineageUUID
lineageHash
upstreamHash
effectiveDate

WAIT, we don’t actually need all of these.

ghost->assignUUID();

any time a ghost is saved with a change (effectiveDate increases), it could get dropped into the FVDB.

iside learn is a “triggerNewIdentity()” which roles over the UUID. But when is this called? cna’t think of a single case. if it has the same AFN, we’d always want it to pick up previous. 

during pre-save, any node without a lineage could get one. look in the FVDB for the same AFN. or same inode and size (later point). 

Only write to FVDB if has stat and hash, and if effective date has changed.

effective date is propogated when marking dirty. dirty also ups the sequence number.

learn should always take the max of the two sequence numbers, and absorb a UUID if one is not already assigned.

UUID should be assigned during pre-save.



fileversions dont’ learn. we just take them all together.

That means that AFN+effective_date is unique inside the FVDB. 
 
so.. should now a backed up thing should know what AFN it backed up??? 

It doesn’t need to. lshistory enumerates all ghosts with that AFN. that tells us the checksums.

as long as the FVDB entries get passed along to the backup location in a way they can be re-integrated, we’re fine.

and i think they are!! why? because the snapshot stored there could be a bunch of non:SNAP things. which owuld be implicit file versions.

so we got all of this with just effectiveDate, sequence # and UUID.

what if user clears .gfs
then it assings a new UUID. that might not match the older ones.
that is true. but a new operation could theoretically link them.  we can write this idea into a formerUUID field, which now creates a forward-looking link between the new UUID and the old one.

is there a generalization of this mechanism for splits and merges?

say 5 different versions of bigfile.PSD are stored in one packfile

or bigfile.PSD is split across 5 different files

maybe that creates a use for “upstream AFN, upstream UUID”??






in my ideal tool, i'd do the following

	gfs init-volume /              laptop-root
	gfs init-volume /Volumes/usb   usb1-spare

	mkdir ~/working-stuff
	add lots of things under here

Now, to "run" a backup...

	gfs push . 

	# Looks for things in laptop-root its never seen before, and then fishes around
	#for a place to stash them. 

	./working-stuff/a,b,c -> usb1-spare/backup-today-20160601

And i can check on replication status whenever I want...

	gfs check .

	15 files have stashed copies on usb1-spare (trusted, but currently offline)
	4 files are novel and unreplicated anywhere else

The key thing is that GFS stays flexible where and how these other things are
saved and stored. And it doesn't create symlinks or other messy things like
that.

GFS also tries to stay flexibile about how blobs are backed up. The lowest
common denominator of a backed up file is just a user-level copy of the file.



# 




# One to Many references

there is probably also a type of thing called an opener.. 

expand:foo/bar.gz:bar

The AFN for an opener is the hash of the object being openeed

expand:234fc234ffeeaab3234b:













So really base case, lets implement 

- singleton object
gfs init 
  - makes all of the directories and stuff
gfs store filex
  - exercises the storeGhost API (which can be hackist, no GST required.)
gfs obj-setname foo bar
  - exercises pointer logic
  



Side note: We need a way of defining a ghoststore that is "backed" by a file such that slurping is completely lazy and uses some sort of native indexing tool. Hate the idea that a million rows of a ghoststore might have to be read just to get some random value. 







# great idea here: (copypaste from BUP technical notes)


bup makes accommodations for the expected "worst-case" filesystem timestamp resolution -- currently one second; examples include VFAT, ext2, ext3, small ext4, etc. Since bup cannot know the filesystem timestamp resolution, and could be traversing multiple filesystems during any given run, it always assumes that the resolution may be no better than one second.

As a practical matter, this means that index updates are a bit imprecise, and so bup save may occasionally record filesystem changes that you didn't expect. That's because, during an index update, if bup encounters a path whose actual timestamps are more recent than one second before the update started, bup will set the index timestamps for that path (mtime and ctime) to exactly one second before the run, -- effectively capping those values.





# Compare mode

a new sematic i didn't realize
its not reconstruct
its stronger
it asks "is everything here exactly as it was in the snapshot?"


# thoughts on the absolute ghost cache (AGC)

## Problem 1

THe absolute cache is an interesting beast and has some peculuarities I need
to work through. First of all, how should it interact with .gfs_ip files? Say
I look for an ghost record and hit the AGC. There is no opportunity for state
to flow up from the .gfs_ip cache into the AGC. And if there is a dirty
record, we'll have to spontaneously make sure that the gfs_ip is loaded, which
itself may disrupt known state..

## Problem 2

When serializing to the AGC, we will write the filenames by asking the ghost
to represent them in an absolute way. There are at least two transformations
we can expect:

	1- The filenames will go to absolute
	2- snapshot roots will get converted to ^234abcdfef things

Now when those AGCs are retrieved, at what point do we "convert" them into
their local versions? Do we ever need to do this?

Lets say we leave them as absolutes. This will mean that anyone who calls
lfn() will see something they may not recognize. But the behavior of the
software may be correct anyway -- its not going to be fed "wrong" information,
just getting data in an unexpected form.

Maybe a better way to think about the AGC is that it is an overarching store
used to populate certain peices of information. If I want a ghost of ./a/test-
file1, I make it and it says local all of its life. But as a pre-loading step,
to "get me started", one source of truth about it is to copy data over from
the AGC. But we can do that with a bunch of "learn" optimizations. I can
g->learnFromOtherGhost(), and when I do that, I just check the AFNs are the
same. If they are, the data is safe to transfer over. The learning algorithim
will be smart to either keep or discard things.

My preference is to deal with .gfs_ip files at termination of the program, so 
lets let the dirty handler take that. 

SO we need really good learn semantics. Maybe we should write those down.

In the case of snapshot, I start with snap:foo:filea. Lets say I want to
operate on that file to do an LS. I'll make a ghost representing
snap:foo:filea. The AFN convert will see it as a ^234db pointer (forcing a
checksum of the file along the way, which is an separate thing), and therefore
we'll potentially hit the global cache, not even reading the file directly.

So decision stands: the AFNs will never directly enter the pool.






## Entries vs. ghosts

An entry is a thing that exists somewhere for the name. A ghost is a view
about state of an entry. There can be many conceptual ghosts pointing at the
same entry. When I call the truthkeeper, my request is to get the most recent
(or best) ghost for that entry. 











things that are not virtual are 'confirmed'.

they could be deleted, present, missing, etc

but to not be virtual, someone somewhere has to either deserialize them
from the past, or find them authoritatively on disk 

that way we don't contaminate the pool of ghosts

if the program that the user is asking to run cannot track down a confirmed ghost,
they can decide what to do (it's basically a file not found.)



# How do ghost work when they cross into the world of snapshots?

hah, interesting issue: what ghosts shpuld we pull from a snapshot? its
written as a bunch of fs:root: things... so  pulling them straight out of the
file contaminates the truth store.

but really, as a snapshot, we need to load it as virtualized objects, in their
world now, they are in a different name space

raises another question: boundary case -if you snapshot a snapshot, what
should happen? - i think the right thing is to either save a snapshot with
rewritten LFNs or to change the LFNs when loading

DECISION: I think its more correct to do on writing, e.g. -> convert  to snapshot LFNs
during the write operation

this requires new FSE semantics
should we think of a snapshot operation as a push into a filesystem?

YES

make a ghost for the snapshot-to-be
mkae FSE
FSE->addObject( $g );
	clone the ghost and re-write the LFN
	make them immutable
FSE->finalize();

we probably will need similiar semantics on object stores








# What is a ghost?

The basic object is a ghost. A ghost is a description of some object
discovered "for real" in the filesystem somewhere.

Things we track for ghosts:

FULLHASH - SHA of the bytestream (aka ghosthash)
FASTHAD - fast SHA of bytestream (empty if not present)
LASTMOD - last modified date from FS
DT_FULLHASH - last full verify
DT_FASTHASH - last fast verify
DT_DISC_OLD - oldest date of discovery
DT_DISC_NEW - newest date of discovery
LFN - one or more pathnames for ghosthash

my $ghost = makeOrFindGhost( LFN );

$GHOST->[FULLHASH];

writeGhostDescriptor( path, GHOST );

A .ghostdesc file is a list of the above entities of information. It is
basically serialized ghosts which have very local basenames for paths. 

The ghoststore is an abstraction around a collection of ghosts there can be
different implementations the most basic one is just a hashing container that
is as lazy as possible.


# About LFNS

LFN - local file name - whatever is colloquial to the USER to
describe something. LFNs can never presume to be universally unique between
invocations of GFS because they may be relative paths or tags/references may
change. But the user's maximal intent must be captured in the LFN.

	for instance fs:a depends on your CWD
	for a different PWD, fs:a could refer to a different file

However, there are many different handlers.

	/foo
	fs:/foo
	snap:foo.snap:file123
	remote:jgilbert@auspice.net:file123

In this case, the "LFN" is a composite of a handler, a root and a path. The
user may provide some or all of these peices on the command line. We need to
be be able  to intelligently construct the rest.

LFN's can be fullqualified, which means fully specify the missing strings and
to canonicalized the path.

To determine if two LFNs are the same inside one running instance of GFS, the
only way to do it is to canonicalize that path portion both to remove . and
../ etc. Then you need you make sure to attach the ROOT and the HANDLER.

across instances, the only way is to convert to an AFN (absolute file name)
The AFN is a unique permanent name for the location

FQAFN - fully qualified AFN: handler:root:afn
FQLFN - fully qualified LFN: handler:root:path (can be local)

snapshots will generally represent LFNs, typically from the point of view of
the PWD but their power is that they should NOT be foreced to contain AFNs -
the snapshot could be  reconstructed anywhere and we'll leave it to the user
to either feed a snapshot the AFNs or the LFNS.

in the nested case, assume the following data is stored

	snap:testdir/snapshot.gfssnap:
	snap:testdir/snapshot.gfssnap:a
	snap:testdir/snapshot.gfssnap:b

in this case, the LFN of the snapshot file itself is testdir/snapshot.gfssnap

So LFNs are designed to hold three peices of information, and together forms a
unique namespace.















